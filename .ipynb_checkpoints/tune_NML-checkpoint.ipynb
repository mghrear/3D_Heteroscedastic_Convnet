{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d12e6-af66-4e57-adba-9a76efc64f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mytools\n",
    "import mymodels\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51c421-e4d4-475f-ab1a-b3179cab9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "# Select Energy (35 or 50 keV)\n",
    "Energy = 50\n",
    "# Select diff amount (H or L)\n",
    "Diff = 'H'\n",
    "#Cheat True/False\n",
    "cheat = True\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6c0db-da0f-4961-b947-65146ee7cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store results\n",
    "df_tune = pd.DataFrame(columns = [\"wo\", \"n_sigma_L\", \"n_sigma_H\", \"Loss\" , \"eff\"])\n",
    "\n",
    "# Load the non-ML model\n",
    "model_NML = mymodels.NML\n",
    "\n",
    "# x/y/z length being kept in cm\n",
    "eff_l= mytools.voxel_grid['eff_l']\n",
    "# Voxel size in cm\n",
    "vox_l = mytools.voxel_grid['vox_l']\n",
    "    \n",
    "# Copy sparse tensor info dataframe\n",
    "NML_info = pd.read_pickle('/home/majd/sparse_testing_tensors_'+str(Energy)+'keV_'+Diff+'diff/sparse_tensor_info.pk')\n",
    "# Promote index to column\n",
    "NML_info = NML_info.reset_index()\n",
    "\n",
    "# Add position and charge information for each row\n",
    "NML_info[\"positions\"] = NML_info.apply(lambda row: torch.load('/home/majd/sparse_testing_tensors_'+str(Energy)+'keV_'+Diff+'diff/'+ 'sparse_recoils_' + str(row['index']) + '.pt').type(torch.FloatTensor).coalesce().indices().int().numpy()*vox_l-eff_l, axis=1)\n",
    "NML_info[\"charges\"] = NML_info.apply(lambda row: torch.load('/home/majd/sparse_testing_tensors_'+str(Energy)+'keV_'+Diff+'diff/'+ 'sparse_recoils_' + str(row['index']) + '.pt').type(torch.FloatTensor).coalesce().values().flatten().numpy()*1.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79391d05-5d39-42e3-8c93-1bf8f247856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wo in np.arange(0.03,0.08,0.01):\n",
    "    for n_sigma_L in np.arange(1.3,1.8,0.1):\n",
    "        for delta_sigma in np.arange(1.3,1.8,0.1):\n",
    "            \n",
    "            n_sigma_H = n_sigma_L + delta_sigma\n",
    "            \n",
    "            v_pred_temp, v_true_temp, off_true_temp = mytools.test_NML(NML_info, model_NML, n_sigma_L, n_sigma_H, wo, cheat)\n",
    "            \n",
    "            NML_eff_temp = len(v_pred_temp)/len(NML_info)\n",
    "            NML_Loss_temp = mytools.CSloss(torch.Tensor(v_pred_temp), torch.Tensor(v_true_temp))\n",
    "            \n",
    "            df_tune = df_tune.append({ 'wo' : wo, 'n_sigma_L' :  n_sigma_L, 'n_sigma_H' : n_sigma_H, 'Loss' : NML_Loss_temp, 'eff' : NML_eff_temp }, ignore_index = True)\n",
    "            df_tune.to_pickle('tune_NML_'+str(Energy)+'keV_'+Diff+'diff_cheat-'+str(cheat)+'.pk')\n",
    "            print('wo: ', wo, ' n_sigma_L: ',  n_sigma_L, ' n_sigma_H: ', n_sigma_H, ' Loss: ', NML_Loss_temp, ' eff: ', NML_eff_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c2e73-cf6e-4874-8e4e-7b7babd67706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
