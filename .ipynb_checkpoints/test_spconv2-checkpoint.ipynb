{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2e6ffd-ee56-4f14-ba45-439e1e45857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mytools\n",
    "import pandas as pd\n",
    "\n",
    "import spconv.pytorch as spconv\n",
    "from spconv.pytorch import functional as Fsp\n",
    "from spconv.pytorch.utils import PointToVoxel\n",
    "from spconv.pytorch.hash import HashTable\n",
    "import contextlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb73c3-5eea-45bd-a497-c986db160cfa",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10e975b-a5ce-47ca-8301-7184ade2540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  131055\n",
      "Validation samples:  32763\n"
     ]
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "st_info = pd.read_pickle('/home/majd/sparse_training_tensors_test/sparse_tensor_info.pk')\n",
    "st_info.head()\n",
    "\n",
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc='/home/majd/sparse_training_tensors_test/', st_info=st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda4b2c1-9d54-430a-8c39-4e29938b83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W, D]: torch.Size([64, 120, 120, 120, 1])\n",
      "Shape of y: torch.Size([64, 3]) torch.float32\n",
      "Offsets:  torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print tensor shapes\n",
    "for X_plot, y_plot, offset_plot in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W, D]: {X_plot.shape}\")\n",
    "    print(f\"Shape of y: {y_plot.shape} {y_plot.dtype}\")\n",
    "    print(\"Offsets: \", offset_plot.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042d17c-6d88-4929-bc97-c1427004f3e4",
   "metadata": {},
   "source": [
    "# Define Convnet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4caed557-334d-4e49-b66b-acd1eabc8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = spconv.SparseSequential(\n",
    "            spconv.SparseConv3d(in_channels=1, out_channels=50, kernel_size=6, stride=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseMaxPool3d(kernel_size=2, stride=2),\n",
    "            spconv.SparseConv3d(in_channels=50, out_channels=30, kernel_size=4, stride=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseConv3d(in_channels=30, out_channels=20, kernel_size=3, stride=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseMaxPool3d(kernel_size=2, stride=2),\n",
    "            spconv.ToDense(),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(12**3 *20, 100)\n",
    "        self.fc2 = nn.Linear(100, 30)\n",
    "        self.fc3 = nn.Linear(30, 3)\n",
    "        \n",
    "        self.shape = shape\n",
    "        \n",
    "\n",
    "    def forward(self, features, indices, batch_size):\n",
    "        \n",
    "        x_sp = spconv.SparseConvTensor(features, indices, self.shape, batch_size)\n",
    "                        \n",
    "        x = self.net(x_sp)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        output = F.normalize(self.fc3(x),dim=1)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "model = Net(shape =torch.Tensor((120,120,120))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840324e3-654c-4dd0-8de1-36fbb89a82e1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdcc104e-7746-4794-a117-fa2b4b88a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-07)\n",
    "\n",
    "# Define Loss function\n",
    "CS = nn.CosineSimilarity()\n",
    "def loss_fn(output, target):\n",
    "    loss = torch.mean(-1.0*CS(output,target))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02846e2e-d3e9-4eb2-865b-d57284c43c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Current batch training loss: -0.055110  [    0/131055]\n",
      "Current batch training loss: -0.309562  [ 6400/131055]\n",
      "Current batch training loss: -0.337915  [12800/131055]\n",
      "Current batch training loss: -0.500363  [19200/131055]\n",
      "Current batch training loss: -0.543080  [25600/131055]\n",
      "Current batch training loss: -0.446381  [32000/131055]\n",
      "Current batch training loss: -0.562182  [38400/131055]\n",
      "Current batch training loss: -0.464920  [44800/131055]\n",
      "Current batch training loss: -0.465194  [51200/131055]\n",
      "Current batch training loss: -0.475866  [57600/131055]\n",
      "Current batch training loss: -0.595273  [64000/131055]\n",
      "Current batch training loss: -0.485958  [70400/131055]\n"
     ]
    }
   ],
   "source": [
    "# Implement early stopping in training loop\n",
    "# Stop if validation loss has not decreased for the last [patience] epochs\n",
    "# The model with the lowest loss is stored\n",
    "patience = 2\n",
    "\n",
    "Training_losses = np.array([])\n",
    "Validation_losses = np.array([])\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    Training_losses = np.append(Training_losses, mytools.train_sparse(train_dataloader, model, loss_fn, optimizer, device))\n",
    "    Validation_losses = np.append(Validation_losses, mytools.validate_sparse(val_dataloader, model, loss_fn, device))\n",
    "    \n",
    "    # Keep a running copy of the model with the lowest loss\n",
    "    if Validation_losses[-1] == np.min(Validation_losses):\n",
    "        final_model = copy.deepcopy(model)\n",
    "    \n",
    "    if len(Validation_losses) > patience:\n",
    "        if np.sum((Validation_losses[-1*np.arange(patience)-1] - Validation_losses[-1*np.arange(patience)-2]) < 0) == 0:\n",
    "            print(\"Stopping early!\")\n",
    "            break\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6c9d9-757e-420c-ad6f-63911baab785",
   "metadata": {},
   "source": [
    "# Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c85e00-c2a4-47d0-9639-5c7ed7adb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss\n",
    "# Indicate where the final model stopped training\n",
    "\n",
    "best_epoch = np.argmin(Validation_losses)\n",
    "\n",
    "plt.plot(np.arange(len(Training_losses)),Training_losses,label=\"Training Loss\")\n",
    "plt.plot(np.arange(len(Validation_losses)),Validation_losses,label=\"Validation Loss\")\n",
    "plt.axvline(best_epoch,label=\"Final model stopped here\",color='k')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cosine Similarity Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c2ff-12bc-4799-bcfd-47142a9baebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137a2665c242313c11d472736bb1efbdaf7608c607fce3fc4f47a32817024ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
