{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2e6ffd-ee56-4f14-ba45-439e1e45857d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import spconv.pytorch as spconv\n",
    "import matplotlib.pyplot as plt\n",
    "import mytools\n",
    "import mymodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb73c3-5eea-45bd-a497-c986db160cfa",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d73f31-4c34-4f0a-b253-1e335cde8f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>offset</th>\n",
       "      <th>diff</th>\n",
       "      <th>energy</th>\n",
       "      <th>true_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.5923457337920527, -0.5369941830475861, -0.6...</td>\n",
       "      <td>[-0.851898273495669, 2.1253245532459824, 0.445...</td>\n",
       "      <td>0.046168</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.6164927192719855, 0.5695943083433039, -0.5...</td>\n",
       "      <td>[-1.017085182270888, -1.6805460012244295, 1.10...</td>\n",
       "      <td>0.028843</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6322337566233259, -0.16773581669128113, -0....</td>\n",
       "      <td>[-0.10613203070195368, 0.22289410895907838, 1....</td>\n",
       "      <td>0.025293</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.2908139608694231, -0.8484810341097399, -0.4...</td>\n",
       "      <td>[-1.0096727220437194, 1.2613684348817842, 1.42...</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.7738521869833273, -0.07925597736546798, -0....</td>\n",
       "      <td>[0.7752193984015442, 0.8404383794565299, 0.902...</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766793</th>\n",
       "      <td>[-0.2069418024409927, 0.2672370217202399, -0.9...</td>\n",
       "      <td>[-0.5891835692649702, -0.5092523892090935, 1.8...</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>50</td>\n",
       "      <td>2766793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766794</th>\n",
       "      <td>[0.5942730241053608, 0.6175260630673811, -0.51...</td>\n",
       "      <td>[-0.723736545709404, -0.10798660967928463, 0.6...</td>\n",
       "      <td>0.030250</td>\n",
       "      <td>50</td>\n",
       "      <td>2766794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766795</th>\n",
       "      <td>[0.3312106913072638, 0.5407131844563555, 0.773...</td>\n",
       "      <td>[-0.3615579024644222, 1.934812461239543, -2.26...</td>\n",
       "      <td>0.030298</td>\n",
       "      <td>50</td>\n",
       "      <td>2766795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766796</th>\n",
       "      <td>[-0.0071324298603245555, 0.623790473641556, 0....</td>\n",
       "      <td>[0.7469683683355023, -2.603944946514045, -0.74...</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>50</td>\n",
       "      <td>2766796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766797</th>\n",
       "      <td>[-0.06412683698521963, -0.9977986677530827, 0....</td>\n",
       "      <td>[1.8179965860730307, 1.618875928754491, -1.181...</td>\n",
       "      <td>0.024449</td>\n",
       "      <td>50</td>\n",
       "      <td>2766797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2766798 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       dir  \\\n",
       "0        [0.5923457337920527, -0.5369941830475861, -0.6...   \n",
       "1        [-0.6164927192719855, 0.5695943083433039, -0.5...   \n",
       "2        [0.6322337566233259, -0.16773581669128113, -0....   \n",
       "3        [0.2908139608694231, -0.8484810341097399, -0.4...   \n",
       "4        [0.7738521869833273, -0.07925597736546798, -0....   \n",
       "...                                                    ...   \n",
       "2766793  [-0.2069418024409927, 0.2672370217202399, -0.9...   \n",
       "2766794  [0.5942730241053608, 0.6175260630673811, -0.51...   \n",
       "2766795  [0.3312106913072638, 0.5407131844563555, 0.773...   \n",
       "2766796  [-0.0071324298603245555, 0.623790473641556, 0....   \n",
       "2766797  [-0.06412683698521963, -0.9977986677530827, 0....   \n",
       "\n",
       "                                                    offset      diff energy  \\\n",
       "0        [-0.851898273495669, 2.1253245532459824, 0.445...  0.046168     40   \n",
       "1        [-1.017085182270888, -1.6805460012244295, 1.10...  0.028843     40   \n",
       "2        [-0.10613203070195368, 0.22289410895907838, 1....  0.025293     40   \n",
       "3        [-1.0096727220437194, 1.2613684348817842, 1.42...  0.034410     40   \n",
       "4        [0.7752193984015442, 0.8404383794565299, 0.902...  0.033654     40   \n",
       "...                                                    ...       ...    ...   \n",
       "2766793  [-0.5891835692649702, -0.5092523892090935, 1.8...  0.043683     50   \n",
       "2766794  [-0.723736545709404, -0.10798660967928463, 0.6...  0.030250     50   \n",
       "2766795  [-0.3615579024644222, 1.934812461239543, -2.26...  0.030298     50   \n",
       "2766796  [0.7469683683355023, -2.603944946514045, -0.74...  0.039175     50   \n",
       "2766797  [1.8179965860730307, 1.618875928754491, -1.181...  0.024449     50   \n",
       "\n",
       "        true_index  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  \n",
       "...            ...  \n",
       "2766793    2766793  \n",
       "2766794    2766794  \n",
       "2766795    2766795  \n",
       "2766796    2766796  \n",
       "2766797    2766797  \n",
       "\n",
       "[2766798 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "file_loc = '/mnt/scratch/lustre_01/scratch/majd/sparse_training_tensors/'\n",
    "st_info = pd.read_pickle(file_loc+'sparse_tensor_info.pk')\n",
    "st_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10e975b-a5ce-47ca-8301-7184ade2540b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  2213439\n",
      "Validation samples:  553359\n"
     ]
    }
   ],
   "source": [
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc = file_loc, st_info = st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create training and validation DataLoaders\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c43560-68ac-43a3-af52-5832fd0d22ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W, D]: torch.Size([256, 120, 120, 120, 1])\n",
      "Shape of y: torch.Size([256, 3]) torch.float32\n",
      "Offsets:  torch.Size([256, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print tensor shapes\n",
    "for X_plot, y_plot, offset_plot in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W, D]: {X_plot.shape}\")\n",
    "    print(f\"Shape of y: {y_plot.shape} {y_plot.dtype}\")\n",
    "    print(\"Offsets: \", offset_plot.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3e353-2f8e-47c5-a62a-e2fa5b10c2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel grid shape:  torch.Size([120, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "#Record shape of voxel grid\n",
    "grid_shape = X_plot.shape[1:4]\n",
    "print(\"Voxel grid shape: \" , grid_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042d17c-6d88-4929-bc97-c1427004f3e4",
   "metadata": {},
   "source": [
    "# Load Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2164f207-01a8-4f08-b732-aa8afdc31400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "spConvnet_HSCDC_subM(\n",
      "  (net): SparseSequential(\n",
      "    (0): SubMConv3d(1, 32, kernel_size=[7, 7, 7], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (1): ReLU()\n",
      "    (2): SubMConv3d(32, 40, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (3): ReLU()\n",
      "    (4): SparseConv3d(40, 50, kernel_size=[6, 6, 6], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (5): ReLU()\n",
      "    (6): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (7): SparseConv3d(50, 30, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (8): ReLU()\n",
      "    (9): SparseConv3d(30, 10, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (10): ReLU()\n",
      "    (11): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (12): ToDense()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2160, out_features=500, bias=True)\n",
      "  (fc2_1): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_1): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_1): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (fc2_2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = torch.load('../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256.pt').to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840324e3-654c-4dd0-8de1-36fbb89a82e1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b476f68-e293-4f96-9d73-dfbeeb7c1f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-07)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b95247-c06f-49d6-aa3d-81f761452e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss for HSCDC convnet\n",
    "def NLLloss(output, target):\n",
    "    \n",
    "    # target us the x parameters in the Kent distribution\n",
    "    G = output[0] # \\gamma_1 parameters in Kent distribution\n",
    "    K = output[1] # \\kappa parameter in Kent distribution\n",
    "    \n",
    "    #print(K.flatten() )\n",
    "    #print(-1.0*torch.log(torch.div(K,4*torch.pi*torch.sinh(K))).flatten() [K.flatten() < 70])\n",
    "    \n",
    "    loss1 = -1.0 * torch.log(torch.div(K,4*torch.pi*torch.sinh(K))).flatten()\n",
    "    loss2 = -1.0 * ( torch.log(torch.div(K,2*torch.pi)) - K ).flatten()\n",
    "    \n",
    "    # Compute negative log likelihood using Kent distribution\n",
    "    loss = torch.mean( torch.minimum(loss1,loss2) - ( K.flatten() * torch.sum(G*target,dim=1) ) )\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325e6d28-1957-4730-a60e-894c8b478e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y, offset) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.type(torch.FloatTensor).to(device), y.to(device)\n",
    "        \n",
    "        X = X.coalesce()\n",
    "        indices = X.indices().permute(1, 0).contiguous().int()\n",
    "        features = X.values()\n",
    "            \n",
    "        # Compute prediction error\n",
    "        pred = model(features,indices,X.shape[0])\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Norm Clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 10, norm_type=2)\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(\"nan gradient found\")\n",
    "                print(\"The loss is :\", loss.item())\n",
    "                raise SystemExit\n",
    "                \n",
    "        optimizer.step()\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "            \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Current batch training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    print(f\"Training loss: {train_loss:>7f}\")\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de98c3d9-cc06-4d4f-aa0d-1452af9cdd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch training loss: 0.140218  [    0/2213439]\n",
      "Current batch training loss: 0.060658  [25600/2213439]\n",
      "Current batch training loss: 0.143541  [51200/2213439]\n",
      "Current batch training loss: 0.205980  [76800/2213439]\n",
      "Current batch training loss: 0.069807  [102400/2213439]\n",
      "Current batch training loss: -0.032565  [128000/2213439]\n",
      "Current batch training loss: -0.011085  [153600/2213439]\n",
      "Current batch training loss: 0.175005  [179200/2213439]\n",
      "Current batch training loss: 0.058386  [204800/2213439]\n",
      "Current batch training loss: 0.113592  [230400/2213439]\n",
      "Current batch training loss: 0.017840  [256000/2213439]\n",
      "Current batch training loss: -0.011780  [281600/2213439]\n",
      "Current batch training loss: 0.119567  [307200/2213439]\n",
      "Current batch training loss: 0.494554  [332800/2213439]\n",
      "Current batch training loss: 0.187047  [358400/2213439]\n",
      "Current batch training loss: -0.144897  [384000/2213439]\n",
      "Current batch training loss: 0.157405  [409600/2213439]\n",
      "Current batch training loss: 0.304793  [435200/2213439]\n",
      "Current batch training loss: 0.064016  [460800/2213439]\n",
      "Current batch training loss: 0.251767  [486400/2213439]\n",
      "Current batch training loss: 0.137187  [512000/2213439]\n",
      "Current batch training loss: 0.060256  [537600/2213439]\n",
      "Current batch training loss: 0.085881  [563200/2213439]\n",
      "Current batch training loss: 0.079490  [588800/2213439]\n",
      "Current batch training loss: 0.325905  [614400/2213439]\n",
      "Current batch training loss: 0.058038  [640000/2213439]\n",
      "Current batch training loss: 0.148271  [665600/2213439]\n",
      "Current batch training loss: 0.340844  [691200/2213439]\n",
      "Current batch training loss: -0.021140  [716800/2213439]\n",
      "Current batch training loss: 0.248867  [742400/2213439]\n",
      "Current batch training loss: -0.026561  [768000/2213439]\n",
      "Current batch training loss: 0.079050  [793600/2213439]\n",
      "Current batch training loss: 0.012737  [819200/2213439]\n",
      "Current batch training loss: 0.132324  [844800/2213439]\n",
      "Current batch training loss: -0.090477  [870400/2213439]\n",
      "Current batch training loss: 0.265381  [896000/2213439]\n",
      "Current batch training loss: 0.104696  [921600/2213439]\n",
      "Current batch training loss: 0.254579  [947200/2213439]\n",
      "Current batch training loss: 0.140342  [972800/2213439]\n",
      "Current batch training loss: 0.099587  [998400/2213439]\n",
      "Current batch training loss: 0.221072  [1024000/2213439]\n",
      "Current batch training loss: 0.444977  [1049600/2213439]\n",
      "Current batch training loss: 0.077183  [1075200/2213439]\n",
      "Current batch training loss: 0.338344  [1100800/2213439]\n",
      "Current batch training loss: 0.099974  [1126400/2213439]\n",
      "Current batch training loss: 0.095837  [1152000/2213439]\n",
      "Current batch training loss: 0.118811  [1177600/2213439]\n",
      "Current batch training loss: 0.204059  [1203200/2213439]\n",
      "Current batch training loss: -0.071662  [1228800/2213439]\n",
      "Current batch training loss: 0.321562  [1254400/2213439]\n",
      "Current batch training loss: 0.068548  [1280000/2213439]\n",
      "Current batch training loss: 0.075069  [1305600/2213439]\n",
      "Current batch training loss: 0.180479  [1331200/2213439]\n",
      "Current batch training loss: 0.214700  [1356800/2213439]\n",
      "Current batch training loss: 0.182823  [1382400/2213439]\n",
      "Current batch training loss: 0.276515  [1408000/2213439]\n",
      "Current batch training loss: -0.007653  [1433600/2213439]\n",
      "Current batch training loss: 0.252206  [1459200/2213439]\n",
      "Current batch training loss: 0.100552  [1484800/2213439]\n",
      "Current batch training loss: 0.210125  [1510400/2213439]\n",
      "Current batch training loss: 0.091712  [1536000/2213439]\n",
      "Current batch training loss: 0.160363  [1561600/2213439]\n",
      "Current batch training loss: 0.245906  [1587200/2213439]\n",
      "Current batch training loss: 0.377596  [1612800/2213439]\n",
      "Current batch training loss: 0.104596  [1638400/2213439]\n",
      "Current batch training loss: 0.007476  [1664000/2213439]\n",
      "Current batch training loss: -0.052236  [1689600/2213439]\n",
      "Current batch training loss: 0.200800  [1715200/2213439]\n",
      "Current batch training loss: 0.236031  [1740800/2213439]\n",
      "Current batch training loss: 0.196351  [1766400/2213439]\n",
      "Current batch training loss: 0.212876  [1792000/2213439]\n",
      "Current batch training loss: 0.006803  [1817600/2213439]\n",
      "Current batch training loss: 0.015566  [1843200/2213439]\n",
      "Current batch training loss: 0.246883  [1868800/2213439]\n",
      "Current batch training loss: 0.214098  [1894400/2213439]\n",
      "Current batch training loss: 0.102319  [1920000/2213439]\n",
      "Current batch training loss: 0.249576  [1945600/2213439]\n",
      "Current batch training loss: 0.003845  [1971200/2213439]\n",
      "Current batch training loss: 0.090256  [1996800/2213439]\n",
      "Current batch training loss: 0.331538  [2022400/2213439]\n",
      "Current batch training loss: 0.072423  [2048000/2213439]\n",
      "Current batch training loss: 0.156284  [2073600/2213439]\n",
      "Current batch training loss: 0.518107  [2099200/2213439]\n",
      "Current batch training loss: 0.194478  [2124800/2213439]\n",
      "Current batch training loss: 0.023561  [2150400/2213439]\n",
      "Current batch training loss: 0.278304  [2176000/2213439]\n",
      "Current batch training loss: 0.152606  [2201600/2213439]\n",
      "Training loss: 0.127553\n"
     ]
    }
   ],
   "source": [
    "Training_losses = np.array([])\n",
    "Training_losses = np.append(Training_losses, train(train_dataloader, model, NLLloss, optimizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da78453-26d2-4a56-9245-5636e2f8ae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch training loss: 0.151439  [    0/2213439]\n",
      "Current batch training loss: 0.273674  [25600/2213439]\n",
      "Current batch training loss: -0.088438  [51200/2213439]\n",
      "Current batch training loss: 0.060766  [76800/2213439]\n",
      "Current batch training loss: -0.163592  [102400/2213439]\n",
      "Current batch training loss: -0.058581  [128000/2213439]\n",
      "Current batch training loss: 0.105056  [153600/2213439]\n",
      "Current batch training loss: 0.034851  [179200/2213439]\n",
      "Current batch training loss: 0.073247  [204800/2213439]\n",
      "Current batch training loss: 0.191679  [230400/2213439]\n",
      "Current batch training loss: -0.004271  [256000/2213439]\n",
      "Current batch training loss: 0.005362  [281600/2213439]\n",
      "Current batch training loss: 0.127751  [307200/2213439]\n",
      "Current batch training loss: 0.155060  [332800/2213439]\n",
      "Current batch training loss: 0.036145  [358400/2213439]\n",
      "Current batch training loss: 0.108818  [384000/2213439]\n",
      "Current batch training loss: 0.223151  [409600/2213439]\n",
      "Current batch training loss: 0.517959  [435200/2213439]\n",
      "Current batch training loss: 0.237310  [460800/2213439]\n",
      "Current batch training loss: 0.178050  [486400/2213439]\n",
      "Current batch training loss: 0.055686  [512000/2213439]\n",
      "Current batch training loss: 0.104612  [537600/2213439]\n",
      "Current batch training loss: 0.119908  [563200/2213439]\n",
      "Current batch training loss: 0.273414  [588800/2213439]\n",
      "Current batch training loss: 0.167976  [614400/2213439]\n",
      "Current batch training loss: 0.088771  [640000/2213439]\n",
      "Current batch training loss: -0.008490  [665600/2213439]\n",
      "Current batch training loss: -0.090942  [691200/2213439]\n",
      "Current batch training loss: 0.147246  [716800/2213439]\n",
      "Current batch training loss: 0.024760  [742400/2213439]\n",
      "Current batch training loss: 0.016595  [768000/2213439]\n",
      "Current batch training loss: 0.097657  [793600/2213439]\n",
      "Current batch training loss: 0.002856  [819200/2213439]\n",
      "Current batch training loss: 0.041096  [844800/2213439]\n",
      "Current batch training loss: 0.143728  [870400/2213439]\n",
      "Current batch training loss: 0.050094  [896000/2213439]\n",
      "Current batch training loss: -0.019397  [921600/2213439]\n",
      "Current batch training loss: -0.007517  [947200/2213439]\n",
      "Current batch training loss: 0.117421  [972800/2213439]\n",
      "Current batch training loss: 0.289667  [998400/2213439]\n",
      "Current batch training loss: 0.030320  [1024000/2213439]\n",
      "Current batch training loss: 0.311147  [1049600/2213439]\n",
      "Current batch training loss: 0.019587  [1075200/2213439]\n",
      "Current batch training loss: 0.398350  [1100800/2213439]\n",
      "Current batch training loss: 0.308523  [1126400/2213439]\n",
      "Current batch training loss: 0.160208  [1152000/2213439]\n",
      "Current batch training loss: 0.102685  [1177600/2213439]\n",
      "Current batch training loss: 0.088191  [1203200/2213439]\n",
      "Current batch training loss: 0.002025  [1228800/2213439]\n",
      "Current batch training loss: 0.043321  [1254400/2213439]\n",
      "Current batch training loss: 0.278609  [1280000/2213439]\n",
      "Current batch training loss: 0.068663  [1305600/2213439]\n",
      "Current batch training loss: 0.003483  [1331200/2213439]\n",
      "nan gradient found\n",
      "The loss is : -0.06579864770174026\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majd/.conda/envs/mypt/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "Training_losses = np.append(Training_losses, train(train_dataloader, model, NLLloss, optimizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e48907-ff2e-44a4-b0a3-9e0f28f0e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_losses = np.append(Training_losses, train(train_dataloader, model, NLLloss, optimizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea1f1b-f740-4137-8985-00ed05c798c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_losses = np.append(Training_losses, train(train_dataloader, model, NLLloss, optimizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78169dc6-6f43-4cd0-83bf-aece5086090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_losses = np.append(Training_losses, train(train_dataloader, model, NLLloss, optimizer, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6f17-057a-4fad-8b14-a86b6d30e259",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176dad4a-cc29-488c-887b-aad8ae00be06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T01:21:40.062601Z",
     "iopub.status.busy": "2023-06-15T01:21:40.062163Z",
     "iopub.status.idle": "2023-06-15T01:21:40.127919Z",
     "shell.execute_reply": "2023-06-15T01:21:40.127011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to ../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(final_model, \"../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256.pt\")\n",
    "print(\"Saved PyTorch Model State to ../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c2ff-12bc-4799-bcfd-47142a9baebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039de899-ca03-40d8-befc-c5c75c4b87f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137a2665c242313c11d472736bb1efbdaf7608c607fce3fc4f47a32817024ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
