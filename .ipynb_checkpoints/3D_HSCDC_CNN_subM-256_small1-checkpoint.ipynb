{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2e6ffd-ee56-4f14-ba45-439e1e45857d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:30.574618Z",
     "iopub.status.busy": "2023-06-17T01:03:30.574085Z",
     "iopub.status.idle": "2023-06-17T01:03:37.649259Z",
     "shell.execute_reply": "2023-06-17T01:03:37.648201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import spconv.pytorch as spconv\n",
    "import matplotlib.pyplot as plt\n",
    "import mytools\n",
    "import mymodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb73c3-5eea-45bd-a497-c986db160cfa",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d73f31-4c34-4f0a-b253-1e335cde8f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:37.656084Z",
     "iopub.status.busy": "2023-06-17T01:03:37.654797Z",
     "iopub.status.idle": "2023-06-17T01:03:43.723541Z",
     "shell.execute_reply": "2023-06-17T01:03:43.722163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>offset</th>\n",
       "      <th>diff</th>\n",
       "      <th>energy</th>\n",
       "      <th>true_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.5923457337920527, -0.5369941830475861, -0.6...</td>\n",
       "      <td>[-0.851898273495669, 2.1253245532459824, 0.445...</td>\n",
       "      <td>0.046168</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.6164927192719855, 0.5695943083433039, -0.5...</td>\n",
       "      <td>[-1.017085182270888, -1.6805460012244295, 1.10...</td>\n",
       "      <td>0.028843</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.6322337566233259, -0.16773581669128113, -0....</td>\n",
       "      <td>[-0.10613203070195368, 0.22289410895907838, 1....</td>\n",
       "      <td>0.025293</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.2908139608694231, -0.8484810341097399, -0.4...</td>\n",
       "      <td>[-1.0096727220437194, 1.2613684348817842, 1.42...</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.7738521869833273, -0.07925597736546798, -0....</td>\n",
       "      <td>[0.7752193984015442, 0.8404383794565299, 0.902...</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766793</th>\n",
       "      <td>[-0.2069418024409927, 0.2672370217202399, -0.9...</td>\n",
       "      <td>[-0.5891835692649702, -0.5092523892090935, 1.8...</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>50</td>\n",
       "      <td>2766793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766794</th>\n",
       "      <td>[0.5942730241053608, 0.6175260630673811, -0.51...</td>\n",
       "      <td>[-0.723736545709404, -0.10798660967928463, 0.6...</td>\n",
       "      <td>0.030250</td>\n",
       "      <td>50</td>\n",
       "      <td>2766794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766795</th>\n",
       "      <td>[0.3312106913072638, 0.5407131844563555, 0.773...</td>\n",
       "      <td>[-0.3615579024644222, 1.934812461239543, -2.26...</td>\n",
       "      <td>0.030298</td>\n",
       "      <td>50</td>\n",
       "      <td>2766795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766796</th>\n",
       "      <td>[-0.0071324298603245555, 0.623790473641556, 0....</td>\n",
       "      <td>[0.7469683683355023, -2.603944946514045, -0.74...</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>50</td>\n",
       "      <td>2766796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766797</th>\n",
       "      <td>[-0.06412683698521963, -0.9977986677530827, 0....</td>\n",
       "      <td>[1.8179965860730307, 1.618875928754491, -1.181...</td>\n",
       "      <td>0.024449</td>\n",
       "      <td>50</td>\n",
       "      <td>2766797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2766798 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       dir  \\\n",
       "0        [0.5923457337920527, -0.5369941830475861, -0.6...   \n",
       "1        [-0.6164927192719855, 0.5695943083433039, -0.5...   \n",
       "2        [0.6322337566233259, -0.16773581669128113, -0....   \n",
       "3        [0.2908139608694231, -0.8484810341097399, -0.4...   \n",
       "4        [0.7738521869833273, -0.07925597736546798, -0....   \n",
       "...                                                    ...   \n",
       "2766793  [-0.2069418024409927, 0.2672370217202399, -0.9...   \n",
       "2766794  [0.5942730241053608, 0.6175260630673811, -0.51...   \n",
       "2766795  [0.3312106913072638, 0.5407131844563555, 0.773...   \n",
       "2766796  [-0.0071324298603245555, 0.623790473641556, 0....   \n",
       "2766797  [-0.06412683698521963, -0.9977986677530827, 0....   \n",
       "\n",
       "                                                    offset      diff energy  \\\n",
       "0        [-0.851898273495669, 2.1253245532459824, 0.445...  0.046168     40   \n",
       "1        [-1.017085182270888, -1.6805460012244295, 1.10...  0.028843     40   \n",
       "2        [-0.10613203070195368, 0.22289410895907838, 1....  0.025293     40   \n",
       "3        [-1.0096727220437194, 1.2613684348817842, 1.42...  0.034410     40   \n",
       "4        [0.7752193984015442, 0.8404383794565299, 0.902...  0.033654     40   \n",
       "...                                                    ...       ...    ...   \n",
       "2766793  [-0.5891835692649702, -0.5092523892090935, 1.8...  0.043683     50   \n",
       "2766794  [-0.723736545709404, -0.10798660967928463, 0.6...  0.030250     50   \n",
       "2766795  [-0.3615579024644222, 1.934812461239543, -2.26...  0.030298     50   \n",
       "2766796  [0.7469683683355023, -2.603944946514045, -0.74...  0.039175     50   \n",
       "2766797  [1.8179965860730307, 1.618875928754491, -1.181...  0.024449     50   \n",
       "\n",
       "        true_index  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  \n",
       "...            ...  \n",
       "2766793    2766793  \n",
       "2766794    2766794  \n",
       "2766795    2766795  \n",
       "2766796    2766796  \n",
       "2766797    2766797  \n",
       "\n",
       "[2766798 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "file_loc = '/mnt/scratch/lustre_01/scratch/majd/sparse_training_tensors/'\n",
    "st_info = pd.read_pickle(file_loc+'sparse_tensor_info.pk')\n",
    "st_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10e975b-a5ce-47ca-8301-7184ade2540b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:43.728265Z",
     "iopub.status.busy": "2023-06-17T01:03:43.727413Z",
     "iopub.status.idle": "2023-06-17T01:03:43.866296Z",
     "shell.execute_reply": "2023-06-17T01:03:43.865703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  2213439\n",
      "Validation samples:  553359\n"
     ]
    }
   ],
   "source": [
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc = file_loc, st_info = st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create training and validation DataLoaders\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c43560-68ac-43a3-af52-5832fd0d22ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:43.869219Z",
     "iopub.status.busy": "2023-06-17T01:03:43.868841Z",
     "iopub.status.idle": "2023-06-17T01:03:45.204531Z",
     "shell.execute_reply": "2023-06-17T01:03:45.203777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W, D]: torch.Size([256, 120, 120, 120, 1])\n",
      "Shape of y: torch.Size([256, 3]) torch.float32\n",
      "Offsets:  torch.Size([256, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print tensor shapes\n",
    "for X_plot, y_plot, offset_plot in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W, D]: {X_plot.shape}\")\n",
    "    print(f\"Shape of y: {y_plot.shape} {y_plot.dtype}\")\n",
    "    print(\"Offsets: \", offset_plot.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3e353-2f8e-47c5-a62a-e2fa5b10c2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:45.207787Z",
     "iopub.status.busy": "2023-06-17T01:03:45.207529Z",
     "iopub.status.idle": "2023-06-17T01:03:45.213088Z",
     "shell.execute_reply": "2023-06-17T01:03:45.212289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel grid shape:  torch.Size([120, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "#Record shape of voxel grid\n",
    "grid_shape = X_plot.shape[1:4]\n",
    "print(\"Voxel grid shape: \" , grid_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf371ce2",
   "metadata": {},
   "source": [
    "# Visualize a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fb80f6-0b58-42c5-aab5-20bae17fd59c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:03:45.216788Z",
     "iopub.status.busy": "2023-06-17T01:03:45.216549Z",
     "iopub.status.idle": "2023-06-17T01:04:07.390122Z",
     "shell.execute_reply": "2023-06-17T01:04:07.388851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot an example as a sanity check\n",
    "\n",
    "# x/y/z length being kept in cm\n",
    "eff_l = mytools.voxel_grid['eff_l']\n",
    "# Voxel size in cm\n",
    "vox_l = mytools.voxel_grid['vox_l']\n",
    "\n",
    "#Convert to dense, reshape and convert to numpy\n",
    "X_plot = X_plot.to_dense().reshape(-1, 1, 120, 120, 120).numpy()\n",
    "\n",
    "index = 2\n",
    "mytools.plot_tensor_dir(tensor = X_plot[index], start = offset_plot[index].numpy(), direction = y_plot[index].numpy(), eff_l=eff_l, vox_l=vox_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042d17c-6d88-4929-bc97-c1427004f3e4",
   "metadata": {},
   "source": [
    "# Initialize Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2164f207-01a8-4f08-b732-aa8afdc31400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:04:07.395160Z",
     "iopub.status.busy": "2023-06-17T01:04:07.394239Z",
     "iopub.status.idle": "2023-06-17T01:04:09.413471Z",
     "shell.execute_reply": "2023-06-17T01:04:09.412143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "spConvnet_HSCDC_subM(\n",
      "  (net): SparseSequential(\n",
      "    (0): SubMConv3d(1, 32, kernel_size=[7, 7, 7], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (1): ReLU()\n",
      "    (2): SubMConv3d(32, 40, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (3): ReLU()\n",
      "    (4): SparseConv3d(40, 50, kernel_size=[6, 6, 6], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (5): ReLU()\n",
      "    (6): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (7): SparseConv3d(50, 30, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (8): ReLU()\n",
      "    (9): SparseConv3d(30, 10, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (10): ReLU()\n",
      "    (11): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (12): ToDense()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2160, out_features=500, bias=True)\n",
      "  (fc2_1): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_1): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_1): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (fc2_2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = mymodels.spConvnet_HSCDC_small1(shape = grid_shape ).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840324e3-654c-4dd0-8de1-36fbb89a82e1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b476f68-e293-4f96-9d73-dfbeeb7c1f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:04:09.418245Z",
     "iopub.status.busy": "2023-06-17T01:04:09.417225Z",
     "iopub.status.idle": "2023-06-17T01:04:09.425272Z",
     "shell.execute_reply": "2023-06-17T01:04:09.423980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-07)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02846e2e-d3e9-4eb2-865b-d57284c43c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T01:04:09.429767Z",
     "iopub.status.busy": "2023-06-17T01:04:09.428947Z",
     "iopub.status.idle": "2023-06-18T14:27:57.385406Z",
     "shell.execute_reply": "2023-06-18T14:27:57.384469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Current batch training loss: 2.604842  [    0/2213439]\n",
      "Current batch training loss: 2.403850  [25600/2213439]\n",
      "Current batch training loss: 2.204746  [51200/2213439]\n",
      "Current batch training loss: 2.179865  [76800/2213439]\n",
      "Current batch training loss: 2.235632  [102400/2213439]\n",
      "Current batch training loss: 2.054647  [128000/2213439]\n",
      "Current batch training loss: 2.051179  [153600/2213439]\n",
      "Current batch training loss: 2.040403  [179200/2213439]\n",
      "Current batch training loss: 2.004826  [204800/2213439]\n",
      "Current batch training loss: 1.935434  [230400/2213439]\n",
      "Current batch training loss: 1.931389  [256000/2213439]\n",
      "Current batch training loss: 1.845670  [281600/2213439]\n",
      "Current batch training loss: 1.869758  [307200/2213439]\n",
      "Current batch training loss: 1.809023  [332800/2213439]\n",
      "Current batch training loss: 1.764094  [358400/2213439]\n",
      "Current batch training loss: 1.759223  [384000/2213439]\n",
      "Current batch training loss: 1.627854  [409600/2213439]\n",
      "Current batch training loss: 1.823593  [435200/2213439]\n",
      "Current batch training loss: 1.678430  [460800/2213439]\n",
      "Current batch training loss: 1.489271  [486400/2213439]\n",
      "Current batch training loss: 1.567323  [512000/2213439]\n",
      "Current batch training loss: 1.543320  [537600/2213439]\n",
      "Current batch training loss: 1.483623  [563200/2213439]\n",
      "Current batch training loss: 1.479194  [588800/2213439]\n",
      "Current batch training loss: 1.307533  [614400/2213439]\n",
      "Current batch training loss: 1.396764  [640000/2213439]\n",
      "Current batch training loss: 1.402986  [665600/2213439]\n",
      "Current batch training loss: 1.234149  [691200/2213439]\n",
      "Current batch training loss: 1.314115  [716800/2213439]\n",
      "Current batch training loss: 1.322970  [742400/2213439]\n",
      "Current batch training loss: 1.158103  [768000/2213439]\n",
      "Current batch training loss: 1.112450  [793600/2213439]\n",
      "Current batch training loss: 1.068744  [819200/2213439]\n",
      "Current batch training loss: 1.085958  [844800/2213439]\n",
      "Current batch training loss: 1.006930  [870400/2213439]\n",
      "Current batch training loss: 1.015019  [896000/2213439]\n",
      "Current batch training loss: 1.064375  [921600/2213439]\n",
      "Current batch training loss: 1.107867  [947200/2213439]\n",
      "Current batch training loss: 1.109988  [972800/2213439]\n",
      "Current batch training loss: 0.900006  [998400/2213439]\n",
      "Current batch training loss: 0.937641  [1024000/2213439]\n",
      "Current batch training loss: 0.965008  [1049600/2213439]\n",
      "Current batch training loss: 0.935356  [1075200/2213439]\n",
      "Current batch training loss: 1.107110  [1100800/2213439]\n",
      "Current batch training loss: 1.109628  [1126400/2213439]\n",
      "Current batch training loss: 0.912159  [1152000/2213439]\n",
      "Current batch training loss: 0.883073  [1177600/2213439]\n",
      "Current batch training loss: 0.954425  [1203200/2213439]\n",
      "Current batch training loss: 0.679717  [1228800/2213439]\n",
      "Current batch training loss: 0.950148  [1254400/2213439]\n",
      "Current batch training loss: 0.859920  [1280000/2213439]\n",
      "Current batch training loss: 0.944804  [1305600/2213439]\n",
      "Current batch training loss: 0.842425  [1331200/2213439]\n",
      "Current batch training loss: 0.956115  [1356800/2213439]\n",
      "Current batch training loss: 0.883191  [1382400/2213439]\n",
      "Current batch training loss: 0.753088  [1408000/2213439]\n",
      "Current batch training loss: 0.788908  [1433600/2213439]\n",
      "Current batch training loss: 0.630745  [1459200/2213439]\n",
      "Current batch training loss: 0.667499  [1484800/2213439]\n",
      "Current batch training loss: 0.729416  [1510400/2213439]\n",
      "Current batch training loss: 0.849850  [1536000/2213439]\n",
      "Current batch training loss: 0.964621  [1561600/2213439]\n",
      "Current batch training loss: 0.767549  [1587200/2213439]\n",
      "Current batch training loss: 0.695951  [1612800/2213439]\n",
      "Current batch training loss: 0.778075  [1638400/2213439]\n",
      "Current batch training loss: 0.711974  [1664000/2213439]\n",
      "Current batch training loss: 0.707226  [1689600/2213439]\n",
      "Current batch training loss: 0.619844  [1715200/2213439]\n",
      "Current batch training loss: 0.581551  [1740800/2213439]\n",
      "Current batch training loss: 0.694969  [1766400/2213439]\n",
      "Current batch training loss: 0.773954  [1792000/2213439]\n",
      "Current batch training loss: 0.559023  [1817600/2213439]\n",
      "Current batch training loss: 0.514043  [1843200/2213439]\n",
      "Current batch training loss: 0.526645  [1868800/2213439]\n",
      "Current batch training loss: 0.608068  [1894400/2213439]\n",
      "Current batch training loss: 0.664391  [1920000/2213439]\n",
      "Current batch training loss: 0.394918  [1945600/2213439]\n",
      "Current batch training loss: 0.471658  [1971200/2213439]\n",
      "Current batch training loss: 0.764140  [1996800/2213439]\n",
      "Current batch training loss: 0.578940  [2022400/2213439]\n",
      "Current batch training loss: 0.492322  [2048000/2213439]\n",
      "Current batch training loss: 0.650734  [2073600/2213439]\n",
      "Current batch training loss: 0.506552  [2099200/2213439]\n",
      "Current batch training loss: 0.587539  [2124800/2213439]\n",
      "Current batch training loss: 0.651290  [2150400/2213439]\n",
      "Current batch training loss: 0.439373  [2176000/2213439]\n",
      "Current batch training loss: 0.430071  [2201600/2213439]\n",
      "Training loss: 1.118808\n",
      "Validation loss: 0.503200 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Current batch training loss: 0.364542  [    0/2213439]\n",
      "Current batch training loss: 0.377252  [25600/2213439]\n",
      "Current batch training loss: 0.633082  [51200/2213439]\n",
      "Current batch training loss: 0.383134  [76800/2213439]\n",
      "Current batch training loss: 0.561683  [102400/2213439]\n",
      "Current batch training loss: 0.290507  [128000/2213439]\n",
      "Current batch training loss: 0.527048  [153600/2213439]\n",
      "Current batch training loss: 0.588778  [179200/2213439]\n",
      "Current batch training loss: 0.473758  [204800/2213439]\n",
      "Current batch training loss: 0.372418  [230400/2213439]\n",
      "Current batch training loss: 0.211472  [256000/2213439]\n",
      "Current batch training loss: 0.458690  [281600/2213439]\n",
      "Current batch training loss: 0.325789  [307200/2213439]\n",
      "Current batch training loss: 0.532857  [332800/2213439]\n",
      "Current batch training loss: 0.191881  [358400/2213439]\n",
      "Current batch training loss: 0.435582  [384000/2213439]\n",
      "Current batch training loss: 0.450186  [409600/2213439]\n",
      "Current batch training loss: 0.397680  [435200/2213439]\n",
      "Current batch training loss: 0.459176  [460800/2213439]\n",
      "Current batch training loss: 0.630306  [486400/2213439]\n",
      "Current batch training loss: 0.349028  [512000/2213439]\n",
      "Current batch training loss: 0.241424  [537600/2213439]\n",
      "Current batch training loss: 0.447165  [563200/2213439]\n",
      "Current batch training loss: 0.345863  [588800/2213439]\n",
      "Current batch training loss: 0.378389  [614400/2213439]\n",
      "Current batch training loss: 0.435444  [640000/2213439]\n",
      "Current batch training loss: 0.261190  [665600/2213439]\n",
      "Current batch training loss: 0.471449  [691200/2213439]\n",
      "Current batch training loss: 0.372768  [716800/2213439]\n",
      "Current batch training loss: 0.470346  [742400/2213439]\n",
      "Current batch training loss: 0.569309  [768000/2213439]\n",
      "Current batch training loss: 0.326030  [793600/2213439]\n",
      "Current batch training loss: 0.333779  [819200/2213439]\n",
      "Current batch training loss: 0.317972  [844800/2213439]\n",
      "Current batch training loss: 0.454061  [870400/2213439]\n",
      "Current batch training loss: 0.324719  [896000/2213439]\n",
      "Current batch training loss: 0.417324  [921600/2213439]\n",
      "Current batch training loss: 0.357009  [947200/2213439]\n",
      "Current batch training loss: 0.283517  [972800/2213439]\n",
      "Current batch training loss: 0.343509  [998400/2213439]\n",
      "Current batch training loss: 0.451729  [1024000/2213439]\n",
      "Current batch training loss: 0.185001  [1049600/2213439]\n",
      "Current batch training loss: 0.228323  [1075200/2213439]\n",
      "Current batch training loss: 0.239570  [1100800/2213439]\n",
      "Current batch training loss: 0.684711  [1126400/2213439]\n",
      "Current batch training loss: 0.242641  [1152000/2213439]\n",
      "Current batch training loss: 0.233153  [1177600/2213439]\n",
      "Current batch training loss: 0.453398  [1203200/2213439]\n",
      "Current batch training loss: 0.358255  [1228800/2213439]\n",
      "Current batch training loss: 0.256497  [1254400/2213439]\n",
      "Current batch training loss: 0.600488  [1280000/2213439]\n",
      "Current batch training loss: 0.228568  [1305600/2213439]\n",
      "Current batch training loss: 0.424696  [1331200/2213439]\n",
      "Current batch training loss: 0.320151  [1356800/2213439]\n",
      "Current batch training loss: 0.312429  [1382400/2213439]\n",
      "Current batch training loss: 0.207922  [1408000/2213439]\n",
      "Current batch training loss: 0.119918  [1433600/2213439]\n",
      "Current batch training loss: 0.435907  [1459200/2213439]\n",
      "Current batch training loss: 0.174067  [1484800/2213439]\n",
      "Current batch training loss: 0.168592  [1510400/2213439]\n",
      "Current batch training loss: 0.239559  [1536000/2213439]\n",
      "Current batch training loss: 0.359946  [1561600/2213439]\n",
      "Current batch training loss: 0.118432  [1587200/2213439]\n",
      "Current batch training loss: 0.125770  [1612800/2213439]\n",
      "Current batch training loss: 0.128383  [1638400/2213439]\n",
      "Current batch training loss: 0.186565  [1664000/2213439]\n",
      "Current batch training loss: 0.348682  [1689600/2213439]\n",
      "Current batch training loss: 0.147771  [1715200/2213439]\n",
      "Current batch training loss: 0.170828  [1740800/2213439]\n",
      "Current batch training loss: 0.329673  [1766400/2213439]\n",
      "Current batch training loss: 0.268334  [1792000/2213439]\n",
      "Current batch training loss: 0.051782  [1817600/2213439]\n",
      "Current batch training loss: 0.186650  [1843200/2213439]\n",
      "Current batch training loss: 0.214741  [1868800/2213439]\n",
      "Current batch training loss: 0.203298  [1894400/2213439]\n",
      "Current batch training loss: 0.269300  [1920000/2213439]\n",
      "Current batch training loss: -0.077141  [1945600/2213439]\n",
      "Current batch training loss: 0.617418  [1971200/2213439]\n",
      "Current batch training loss: 0.273951  [1996800/2213439]\n",
      "Current batch training loss: 0.189505  [2022400/2213439]\n",
      "Current batch training loss: 0.175740  [2048000/2213439]\n",
      "Current batch training loss: 0.332818  [2073600/2213439]\n",
      "Current batch training loss: 0.304747  [2099200/2213439]\n",
      "Current batch training loss: 0.497832  [2124800/2213439]\n",
      "Current batch training loss: 0.260341  [2150400/2213439]\n",
      "Current batch training loss: 0.093414  [2176000/2213439]\n",
      "Current batch training loss: 0.198569  [2201600/2213439]\n",
      "Training loss: 0.334665\n",
      "Validation loss: 0.252044 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Current batch training loss: 0.128899  [    0/2213439]\n",
      "Current batch training loss: 0.102692  [25600/2213439]\n",
      "Current batch training loss: 0.320218  [51200/2213439]\n",
      "Current batch training loss: 0.243240  [76800/2213439]\n",
      "Current batch training loss: 0.290349  [102400/2213439]\n",
      "Current batch training loss: 0.294239  [128000/2213439]\n",
      "Current batch training loss: 0.178725  [153600/2213439]\n",
      "Current batch training loss: 0.020228  [179200/2213439]\n",
      "Current batch training loss: 0.229738  [204800/2213439]\n",
      "Current batch training loss: 0.283120  [230400/2213439]\n",
      "Current batch training loss: 0.340178  [256000/2213439]\n",
      "Current batch training loss: 0.163817  [281600/2213439]\n",
      "Current batch training loss: 0.203892  [307200/2213439]\n",
      "Current batch training loss: 0.193540  [332800/2213439]\n",
      "Current batch training loss: 0.527228  [358400/2213439]\n",
      "Current batch training loss: 0.250832  [384000/2213439]\n",
      "Current batch training loss: 0.167299  [409600/2213439]\n",
      "Current batch training loss: 0.046467  [435200/2213439]\n",
      "Current batch training loss: 0.427715  [460800/2213439]\n",
      "Current batch training loss: 0.303132  [486400/2213439]\n",
      "Current batch training loss: 0.214034  [512000/2213439]\n",
      "Current batch training loss: 0.287055  [537600/2213439]\n",
      "Current batch training loss: 0.323333  [563200/2213439]\n",
      "Current batch training loss: 0.342493  [588800/2213439]\n",
      "Current batch training loss: 0.070503  [614400/2213439]\n",
      "Current batch training loss: 0.359667  [640000/2213439]\n",
      "Current batch training loss: 0.271742  [665600/2213439]\n",
      "Current batch training loss: 0.083152  [691200/2213439]\n",
      "Current batch training loss: 0.220166  [716800/2213439]\n",
      "Current batch training loss: 0.146825  [742400/2213439]\n",
      "Current batch training loss: 0.224825  [768000/2213439]\n",
      "Current batch training loss: 0.378768  [793600/2213439]\n",
      "Current batch training loss: 0.070416  [819200/2213439]\n",
      "Current batch training loss: 0.466628  [844800/2213439]\n",
      "Current batch training loss: 0.210403  [870400/2213439]\n",
      "Current batch training loss: 0.328489  [896000/2213439]\n",
      "Current batch training loss: 0.303110  [921600/2213439]\n",
      "Current batch training loss: 0.065030  [947200/2213439]\n",
      "Current batch training loss: 0.603831  [972800/2213439]\n",
      "Current batch training loss: 0.396935  [998400/2213439]\n",
      "Current batch training loss: 0.012334  [1024000/2213439]\n",
      "Current batch training loss: 0.110492  [1049600/2213439]\n",
      "Current batch training loss: 0.068013  [1075200/2213439]\n",
      "Current batch training loss: -0.013787  [1100800/2213439]\n",
      "Current batch training loss: 0.095742  [1126400/2213439]\n",
      "Current batch training loss: 0.428524  [1152000/2213439]\n",
      "Current batch training loss: 0.109551  [1177600/2213439]\n",
      "Current batch training loss: 0.332097  [1203200/2213439]\n",
      "Current batch training loss: 0.255212  [1228800/2213439]\n",
      "Current batch training loss: 0.337781  [1254400/2213439]\n",
      "Current batch training loss: 0.341526  [1280000/2213439]\n",
      "Current batch training loss: 0.250759  [1305600/2213439]\n",
      "Current batch training loss: 0.336612  [1331200/2213439]\n",
      "Current batch training loss: 0.104830  [1356800/2213439]\n",
      "Current batch training loss: 0.136812  [1382400/2213439]\n",
      "Current batch training loss: 0.152957  [1408000/2213439]\n",
      "Current batch training loss: 0.306351  [1433600/2213439]\n",
      "Current batch training loss: -0.014022  [1459200/2213439]\n",
      "Current batch training loss: -0.064204  [1484800/2213439]\n",
      "Current batch training loss: 0.080023  [1510400/2213439]\n",
      "Current batch training loss: 0.271971  [1536000/2213439]\n",
      "Current batch training loss: 0.523261  [1561600/2213439]\n",
      "Current batch training loss: 0.084467  [1587200/2213439]\n",
      "Current batch training loss: 0.174034  [1612800/2213439]\n",
      "Current batch training loss: 0.022286  [1638400/2213439]\n",
      "Current batch training loss: 0.146118  [1664000/2213439]\n",
      "Current batch training loss: 0.245350  [1689600/2213439]\n",
      "Current batch training loss: 0.082970  [1715200/2213439]\n",
      "Current batch training loss: 0.288116  [1740800/2213439]\n",
      "Current batch training loss: 0.066771  [1766400/2213439]\n",
      "Current batch training loss: 0.435170  [1792000/2213439]\n",
      "Current batch training loss: 0.072575  [1817600/2213439]\n",
      "Current batch training loss: 0.156089  [1843200/2213439]\n",
      "Current batch training loss: 0.325531  [1868800/2213439]\n",
      "Current batch training loss: 0.215841  [1894400/2213439]\n",
      "Current batch training loss: 0.212650  [1920000/2213439]\n",
      "Current batch training loss: 0.199703  [1945600/2213439]\n",
      "Current batch training loss: 0.098081  [1971200/2213439]\n",
      "Current batch training loss: 0.114736  [1996800/2213439]\n",
      "Current batch training loss: 0.306001  [2022400/2213439]\n",
      "Current batch training loss: 0.120750  [2048000/2213439]\n",
      "Current batch training loss: 0.079403  [2073600/2213439]\n",
      "Current batch training loss: 0.398981  [2099200/2213439]\n",
      "Current batch training loss: 0.263058  [2124800/2213439]\n",
      "Current batch training loss: 0.104311  [2150400/2213439]\n",
      "Current batch training loss: 0.238724  [2176000/2213439]\n",
      "Current batch training loss: 0.063690  [2201600/2213439]\n",
      "Training loss: 0.216197\n",
      "Validation loss: 0.210615 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Current batch training loss: 0.219734  [    0/2213439]\n",
      "Current batch training loss: 0.334400  [25600/2213439]\n",
      "Current batch training loss: 0.171699  [51200/2213439]\n",
      "Current batch training loss: 0.043248  [76800/2213439]\n",
      "Current batch training loss: 0.247178  [102400/2213439]\n",
      "Current batch training loss: 0.258758  [128000/2213439]\n",
      "Current batch training loss: 0.476729  [153600/2213439]\n",
      "Current batch training loss: 0.423142  [179200/2213439]\n",
      "Current batch training loss: 0.185892  [204800/2213439]\n",
      "Current batch training loss: 0.144913  [230400/2213439]\n",
      "Current batch training loss: 0.079612  [256000/2213439]\n",
      "Current batch training loss: -0.029253  [281600/2213439]\n",
      "Current batch training loss: 0.133937  [307200/2213439]\n",
      "Current batch training loss: 0.298113  [332800/2213439]\n",
      "Current batch training loss: 0.212893  [358400/2213439]\n",
      "Current batch training loss: -0.013119  [384000/2213439]\n",
      "Current batch training loss: 0.214965  [409600/2213439]\n",
      "Current batch training loss: 0.160463  [435200/2213439]\n",
      "Current batch training loss: 0.346816  [460800/2213439]\n",
      "Current batch training loss: 0.562328  [486400/2213439]\n",
      "Current batch training loss: 0.052271  [512000/2213439]\n",
      "Current batch training loss: 0.234019  [537600/2213439]\n",
      "Current batch training loss: 0.135353  [563200/2213439]\n",
      "Current batch training loss: 0.305679  [588800/2213439]\n",
      "Current batch training loss: 0.180146  [614400/2213439]\n",
      "Current batch training loss: 0.249646  [640000/2213439]\n",
      "Current batch training loss: 0.180715  [665600/2213439]\n",
      "Current batch training loss: 0.017865  [691200/2213439]\n",
      "Current batch training loss: -0.026481  [716800/2213439]\n",
      "Current batch training loss: 0.076207  [742400/2213439]\n",
      "Current batch training loss: 0.096222  [768000/2213439]\n",
      "Current batch training loss: 0.288001  [793600/2213439]\n",
      "Current batch training loss: 0.350784  [819200/2213439]\n",
      "Current batch training loss: 0.140914  [844800/2213439]\n",
      "Current batch training loss: 0.457085  [870400/2213439]\n",
      "Current batch training loss: 0.272568  [896000/2213439]\n",
      "Current batch training loss: 0.124356  [921600/2213439]\n",
      "Current batch training loss: 0.171417  [947200/2213439]\n",
      "Current batch training loss: 0.404303  [972800/2213439]\n",
      "Current batch training loss: 0.154326  [998400/2213439]\n",
      "Current batch training loss: 0.208079  [1024000/2213439]\n",
      "Current batch training loss: 0.127344  [1049600/2213439]\n",
      "Current batch training loss: -0.087764  [1075200/2213439]\n",
      "Current batch training loss: 0.063158  [1100800/2213439]\n",
      "Current batch training loss: 0.259824  [1126400/2213439]\n",
      "Current batch training loss: 0.014290  [1152000/2213439]\n",
      "Current batch training loss: 0.460490  [1177600/2213439]\n",
      "Current batch training loss: 0.256080  [1203200/2213439]\n",
      "Current batch training loss: 0.328484  [1228800/2213439]\n",
      "Current batch training loss: 0.116815  [1254400/2213439]\n",
      "Current batch training loss: 0.297154  [1280000/2213439]\n",
      "Current batch training loss: 0.193876  [1305600/2213439]\n",
      "Current batch training loss: 0.238521  [1331200/2213439]\n",
      "Current batch training loss: 0.271561  [1356800/2213439]\n",
      "Current batch training loss: 0.174696  [1382400/2213439]\n",
      "Current batch training loss: 0.027923  [1408000/2213439]\n",
      "Current batch training loss: 0.067838  [1433600/2213439]\n",
      "Current batch training loss: 0.110294  [1459200/2213439]\n",
      "Current batch training loss: 0.256792  [1484800/2213439]\n",
      "Current batch training loss: 0.284217  [1510400/2213439]\n",
      "Current batch training loss: 0.146600  [1536000/2213439]\n",
      "Current batch training loss: 0.413643  [1561600/2213439]\n",
      "Current batch training loss: 0.185960  [1587200/2213439]\n",
      "Current batch training loss: 0.128922  [1612800/2213439]\n",
      "Current batch training loss: 0.098129  [1638400/2213439]\n",
      "Current batch training loss: 0.071431  [1664000/2213439]\n",
      "Current batch training loss: 0.390058  [1689600/2213439]\n",
      "Current batch training loss: 0.167374  [1715200/2213439]\n",
      "Current batch training loss: 0.226071  [1740800/2213439]\n",
      "Current batch training loss: 0.157628  [1766400/2213439]\n",
      "Current batch training loss: 0.285834  [1792000/2213439]\n",
      "Current batch training loss: 0.110823  [1817600/2213439]\n",
      "Current batch training loss: 0.226972  [1843200/2213439]\n",
      "Current batch training loss: 0.066658  [1868800/2213439]\n",
      "Current batch training loss: 0.145102  [1894400/2213439]\n",
      "Current batch training loss: 0.163096  [1920000/2213439]\n",
      "Current batch training loss: 0.196000  [1945600/2213439]\n",
      "Current batch training loss: 0.095637  [1971200/2213439]\n",
      "Current batch training loss: 0.229687  [1996800/2213439]\n",
      "Current batch training loss: -0.012902  [2022400/2213439]\n",
      "Current batch training loss: 0.272643  [2048000/2213439]\n",
      "Current batch training loss: 0.126922  [2073600/2213439]\n",
      "Current batch training loss: 0.231370  [2099200/2213439]\n",
      "Current batch training loss: 0.331530  [2124800/2213439]\n",
      "Current batch training loss: 0.052389  [2150400/2213439]\n",
      "Current batch training loss: 0.142348  [2176000/2213439]\n",
      "Current batch training loss: 0.178913  [2201600/2213439]\n",
      "Training loss: 0.174169\n",
      "Validation loss: 0.193716 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Current batch training loss: 0.124618  [    0/2213439]\n",
      "Current batch training loss: -0.046990  [25600/2213439]\n",
      "Current batch training loss: -0.032837  [51200/2213439]\n",
      "Current batch training loss: 0.100526  [76800/2213439]\n",
      "Current batch training loss: -0.061455  [102400/2213439]\n",
      "Current batch training loss: -0.035152  [128000/2213439]\n",
      "Current batch training loss: 0.140809  [153600/2213439]\n",
      "Current batch training loss: -0.010119  [179200/2213439]\n",
      "Current batch training loss: 0.168028  [204800/2213439]\n",
      "Current batch training loss: 0.219828  [230400/2213439]\n",
      "Current batch training loss: 0.256233  [256000/2213439]\n",
      "Current batch training loss: 0.204769  [281600/2213439]\n",
      "Current batch training loss: -0.001868  [307200/2213439]\n",
      "Current batch training loss: 0.111849  [332800/2213439]\n",
      "Current batch training loss: 0.120121  [358400/2213439]\n",
      "Current batch training loss: 0.102684  [384000/2213439]\n",
      "Current batch training loss: 0.049123  [409600/2213439]\n",
      "Current batch training loss: 0.277625  [435200/2213439]\n",
      "Current batch training loss: 0.257614  [460800/2213439]\n",
      "Current batch training loss: 0.135649  [486400/2213439]\n",
      "Current batch training loss: 0.055973  [512000/2213439]\n",
      "Current batch training loss: 0.135547  [537600/2213439]\n",
      "Current batch training loss: 0.218749  [563200/2213439]\n",
      "Current batch training loss: 0.162718  [588800/2213439]\n",
      "Current batch training loss: 0.092397  [614400/2213439]\n",
      "Current batch training loss: -0.015851  [640000/2213439]\n",
      "Current batch training loss: 0.185905  [665600/2213439]\n",
      "Current batch training loss: 0.161720  [691200/2213439]\n",
      "Current batch training loss: 0.119757  [716800/2213439]\n",
      "Current batch training loss: 0.214814  [742400/2213439]\n",
      "Current batch training loss: 0.004003  [768000/2213439]\n",
      "Current batch training loss: 0.289525  [793600/2213439]\n",
      "Current batch training loss: 0.382971  [819200/2213439]\n",
      "Current batch training loss: 0.088744  [844800/2213439]\n",
      "Current batch training loss: 0.205940  [870400/2213439]\n",
      "Current batch training loss: 0.336370  [896000/2213439]\n",
      "Current batch training loss: 0.011559  [921600/2213439]\n",
      "Current batch training loss: 0.095312  [947200/2213439]\n",
      "Current batch training loss: 0.190230  [972800/2213439]\n",
      "Current batch training loss: 0.226835  [998400/2213439]\n",
      "Current batch training loss: 0.114913  [1024000/2213439]\n",
      "Current batch training loss: 0.196711  [1049600/2213439]\n",
      "Current batch training loss: 0.234841  [1075200/2213439]\n",
      "Current batch training loss: 0.141696  [1100800/2213439]\n",
      "Current batch training loss: 0.084455  [1126400/2213439]\n",
      "Current batch training loss: 0.005323  [1152000/2213439]\n",
      "Current batch training loss: -0.032296  [1177600/2213439]\n",
      "Current batch training loss: 0.186443  [1203200/2213439]\n",
      "Current batch training loss: 0.041330  [1228800/2213439]\n",
      "Current batch training loss: -0.122158  [1254400/2213439]\n",
      "Current batch training loss: 0.015746  [1280000/2213439]\n",
      "Current batch training loss: 0.031238  [1305600/2213439]\n",
      "Current batch training loss: 0.294790  [1331200/2213439]\n",
      "Current batch training loss: 0.153810  [1356800/2213439]\n",
      "Current batch training loss: 0.169559  [1382400/2213439]\n",
      "Current batch training loss: 0.070499  [1408000/2213439]\n",
      "Current batch training loss: 0.325374  [1433600/2213439]\n",
      "Current batch training loss: 0.364205  [1459200/2213439]\n",
      "Current batch training loss: 0.185964  [1484800/2213439]\n",
      "Current batch training loss: 0.206368  [1510400/2213439]\n",
      "Current batch training loss: 0.156570  [1536000/2213439]\n",
      "Current batch training loss: -0.046885  [1561600/2213439]\n",
      "Current batch training loss: 0.134183  [1587200/2213439]\n",
      "Current batch training loss: 0.014543  [1612800/2213439]\n",
      "Current batch training loss: 0.167389  [1638400/2213439]\n",
      "Current batch training loss: 0.318206  [1664000/2213439]\n",
      "Current batch training loss: 0.141347  [1689600/2213439]\n",
      "Current batch training loss: 0.360494  [1715200/2213439]\n",
      "Current batch training loss: 0.018620  [1740800/2213439]\n",
      "Current batch training loss: 0.112120  [1766400/2213439]\n",
      "Current batch training loss: 0.096539  [1792000/2213439]\n",
      "Current batch training loss: 0.122282  [1817600/2213439]\n",
      "Current batch training loss: 0.091815  [1843200/2213439]\n",
      "Current batch training loss: 0.203952  [1868800/2213439]\n",
      "Current batch training loss: 0.318321  [1894400/2213439]\n",
      "Current batch training loss: 0.077012  [1920000/2213439]\n",
      "Current batch training loss: 0.006289  [1945600/2213439]\n",
      "Current batch training loss: 0.224201  [1971200/2213439]\n",
      "Current batch training loss: 0.237566  [1996800/2213439]\n",
      "Current batch training loss: 0.090743  [2022400/2213439]\n",
      "Current batch training loss: 0.242250  [2048000/2213439]\n",
      "Current batch training loss: 0.016977  [2073600/2213439]\n",
      "Current batch training loss: 0.334865  [2099200/2213439]\n",
      "Current batch training loss: 0.231054  [2124800/2213439]\n",
      "Current batch training loss: 0.292645  [2150400/2213439]\n",
      "Current batch training loss: 0.170909  [2176000/2213439]\n",
      "Current batch training loss: 0.182485  [2201600/2213439]\n",
      "Training loss: 0.147645\n",
      "Validation loss: 0.186411 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Current batch training loss: 0.215648  [    0/2213439]\n",
      "Current batch training loss: 0.187548  [25600/2213439]\n",
      "Current batch training loss: 0.257048  [51200/2213439]\n",
      "Current batch training loss: -0.001642  [76800/2213439]\n",
      "Current batch training loss: 0.064095  [102400/2213439]\n",
      "Current batch training loss: 0.076209  [128000/2213439]\n",
      "Current batch training loss: 0.024119  [153600/2213439]\n",
      "Current batch training loss: 0.060482  [179200/2213439]\n",
      "Current batch training loss: -0.112234  [204800/2213439]\n",
      "Current batch training loss: 0.081889  [230400/2213439]\n",
      "Current batch training loss: 0.169581  [256000/2213439]\n",
      "Current batch training loss: 0.276980  [281600/2213439]\n",
      "Current batch training loss: -0.086730  [307200/2213439]\n",
      "Current batch training loss: 0.217864  [332800/2213439]\n",
      "Current batch training loss: 0.260140  [358400/2213439]\n",
      "Current batch training loss: 0.117065  [384000/2213439]\n",
      "Current batch training loss: 0.113079  [409600/2213439]\n",
      "Current batch training loss: 0.156023  [435200/2213439]\n",
      "Current batch training loss: 0.324383  [460800/2213439]\n",
      "Current batch training loss: -0.041690  [486400/2213439]\n",
      "Current batch training loss: 0.259524  [512000/2213439]\n",
      "Current batch training loss: 0.019701  [537600/2213439]\n",
      "Current batch training loss: 0.181008  [563200/2213439]\n",
      "Current batch training loss: 0.109301  [588800/2213439]\n",
      "Current batch training loss: -0.007241  [614400/2213439]\n",
      "Current batch training loss: 0.004616  [640000/2213439]\n",
      "Current batch training loss: 0.016728  [665600/2213439]\n",
      "Current batch training loss: 0.428012  [691200/2213439]\n",
      "Current batch training loss: -0.020073  [716800/2213439]\n",
      "Current batch training loss: 0.133382  [742400/2213439]\n",
      "Current batch training loss: 0.074162  [768000/2213439]\n",
      "Current batch training loss: 0.112644  [793600/2213439]\n",
      "Current batch training loss: 0.028981  [819200/2213439]\n",
      "Current batch training loss: 0.252274  [844800/2213439]\n",
      "Current batch training loss: 0.343633  [870400/2213439]\n",
      "Current batch training loss: 0.187992  [896000/2213439]\n",
      "Current batch training loss: -0.053145  [921600/2213439]\n",
      "Current batch training loss: 0.112805  [947200/2213439]\n",
      "Current batch training loss: 0.190601  [972800/2213439]\n",
      "Current batch training loss: 0.000586  [998400/2213439]\n",
      "Current batch training loss: -0.044446  [1024000/2213439]\n",
      "Current batch training loss: 0.031501  [1049600/2213439]\n",
      "Current batch training loss: 0.154468  [1075200/2213439]\n",
      "Current batch training loss: 0.112432  [1100800/2213439]\n",
      "Current batch training loss: 0.047671  [1126400/2213439]\n",
      "Current batch training loss: 0.038133  [1152000/2213439]\n",
      "Current batch training loss: 0.237556  [1177600/2213439]\n",
      "Current batch training loss: 0.063596  [1203200/2213439]\n",
      "Current batch training loss: 0.259143  [1228800/2213439]\n",
      "Current batch training loss: 0.100303  [1254400/2213439]\n",
      "Current batch training loss: 0.042237  [1280000/2213439]\n",
      "Current batch training loss: 0.224975  [1305600/2213439]\n",
      "Current batch training loss: 0.074521  [1331200/2213439]\n",
      "Current batch training loss: 0.263790  [1356800/2213439]\n",
      "Current batch training loss: 0.197435  [1382400/2213439]\n",
      "Current batch training loss: 0.129858  [1408000/2213439]\n",
      "Current batch training loss: 0.193867  [1433600/2213439]\n",
      "Current batch training loss: 0.114834  [1459200/2213439]\n",
      "Current batch training loss: 0.045425  [1484800/2213439]\n",
      "Current batch training loss: 0.106155  [1510400/2213439]\n",
      "Current batch training loss: 0.012192  [1536000/2213439]\n",
      "Current batch training loss: 0.149305  [1561600/2213439]\n",
      "Current batch training loss: 0.234485  [1587200/2213439]\n",
      "Current batch training loss: 0.203516  [1612800/2213439]\n",
      "Current batch training loss: 0.285732  [1638400/2213439]\n",
      "Current batch training loss: 0.182171  [1664000/2213439]\n",
      "Current batch training loss: 0.193369  [1689600/2213439]\n",
      "Current batch training loss: 0.191486  [1715200/2213439]\n",
      "Current batch training loss: 0.201639  [1740800/2213439]\n",
      "Current batch training loss: 0.007608  [1766400/2213439]\n",
      "Current batch training loss: 0.051923  [1792000/2213439]\n",
      "Current batch training loss: 0.121447  [1817600/2213439]\n",
      "Current batch training loss: -0.111673  [1843200/2213439]\n",
      "Current batch training loss: 0.236431  [1868800/2213439]\n",
      "Current batch training loss: 0.291990  [1894400/2213439]\n",
      "Current batch training loss: -0.031606  [1920000/2213439]\n",
      "Current batch training loss: 0.113495  [1945600/2213439]\n",
      "Current batch training loss: -0.020133  [1971200/2213439]\n",
      "Current batch training loss: 0.359551  [1996800/2213439]\n",
      "Current batch training loss: 0.201382  [2022400/2213439]\n",
      "Current batch training loss: 0.020971  [2048000/2213439]\n",
      "Current batch training loss: 0.203580  [2073600/2213439]\n",
      "Current batch training loss: 0.118835  [2099200/2213439]\n",
      "Current batch training loss: 0.066003  [2124800/2213439]\n",
      "Current batch training loss: 0.139137  [2150400/2213439]\n",
      "Current batch training loss: 0.179962  [2176000/2213439]\n",
      "Current batch training loss: 0.137820  [2201600/2213439]\n",
      "Training loss: 0.126540\n",
      "Validation loss: 0.191743 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Current batch training loss: 0.157108  [    0/2213439]\n",
      "Current batch training loss: 0.069429  [25600/2213439]\n",
      "Current batch training loss: 0.246109  [51200/2213439]\n",
      "Current batch training loss: -0.051978  [76800/2213439]\n",
      "Current batch training loss: 0.251849  [102400/2213439]\n",
      "Current batch training loss: 0.173945  [128000/2213439]\n",
      "Current batch training loss: 0.132051  [153600/2213439]\n",
      "Current batch training loss: 0.035212  [179200/2213439]\n",
      "Current batch training loss: 0.186870  [204800/2213439]\n",
      "Current batch training loss: 0.184783  [230400/2213439]\n",
      "Current batch training loss: 0.310120  [256000/2213439]\n",
      "Current batch training loss: 0.095500  [281600/2213439]\n",
      "Current batch training loss: 0.215798  [307200/2213439]\n",
      "Current batch training loss: 0.368847  [332800/2213439]\n",
      "Current batch training loss: 0.240537  [358400/2213439]\n",
      "Current batch training loss: 0.207580  [384000/2213439]\n",
      "Current batch training loss: 0.122535  [409600/2213439]\n",
      "Current batch training loss: 0.106571  [435200/2213439]\n",
      "Current batch training loss: 0.015757  [460800/2213439]\n",
      "Current batch training loss: -0.102926  [486400/2213439]\n",
      "Current batch training loss: 0.095455  [512000/2213439]\n",
      "Current batch training loss: 0.286604  [537600/2213439]\n",
      "Current batch training loss: 0.094092  [563200/2213439]\n",
      "Current batch training loss: -0.033375  [588800/2213439]\n",
      "Current batch training loss: 0.177274  [614400/2213439]\n",
      "Current batch training loss: 0.095335  [640000/2213439]\n",
      "Current batch training loss: -0.010636  [665600/2213439]\n",
      "Current batch training loss: 0.199669  [691200/2213439]\n",
      "Current batch training loss: 0.006459  [716800/2213439]\n",
      "Current batch training loss: 0.175766  [742400/2213439]\n",
      "Current batch training loss: 0.095550  [768000/2213439]\n",
      "Current batch training loss: 0.045530  [793600/2213439]\n",
      "Current batch training loss: 0.002530  [819200/2213439]\n",
      "Current batch training loss: 0.061920  [844800/2213439]\n",
      "Current batch training loss: 0.075855  [870400/2213439]\n",
      "Current batch training loss: 0.224777  [896000/2213439]\n",
      "Current batch training loss: 0.257890  [921600/2213439]\n",
      "Current batch training loss: 0.314973  [947200/2213439]\n",
      "Current batch training loss: 0.130478  [972800/2213439]\n",
      "Current batch training loss: 0.029784  [998400/2213439]\n",
      "Current batch training loss: 0.094568  [1024000/2213439]\n",
      "Current batch training loss: 0.114236  [1049600/2213439]\n",
      "Current batch training loss: 0.318108  [1075200/2213439]\n",
      "Current batch training loss: 0.223707  [1100800/2213439]\n",
      "Current batch training loss: 0.057498  [1126400/2213439]\n",
      "Current batch training loss: 0.153021  [1152000/2213439]\n",
      "Current batch training loss: 0.288790  [1177600/2213439]\n",
      "Current batch training loss: -0.079303  [1203200/2213439]\n",
      "Current batch training loss: -0.085060  [1228800/2213439]\n",
      "Current batch training loss: 0.001237  [1254400/2213439]\n",
      "Current batch training loss: 0.072247  [1280000/2213439]\n",
      "Current batch training loss: -0.008009  [1305600/2213439]\n",
      "Current batch training loss: 0.108679  [1331200/2213439]\n",
      "Current batch training loss: 0.082842  [1356800/2213439]\n",
      "Current batch training loss: -0.040195  [1382400/2213439]\n",
      "Current batch training loss: 0.017375  [1408000/2213439]\n",
      "Current batch training loss: 0.163114  [1433600/2213439]\n",
      "Current batch training loss: 0.150640  [1459200/2213439]\n",
      "Current batch training loss: 0.154064  [1484800/2213439]\n",
      "Current batch training loss: -0.021143  [1510400/2213439]\n",
      "Current batch training loss: 0.155945  [1536000/2213439]\n",
      "Current batch training loss: 0.014048  [1561600/2213439]\n",
      "Current batch training loss: 0.021888  [1587200/2213439]\n",
      "Current batch training loss: -0.079671  [1612800/2213439]\n",
      "Current batch training loss: 0.046612  [1638400/2213439]\n",
      "Current batch training loss: 0.262608  [1664000/2213439]\n",
      "Current batch training loss: 0.035328  [1689600/2213439]\n",
      "Current batch training loss: 0.063802  [1715200/2213439]\n",
      "Current batch training loss: -0.073004  [1740800/2213439]\n",
      "Current batch training loss: 0.027601  [1766400/2213439]\n",
      "Current batch training loss: -0.100204  [1792000/2213439]\n",
      "Current batch training loss: 0.017737  [1817600/2213439]\n",
      "Current batch training loss: 0.132741  [1843200/2213439]\n",
      "Current batch training loss: 0.018830  [1868800/2213439]\n",
      "Current batch training loss: -0.058013  [1894400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.009880094788968563\n",
      "Current batch training loss: 0.086930  [1920000/2213439]\n",
      "Current batch training loss: -0.020920  [1945600/2213439]\n",
      "Current batch training loss: 0.052737  [1971200/2213439]\n",
      "Current batch training loss: -0.031017  [1996800/2213439]\n",
      "Current batch training loss: -0.071170  [2022400/2213439]\n",
      "Current batch training loss: -0.030896  [2048000/2213439]\n",
      "Current batch training loss: 0.096862  [2073600/2213439]\n",
      "Current batch training loss: 0.078343  [2099200/2213439]\n",
      "Current batch training loss: 0.198445  [2124800/2213439]\n",
      "Current batch training loss: 0.057127  [2150400/2213439]\n",
      "Current batch training loss: -0.054630  [2176000/2213439]\n",
      "Current batch training loss: 0.367554  [2201600/2213439]\n",
      "Training loss: 0.108207\n",
      "Validation loss: 0.179294 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Current batch training loss: 0.022276  [    0/2213439]\n",
      "Current batch training loss: 0.047610  [25600/2213439]\n",
      "Current batch training loss: 0.043875  [51200/2213439]\n",
      "Current batch training loss: 0.099254  [76800/2213439]\n",
      "Current batch training loss: 0.201409  [102400/2213439]\n",
      "Current batch training loss: 0.241346  [128000/2213439]\n",
      "Current batch training loss: 0.101079  [153600/2213439]\n",
      "Current batch training loss: 0.306229  [179200/2213439]\n",
      "Current batch training loss: -0.029352  [204800/2213439]\n",
      "Current batch training loss: 0.051755  [230400/2213439]\n",
      "Current batch training loss: -0.110284  [256000/2213439]\n",
      "Current batch training loss: 0.203742  [281600/2213439]\n",
      "Current batch training loss: 0.128803  [307200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.13043390214443207\n",
      "Current batch training loss: 0.234774  [332800/2213439]\n",
      "Current batch training loss: 0.287244  [358400/2213439]\n",
      "Current batch training loss: 0.037221  [384000/2213439]\n",
      "Current batch training loss: 0.051291  [409600/2213439]\n",
      "Current batch training loss: 0.192603  [435200/2213439]\n",
      "Current batch training loss: 0.128854  [460800/2213439]\n",
      "Current batch training loss: 0.037251  [486400/2213439]\n",
      "Current batch training loss: 0.413266  [512000/2213439]\n",
      "Current batch training loss: 0.016110  [537600/2213439]\n",
      "Current batch training loss: 0.169654  [563200/2213439]\n",
      "Current batch training loss: 0.275341  [588800/2213439]\n",
      "Current batch training loss: 0.138488  [614400/2213439]\n",
      "Current batch training loss: 0.160488  [640000/2213439]\n",
      "Current batch training loss: -0.154056  [665600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  -0.027322374284267426\n",
      "Warning: nan gradient found. The current loss is:  0.18584299087524414\n",
      "Warning: nan gradient found. The current loss is:  -0.04521002247929573\n",
      "Warning: nan gradient found. The current loss is:  0.17899905145168304\n",
      "Warning: nan gradient found. The current loss is:  -0.048980873078107834\n",
      "Warning: nan gradient found. The current loss is:  0.18423524498939514\n",
      "Warning: nan gradient found. The current loss is:  0.16746215522289276\n",
      "Warning: nan gradient found. The current loss is:  0.09559248387813568\n",
      "Warning: nan gradient found. The current loss is:  0.05550602823495865\n",
      "Warning: nan gradient found. The current loss is:  0.24507549405097961\n",
      "Warning: nan gradient found. The current loss is:  0.28352174162864685\n",
      "Warning: nan gradient found. The current loss is:  -0.05182443559169769\n",
      "Warning: nan gradient found. The current loss is:  0.09393232315778732\n",
      "Warning: nan gradient found. The current loss is:  0.2777484059333801\n",
      "Warning: nan gradient found. The current loss is:  0.16385358572006226\n",
      "Warning: nan gradient found. The current loss is:  -0.06211105361580849\n",
      "Warning: nan gradient found. The current loss is:  0.10231541097164154\n",
      "Current batch training loss: 0.102315  [691200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.05578416585922241\n",
      "Warning: nan gradient found. The current loss is:  0.08037598431110382\n",
      "Warning: nan gradient found. The current loss is:  0.3154088854789734\n",
      "Warning: nan gradient found. The current loss is:  0.06578817963600159\n",
      "Warning: nan gradient found. The current loss is:  0.2711460590362549\n",
      "Warning: nan gradient found. The current loss is:  0.3015965223312378\n",
      "Warning: nan gradient found. The current loss is:  0.5814037322998047\n",
      "Warning: nan gradient found. The current loss is:  0.5325040817260742\n",
      "Warning: nan gradient found. The current loss is:  0.6336739659309387\n",
      "Warning: nan gradient found. The current loss is:  0.1318449229001999\n",
      "Warning: nan gradient found. The current loss is:  0.32533544301986694\n",
      "Warning: nan gradient found. The current loss is:  0.4260633587837219\n",
      "Warning: nan gradient found. The current loss is:  0.31526899337768555\n",
      "Warning: nan gradient found. The current loss is:  0.5410759449005127\n",
      "Warning: nan gradient found. The current loss is:  0.1901160627603531\n",
      "Warning: nan gradient found. The current loss is:  0.9117689728736877\n",
      "Warning: nan gradient found. The current loss is:  0.09363166987895966\n",
      "Warning: nan gradient found. The current loss is:  0.5962651968002319\n",
      "Warning: nan gradient found. The current loss is:  0.8076174259185791\n",
      "Warning: nan gradient found. The current loss is:  0.3601507544517517\n",
      "Warning: nan gradient found. The current loss is:  0.5886248350143433\n",
      "Warning: nan gradient found. The current loss is:  0.47006216645240784\n",
      "Warning: nan gradient found. The current loss is:  0.264169305562973\n",
      "Warning: nan gradient found. The current loss is:  0.6081537008285522\n",
      "Warning: nan gradient found. The current loss is:  0.13586995005607605\n",
      "Warning: nan gradient found. The current loss is:  0.380740225315094\n",
      "Warning: nan gradient found. The current loss is:  0.5042576789855957\n",
      "Warning: nan gradient found. The current loss is:  0.19382470846176147\n",
      "Warning: nan gradient found. The current loss is:  0.5769060254096985\n",
      "Warning: nan gradient found. The current loss is:  0.26752033829689026\n",
      "Warning: nan gradient found. The current loss is:  0.24649739265441895\n",
      "Warning: nan gradient found. The current loss is:  0.17789733409881592\n",
      "Warning: nan gradient found. The current loss is:  0.12662070989608765\n",
      "Warning: nan gradient found. The current loss is:  0.2946714162826538\n",
      "Warning: nan gradient found. The current loss is:  0.4036697745323181\n",
      "Warning: nan gradient found. The current loss is:  0.49261730909347534\n",
      "Warning: nan gradient found. The current loss is:  0.32299214601516724\n",
      "Warning: nan gradient found. The current loss is:  0.26946333050727844\n",
      "Warning: nan gradient found. The current loss is:  0.25855255126953125\n",
      "Warning: nan gradient found. The current loss is:  0.4420388340950012\n",
      "Warning: nan gradient found. The current loss is:  0.2736304700374603\n",
      "Warning: nan gradient found. The current loss is:  0.04602412134408951\n",
      "Warning: nan gradient found. The current loss is:  0.2418423891067505\n",
      "Warning: nan gradient found. The current loss is:  0.3858570158481598\n",
      "Warning: nan gradient found. The current loss is:  0.3685072958469391\n",
      "Warning: nan gradient found. The current loss is:  0.08160945028066635\n",
      "Warning: nan gradient found. The current loss is:  0.07343494892120361\n",
      "Warning: nan gradient found. The current loss is:  0.4949495494365692\n",
      "Warning: nan gradient found. The current loss is:  0.32417982816696167\n",
      "Warning: nan gradient found. The current loss is:  0.44212910532951355\n",
      "Warning: nan gradient found. The current loss is:  0.13352333009243011\n",
      "Warning: nan gradient found. The current loss is:  0.5890425443649292\n",
      "Warning: nan gradient found. The current loss is:  0.2694835960865021\n",
      "Warning: nan gradient found. The current loss is:  -0.18091458082199097\n",
      "Warning: nan gradient found. The current loss is:  0.6273870468139648\n",
      "Warning: nan gradient found. The current loss is:  0.2547321617603302\n",
      "Warning: nan gradient found. The current loss is:  0.5583974719047546\n",
      "Warning: nan gradient found. The current loss is:  0.24158942699432373\n",
      "Warning: nan gradient found. The current loss is:  0.9626504182815552\n",
      "Warning: nan gradient found. The current loss is:  0.13901616632938385\n",
      "Warning: nan gradient found. The current loss is:  0.606372594833374\n",
      "Warning: nan gradient found. The current loss is:  -0.06938116252422333\n",
      "Warning: nan gradient found. The current loss is:  0.6831937432289124\n",
      "Warning: nan gradient found. The current loss is:  0.14618176221847534\n",
      "Warning: nan gradient found. The current loss is:  0.17942383885383606\n",
      "Warning: nan gradient found. The current loss is:  0.6107041835784912\n",
      "Warning: nan gradient found. The current loss is:  1.1018527746200562\n",
      "Warning: nan gradient found. The current loss is:  0.49539050459861755\n",
      "Warning: nan gradient found. The current loss is:  1.2486612796783447\n",
      "Warning: nan gradient found. The current loss is:  0.6859815716743469\n",
      "Warning: nan gradient found. The current loss is:  0.015670696273446083\n",
      "Warning: nan gradient found. The current loss is:  0.23269973695278168\n",
      "Warning: nan gradient found. The current loss is:  0.01601356267929077\n",
      "Warning: nan gradient found. The current loss is:  0.44496381282806396\n",
      "Warning: nan gradient found. The current loss is:  0.4490910768508911\n",
      "Warning: nan gradient found. The current loss is:  0.22145213186740875\n",
      "Warning: nan gradient found. The current loss is:  0.42073073983192444\n",
      "Warning: nan gradient found. The current loss is:  0.44552212953567505\n",
      "Warning: nan gradient found. The current loss is:  0.5913887023925781\n",
      "Warning: nan gradient found. The current loss is:  0.6341078281402588\n",
      "Warning: nan gradient found. The current loss is:  0.3615393042564392\n",
      "Warning: nan gradient found. The current loss is:  0.08507857471704483\n",
      "Warning: nan gradient found. The current loss is:  0.18997791409492493\n",
      "Warning: nan gradient found. The current loss is:  0.11290283501148224\n",
      "Warning: nan gradient found. The current loss is:  -0.05927465856075287\n",
      "Warning: nan gradient found. The current loss is:  0.2372473180294037\n",
      "Warning: nan gradient found. The current loss is:  -0.04190243408083916\n",
      "Warning: nan gradient found. The current loss is:  0.4973094165325165\n",
      "Warning: nan gradient found. The current loss is:  0.13136915862560272\n",
      "Warning: nan gradient found. The current loss is:  0.1257721185684204\n",
      "Warning: nan gradient found. The current loss is:  0.33224165439605713\n",
      "Warning: nan gradient found. The current loss is:  0.5372871160507202\n",
      "Warning: nan gradient found. The current loss is:  0.018157288432121277\n",
      "Current batch training loss: 0.018157  [716800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.38745802640914917\n",
      "Warning: nan gradient found. The current loss is:  0.39473384618759155\n",
      "Warning: nan gradient found. The current loss is:  0.5014338493347168\n",
      "Warning: nan gradient found. The current loss is:  0.6291926503181458\n",
      "Warning: nan gradient found. The current loss is:  0.2784588634967804\n",
      "Warning: nan gradient found. The current loss is:  0.32064810395240784\n",
      "Warning: nan gradient found. The current loss is:  0.17592795193195343\n",
      "Warning: nan gradient found. The current loss is:  0.5684523582458496\n",
      "Warning: nan gradient found. The current loss is:  0.7227270603179932\n",
      "Warning: nan gradient found. The current loss is:  0.17747502028942108\n",
      "Warning: nan gradient found. The current loss is:  0.7518806457519531\n",
      "Warning: nan gradient found. The current loss is:  0.7724043130874634\n",
      "Warning: nan gradient found. The current loss is:  0.575416624546051\n",
      "Warning: nan gradient found. The current loss is:  0.3031063973903656\n",
      "Warning: nan gradient found. The current loss is:  0.5783661603927612\n",
      "Warning: nan gradient found. The current loss is:  0.694013774394989\n",
      "Warning: nan gradient found. The current loss is:  0.23076610267162323\n",
      "Warning: nan gradient found. The current loss is:  0.15197744965553284\n",
      "Warning: nan gradient found. The current loss is:  0.5321005582809448\n",
      "Warning: nan gradient found. The current loss is:  0.682469367980957\n",
      "Warning: nan gradient found. The current loss is:  0.48019707202911377\n",
      "Warning: nan gradient found. The current loss is:  1.1166883707046509\n",
      "Warning: nan gradient found. The current loss is:  0.2693166732788086\n",
      "Warning: nan gradient found. The current loss is:  0.6491106748580933\n",
      "Warning: nan gradient found. The current loss is:  0.16799838840961456\n",
      "Warning: nan gradient found. The current loss is:  0.42735201120376587\n",
      "Warning: nan gradient found. The current loss is:  0.369245707988739\n",
      "Warning: nan gradient found. The current loss is:  0.33854854106903076\n",
      "Warning: nan gradient found. The current loss is:  0.3474613428115845\n",
      "Warning: nan gradient found. The current loss is:  0.5401185750961304\n",
      "Warning: nan gradient found. The current loss is:  0.9713956117630005\n",
      "Warning: nan gradient found. The current loss is:  0.8712834715843201\n",
      "Warning: nan gradient found. The current loss is:  0.2853008508682251\n",
      "Warning: nan gradient found. The current loss is:  0.28068503737449646\n",
      "Warning: nan gradient found. The current loss is:  0.45860862731933594\n",
      "Warning: nan gradient found. The current loss is:  0.7672492265701294\n",
      "Warning: nan gradient found. The current loss is:  0.9468536972999573\n",
      "Warning: nan gradient found. The current loss is:  0.02639695443212986\n",
      "Warning: nan gradient found. The current loss is:  0.27918773889541626\n",
      "Warning: nan gradient found. The current loss is:  0.47049689292907715\n",
      "Warning: nan gradient found. The current loss is:  0.36573266983032227\n",
      "Warning: nan gradient found. The current loss is:  0.175870880484581\n",
      "Warning: nan gradient found. The current loss is:  0.7429375648498535\n",
      "Warning: nan gradient found. The current loss is:  0.3242340087890625\n",
      "Warning: nan gradient found. The current loss is:  0.10951529443264008\n",
      "Warning: nan gradient found. The current loss is:  0.1519407331943512\n",
      "Warning: nan gradient found. The current loss is:  0.6451581716537476\n",
      "Warning: nan gradient found. The current loss is:  0.631199836730957\n",
      "Warning: nan gradient found. The current loss is:  2.7590737342834473\n",
      "Warning: nan gradient found. The current loss is:  0.6337740421295166\n",
      "Warning: nan gradient found. The current loss is:  0.38693347573280334\n",
      "Warning: nan gradient found. The current loss is:  0.6148346662521362\n",
      "Warning: nan gradient found. The current loss is:  0.06581217050552368\n",
      "Warning: nan gradient found. The current loss is:  0.8547881841659546\n",
      "Warning: nan gradient found. The current loss is:  0.8431152105331421\n",
      "Warning: nan gradient found. The current loss is:  0.37923285365104675\n",
      "Warning: nan gradient found. The current loss is:  0.07650001347064972\n",
      "Warning: nan gradient found. The current loss is:  0.15152257680892944\n",
      "Warning: nan gradient found. The current loss is:  0.2767794132232666\n",
      "Warning: nan gradient found. The current loss is:  1.2484960556030273\n",
      "Warning: nan gradient found. The current loss is:  0.4978482127189636\n",
      "Warning: nan gradient found. The current loss is:  0.3180197477340698\n",
      "Warning: nan gradient found. The current loss is:  0.41551467776298523\n",
      "Warning: nan gradient found. The current loss is:  -0.03071884997189045\n",
      "Warning: nan gradient found. The current loss is:  1.0250678062438965\n",
      "Warning: nan gradient found. The current loss is:  0.388327956199646\n",
      "Warning: nan gradient found. The current loss is:  0.2701024115085602\n",
      "Warning: nan gradient found. The current loss is:  0.6852282881736755\n",
      "Warning: nan gradient found. The current loss is:  0.28879186511039734\n",
      "Warning: nan gradient found. The current loss is:  0.14534252882003784\n",
      "Warning: nan gradient found. The current loss is:  0.2585548758506775\n",
      "Warning: nan gradient found. The current loss is:  0.38824528455734253\n",
      "Warning: nan gradient found. The current loss is:  0.22745579481124878\n",
      "Warning: nan gradient found. The current loss is:  0.06772825121879578\n",
      "Warning: nan gradient found. The current loss is:  0.5487399697303772\n",
      "Warning: nan gradient found. The current loss is:  0.3551791310310364\n",
      "Warning: nan gradient found. The current loss is:  0.42115771770477295\n",
      "Warning: nan gradient found. The current loss is:  0.5625671148300171\n",
      "Warning: nan gradient found. The current loss is:  0.05572653189301491\n",
      "Warning: nan gradient found. The current loss is:  1.070729374885559\n",
      "Warning: nan gradient found. The current loss is:  0.33331894874572754\n",
      "Warning: nan gradient found. The current loss is:  0.20631200075149536\n",
      "Warning: nan gradient found. The current loss is:  0.2631322145462036\n",
      "Warning: nan gradient found. The current loss is:  0.0973614752292633\n",
      "Warning: nan gradient found. The current loss is:  0.11270295083522797\n",
      "Warning: nan gradient found. The current loss is:  0.4805278778076172\n",
      "Warning: nan gradient found. The current loss is:  0.6272692680358887\n",
      "Warning: nan gradient found. The current loss is:  0.6494128704071045\n",
      "Warning: nan gradient found. The current loss is:  0.6906033754348755\n",
      "Warning: nan gradient found. The current loss is:  0.3987963795661926\n",
      "Warning: nan gradient found. The current loss is:  0.6589230298995972\n",
      "Warning: nan gradient found. The current loss is:  0.6469053030014038\n",
      "Warning: nan gradient found. The current loss is:  0.009463731199502945\n",
      "Warning: nan gradient found. The current loss is:  0.08560919016599655\n",
      "Warning: nan gradient found. The current loss is:  1.1796646118164062\n",
      "Warning: nan gradient found. The current loss is:  0.21749697625637054\n",
      "Warning: nan gradient found. The current loss is:  0.22951838374137878\n",
      "Warning: nan gradient found. The current loss is:  0.5225993394851685\n",
      "Warning: nan gradient found. The current loss is:  -0.22381281852722168\n",
      "Warning: nan gradient found. The current loss is:  0.7404800653457642\n",
      "Current batch training loss: 0.740480  [742400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4752110242843628\n",
      "Warning: nan gradient found. The current loss is:  0.5680536031723022\n",
      "Warning: nan gradient found. The current loss is:  0.46166592836380005\n",
      "Warning: nan gradient found. The current loss is:  -0.051917776465415955\n",
      "Warning: nan gradient found. The current loss is:  0.23305359482765198\n",
      "Warning: nan gradient found. The current loss is:  0.5768711566925049\n",
      "Warning: nan gradient found. The current loss is:  0.8552855253219604\n",
      "Warning: nan gradient found. The current loss is:  0.6650293469429016\n",
      "Warning: nan gradient found. The current loss is:  0.8982112407684326\n",
      "Warning: nan gradient found. The current loss is:  0.2990841269493103\n",
      "Warning: nan gradient found. The current loss is:  0.2932169437408447\n",
      "Warning: nan gradient found. The current loss is:  1.5565497875213623\n",
      "Warning: nan gradient found. The current loss is:  0.9911447167396545\n",
      "Warning: nan gradient found. The current loss is:  0.427121639251709\n",
      "Warning: nan gradient found. The current loss is:  0.26319441199302673\n",
      "Warning: nan gradient found. The current loss is:  0.7551131248474121\n",
      "Warning: nan gradient found. The current loss is:  0.5790427923202515\n",
      "Warning: nan gradient found. The current loss is:  0.7554597854614258\n",
      "Warning: nan gradient found. The current loss is:  0.14119915664196014\n",
      "Warning: nan gradient found. The current loss is:  0.817437469959259\n",
      "Warning: nan gradient found. The current loss is:  0.44591736793518066\n",
      "Warning: nan gradient found. The current loss is:  0.34236112236976624\n",
      "Warning: nan gradient found. The current loss is:  0.5330589413642883\n",
      "Warning: nan gradient found. The current loss is:  0.4381248354911804\n",
      "Warning: nan gradient found. The current loss is:  0.7018198370933533\n",
      "Warning: nan gradient found. The current loss is:  0.3828146457672119\n",
      "Warning: nan gradient found. The current loss is:  0.30974578857421875\n",
      "Warning: nan gradient found. The current loss is:  0.21539416909217834\n",
      "Warning: nan gradient found. The current loss is:  0.7679538726806641\n",
      "Warning: nan gradient found. The current loss is:  0.05155674368143082\n",
      "Warning: nan gradient found. The current loss is:  3.046487808227539\n",
      "Warning: nan gradient found. The current loss is:  0.42993560433387756\n",
      "Warning: nan gradient found. The current loss is:  0.2395114004611969\n",
      "Warning: nan gradient found. The current loss is:  0.08862019330263138\n",
      "Warning: nan gradient found. The current loss is:  0.48856937885284424\n",
      "Warning: nan gradient found. The current loss is:  0.3101794123649597\n",
      "Warning: nan gradient found. The current loss is:  0.2283298671245575\n",
      "Warning: nan gradient found. The current loss is:  0.0013690926134586334\n",
      "Warning: nan gradient found. The current loss is:  0.6095701456069946\n",
      "Warning: nan gradient found. The current loss is:  0.28267499804496765\n",
      "Warning: nan gradient found. The current loss is:  0.26835042238235474\n",
      "Warning: nan gradient found. The current loss is:  0.284069687128067\n",
      "Warning: nan gradient found. The current loss is:  0.4115869104862213\n",
      "Warning: nan gradient found. The current loss is:  0.2781958281993866\n",
      "Warning: nan gradient found. The current loss is:  0.7730158567428589\n",
      "Warning: nan gradient found. The current loss is:  1.2704482078552246\n",
      "Warning: nan gradient found. The current loss is:  0.8891453146934509\n",
      "Warning: nan gradient found. The current loss is:  0.4979943335056305\n",
      "Warning: nan gradient found. The current loss is:  0.43403148651123047\n",
      "Warning: nan gradient found. The current loss is:  0.35165053606033325\n",
      "Warning: nan gradient found. The current loss is:  0.2534559965133667\n",
      "Warning: nan gradient found. The current loss is:  0.4749714732170105\n",
      "Warning: nan gradient found. The current loss is:  -0.014991305768489838\n",
      "Warning: nan gradient found. The current loss is:  0.1481914222240448\n",
      "Warning: nan gradient found. The current loss is:  0.40144801139831543\n",
      "Warning: nan gradient found. The current loss is:  0.6950984001159668\n",
      "Warning: nan gradient found. The current loss is:  0.6198989152908325\n",
      "Warning: nan gradient found. The current loss is:  0.44226622581481934\n",
      "Warning: nan gradient found. The current loss is:  0.20849823951721191\n",
      "Warning: nan gradient found. The current loss is:  0.5448437333106995\n",
      "Warning: nan gradient found. The current loss is:  0.0016347169876098633\n",
      "Warning: nan gradient found. The current loss is:  0.47940751910209656\n",
      "Warning: nan gradient found. The current loss is:  0.702915370464325\n",
      "Warning: nan gradient found. The current loss is:  0.3794858455657959\n",
      "Warning: nan gradient found. The current loss is:  0.1672441065311432\n",
      "Warning: nan gradient found. The current loss is:  -0.0390617810189724\n",
      "Warning: nan gradient found. The current loss is:  0.46126705408096313\n",
      "Warning: nan gradient found. The current loss is:  0.3675496578216553\n",
      "Warning: nan gradient found. The current loss is:  0.42381441593170166\n",
      "Warning: nan gradient found. The current loss is:  0.13351930677890778\n",
      "Warning: nan gradient found. The current loss is:  0.3269462585449219\n",
      "Warning: nan gradient found. The current loss is:  0.11563679575920105\n",
      "Warning: nan gradient found. The current loss is:  0.16317397356033325\n",
      "Warning: nan gradient found. The current loss is:  0.3823099732398987\n",
      "Warning: nan gradient found. The current loss is:  0.28530216217041016\n",
      "Warning: nan gradient found. The current loss is:  1.7786176204681396\n",
      "Warning: nan gradient found. The current loss is:  0.4282744526863098\n",
      "Warning: nan gradient found. The current loss is:  0.27228352427482605\n",
      "Warning: nan gradient found. The current loss is:  0.6029262542724609\n",
      "Warning: nan gradient found. The current loss is:  0.5352891683578491\n",
      "Warning: nan gradient found. The current loss is:  0.3828706741333008\n",
      "Warning: nan gradient found. The current loss is:  0.514293909072876\n",
      "Warning: nan gradient found. The current loss is:  0.35112833976745605\n",
      "Warning: nan gradient found. The current loss is:  0.5856502056121826\n",
      "Warning: nan gradient found. The current loss is:  0.7979083061218262\n",
      "Warning: nan gradient found. The current loss is:  0.668054461479187\n",
      "Warning: nan gradient found. The current loss is:  0.36463186144828796\n",
      "Warning: nan gradient found. The current loss is:  0.10445685684680939\n",
      "Warning: nan gradient found. The current loss is:  0.6276713609695435\n",
      "Warning: nan gradient found. The current loss is:  0.5030485391616821\n",
      "Warning: nan gradient found. The current loss is:  1.1490806341171265\n",
      "Warning: nan gradient found. The current loss is:  0.2579945921897888\n",
      "Warning: nan gradient found. The current loss is:  0.3203980028629303\n",
      "Warning: nan gradient found. The current loss is:  0.23707641661167145\n",
      "Warning: nan gradient found. The current loss is:  0.44217509031295776\n",
      "Warning: nan gradient found. The current loss is:  0.31381332874298096\n",
      "Warning: nan gradient found. The current loss is:  0.2684509754180908\n",
      "Warning: nan gradient found. The current loss is:  0.23685197532176971\n",
      "Warning: nan gradient found. The current loss is:  0.621388852596283\n",
      "Warning: nan gradient found. The current loss is:  0.12444072961807251\n",
      "Current batch training loss: 0.124441  [768000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.30651670694351196\n",
      "Warning: nan gradient found. The current loss is:  0.5213678479194641\n",
      "Warning: nan gradient found. The current loss is:  0.2973392605781555\n",
      "Warning: nan gradient found. The current loss is:  0.20839416980743408\n",
      "Warning: nan gradient found. The current loss is:  0.17216630280017853\n",
      "Warning: nan gradient found. The current loss is:  0.7406778335571289\n",
      "Warning: nan gradient found. The current loss is:  0.08273835480213165\n",
      "Warning: nan gradient found. The current loss is:  1.3013532161712646\n",
      "Warning: nan gradient found. The current loss is:  0.124017134308815\n",
      "Warning: nan gradient found. The current loss is:  0.017015544697642326\n",
      "Warning: nan gradient found. The current loss is:  0.26408302783966064\n",
      "Warning: nan gradient found. The current loss is:  0.8738384246826172\n",
      "Warning: nan gradient found. The current loss is:  0.3617050051689148\n",
      "Warning: nan gradient found. The current loss is:  0.9309507608413696\n",
      "Warning: nan gradient found. The current loss is:  0.9782745242118835\n",
      "Warning: nan gradient found. The current loss is:  0.5373218059539795\n",
      "Warning: nan gradient found. The current loss is:  0.5258204340934753\n",
      "Warning: nan gradient found. The current loss is:  0.07964923977851868\n",
      "Warning: nan gradient found. The current loss is:  0.40971654653549194\n",
      "Warning: nan gradient found. The current loss is:  0.5462744235992432\n",
      "Warning: nan gradient found. The current loss is:  0.2853155732154846\n",
      "Warning: nan gradient found. The current loss is:  0.2662737965583801\n",
      "Warning: nan gradient found. The current loss is:  -0.09725318849086761\n",
      "Warning: nan gradient found. The current loss is:  1.0546859502792358\n",
      "Warning: nan gradient found. The current loss is:  0.7406193017959595\n",
      "Warning: nan gradient found. The current loss is:  0.20266227424144745\n",
      "Warning: nan gradient found. The current loss is:  1.038217306137085\n",
      "Warning: nan gradient found. The current loss is:  0.8983950018882751\n",
      "Warning: nan gradient found. The current loss is:  0.2489914745092392\n",
      "Warning: nan gradient found. The current loss is:  0.5239948034286499\n",
      "Warning: nan gradient found. The current loss is:  0.37951138615608215\n",
      "Warning: nan gradient found. The current loss is:  0.6436493396759033\n",
      "Warning: nan gradient found. The current loss is:  0.3736618161201477\n",
      "Warning: nan gradient found. The current loss is:  0.18361136317253113\n",
      "Warning: nan gradient found. The current loss is:  0.15314078330993652\n",
      "Warning: nan gradient found. The current loss is:  0.26691529154777527\n",
      "Warning: nan gradient found. The current loss is:  0.31398600339889526\n",
      "Warning: nan gradient found. The current loss is:  0.17601974308490753\n",
      "Warning: nan gradient found. The current loss is:  0.9980262517929077\n",
      "Warning: nan gradient found. The current loss is:  0.5099179148674011\n",
      "Warning: nan gradient found. The current loss is:  0.1955491602420807\n",
      "Warning: nan gradient found. The current loss is:  0.4218041002750397\n",
      "Warning: nan gradient found. The current loss is:  -0.08808740973472595\n",
      "Warning: nan gradient found. The current loss is:  0.3605883717536926\n",
      "Warning: nan gradient found. The current loss is:  0.335196316242218\n",
      "Warning: nan gradient found. The current loss is:  -0.1638970673084259\n",
      "Warning: nan gradient found. The current loss is:  0.3408510982990265\n",
      "Warning: nan gradient found. The current loss is:  0.09923757612705231\n",
      "Warning: nan gradient found. The current loss is:  1.9268616437911987\n",
      "Warning: nan gradient found. The current loss is:  0.2815341055393219\n",
      "Warning: nan gradient found. The current loss is:  0.24203814566135406\n",
      "Warning: nan gradient found. The current loss is:  -0.08243118226528168\n",
      "Warning: nan gradient found. The current loss is:  0.3282473683357239\n",
      "Warning: nan gradient found. The current loss is:  1.1665867567062378\n",
      "Warning: nan gradient found. The current loss is:  0.5263634324073792\n",
      "Warning: nan gradient found. The current loss is:  0.22527135908603668\n",
      "Warning: nan gradient found. The current loss is:  0.8875596523284912\n",
      "Warning: nan gradient found. The current loss is:  0.6626180410385132\n",
      "Warning: nan gradient found. The current loss is:  0.4862816035747528\n",
      "Warning: nan gradient found. The current loss is:  0.050289325416088104\n",
      "Warning: nan gradient found. The current loss is:  0.29834896326065063\n",
      "Warning: nan gradient found. The current loss is:  0.5671262741088867\n",
      "Warning: nan gradient found. The current loss is:  0.15427924692630768\n",
      "Warning: nan gradient found. The current loss is:  0.3917277455329895\n",
      "Warning: nan gradient found. The current loss is:  0.5861602425575256\n",
      "Warning: nan gradient found. The current loss is:  0.6994624733924866\n",
      "Warning: nan gradient found. The current loss is:  1.5374624729156494\n",
      "Warning: nan gradient found. The current loss is:  -0.1465209573507309\n",
      "Warning: nan gradient found. The current loss is:  0.1507725566625595\n",
      "Warning: nan gradient found. The current loss is:  0.4426110088825226\n",
      "Warning: nan gradient found. The current loss is:  0.5886933207511902\n",
      "Warning: nan gradient found. The current loss is:  -0.13628366589546204\n",
      "Warning: nan gradient found. The current loss is:  0.028776153922080994\n",
      "Warning: nan gradient found. The current loss is:  0.7481008172035217\n",
      "Warning: nan gradient found. The current loss is:  0.5815024375915527\n",
      "Warning: nan gradient found. The current loss is:  0.7546183466911316\n",
      "Warning: nan gradient found. The current loss is:  0.3961910605430603\n",
      "Warning: nan gradient found. The current loss is:  2.3395326137542725\n",
      "Warning: nan gradient found. The current loss is:  0.004879988729953766\n",
      "Warning: nan gradient found. The current loss is:  0.033286549150943756\n",
      "Warning: nan gradient found. The current loss is:  0.08697725087404251\n",
      "Warning: nan gradient found. The current loss is:  0.4760898947715759\n",
      "Warning: nan gradient found. The current loss is:  0.2523660659790039\n",
      "Warning: nan gradient found. The current loss is:  0.1828528493642807\n",
      "Warning: nan gradient found. The current loss is:  0.2594250738620758\n",
      "Warning: nan gradient found. The current loss is:  0.22659610211849213\n",
      "Warning: nan gradient found. The current loss is:  0.7346534729003906\n",
      "Warning: nan gradient found. The current loss is:  0.46924591064453125\n",
      "Warning: nan gradient found. The current loss is:  0.48585301637649536\n",
      "Warning: nan gradient found. The current loss is:  0.44154679775238037\n",
      "Warning: nan gradient found. The current loss is:  0.7262750267982483\n",
      "Warning: nan gradient found. The current loss is:  0.4411659240722656\n",
      "Warning: nan gradient found. The current loss is:  0.16231566667556763\n",
      "Warning: nan gradient found. The current loss is:  0.46315354108810425\n",
      "Warning: nan gradient found. The current loss is:  0.6714339852333069\n",
      "Warning: nan gradient found. The current loss is:  0.2537750005722046\n",
      "Warning: nan gradient found. The current loss is:  0.4723634123802185\n",
      "Warning: nan gradient found. The current loss is:  0.4609531760215759\n",
      "Warning: nan gradient found. The current loss is:  0.3950106203556061\n",
      "Warning: nan gradient found. The current loss is:  0.35244911909103394\n",
      "Current batch training loss: 0.352449  [793600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.1817288398742676\n",
      "Warning: nan gradient found. The current loss is:  0.8046704530715942\n",
      "Warning: nan gradient found. The current loss is:  0.3125421106815338\n",
      "Warning: nan gradient found. The current loss is:  0.5107837319374084\n",
      "Warning: nan gradient found. The current loss is:  0.31944477558135986\n",
      "Warning: nan gradient found. The current loss is:  0.10587047785520554\n",
      "Warning: nan gradient found. The current loss is:  0.3553099036216736\n",
      "Warning: nan gradient found. The current loss is:  0.4865102767944336\n",
      "Warning: nan gradient found. The current loss is:  0.2177993655204773\n",
      "Warning: nan gradient found. The current loss is:  0.4410199820995331\n",
      "Warning: nan gradient found. The current loss is:  0.3953094482421875\n",
      "Warning: nan gradient found. The current loss is:  0.3687169849872589\n",
      "Warning: nan gradient found. The current loss is:  0.3912392556667328\n",
      "Warning: nan gradient found. The current loss is:  0.6417936086654663\n",
      "Warning: nan gradient found. The current loss is:  0.21941843628883362\n",
      "Warning: nan gradient found. The current loss is:  0.22985388338565826\n",
      "Warning: nan gradient found. The current loss is:  1.342209815979004\n",
      "Warning: nan gradient found. The current loss is:  0.6938297748565674\n",
      "Warning: nan gradient found. The current loss is:  0.5587214827537537\n",
      "Warning: nan gradient found. The current loss is:  0.13779155910015106\n",
      "Warning: nan gradient found. The current loss is:  0.35129106044769287\n",
      "Warning: nan gradient found. The current loss is:  0.43072032928466797\n",
      "Warning: nan gradient found. The current loss is:  0.4771234691143036\n",
      "Warning: nan gradient found. The current loss is:  0.10400556027889252\n",
      "Warning: nan gradient found. The current loss is:  0.9309327006340027\n",
      "Warning: nan gradient found. The current loss is:  0.3867541551589966\n",
      "Warning: nan gradient found. The current loss is:  0.8978385925292969\n",
      "Warning: nan gradient found. The current loss is:  0.8765928745269775\n",
      "Warning: nan gradient found. The current loss is:  0.21141719818115234\n",
      "Warning: nan gradient found. The current loss is:  0.33953213691711426\n",
      "Warning: nan gradient found. The current loss is:  0.07526738196611404\n",
      "Warning: nan gradient found. The current loss is:  0.3551304042339325\n",
      "Warning: nan gradient found. The current loss is:  0.5889566540718079\n",
      "Warning: nan gradient found. The current loss is:  0.4418112635612488\n",
      "Warning: nan gradient found. The current loss is:  0.2721027731895447\n",
      "Warning: nan gradient found. The current loss is:  0.6979451179504395\n",
      "Warning: nan gradient found. The current loss is:  0.6847018599510193\n",
      "Warning: nan gradient found. The current loss is:  0.9628970623016357\n",
      "Warning: nan gradient found. The current loss is:  0.08785621076822281\n",
      "Warning: nan gradient found. The current loss is:  0.4578869640827179\n",
      "Warning: nan gradient found. The current loss is:  0.0019880905747413635\n",
      "Warning: nan gradient found. The current loss is:  0.3049207627773285\n",
      "Warning: nan gradient found. The current loss is:  0.7424435615539551\n",
      "Warning: nan gradient found. The current loss is:  0.19508719444274902\n",
      "Warning: nan gradient found. The current loss is:  0.7529073357582092\n",
      "Warning: nan gradient found. The current loss is:  0.5527681708335876\n",
      "Warning: nan gradient found. The current loss is:  0.3819389045238495\n",
      "Warning: nan gradient found. The current loss is:  0.6214420795440674\n",
      "Warning: nan gradient found. The current loss is:  0.29981303215026855\n",
      "Warning: nan gradient found. The current loss is:  0.6242845058441162\n",
      "Warning: nan gradient found. The current loss is:  0.22978425025939941\n",
      "Warning: nan gradient found. The current loss is:  0.39107999205589294\n",
      "Warning: nan gradient found. The current loss is:  0.5132511854171753\n",
      "Warning: nan gradient found. The current loss is:  1.0442509651184082\n",
      "Warning: nan gradient found. The current loss is:  0.018721887841820717\n",
      "Warning: nan gradient found. The current loss is:  0.46475136280059814\n",
      "Warning: nan gradient found. The current loss is:  0.3371419608592987\n",
      "Warning: nan gradient found. The current loss is:  0.6665928959846497\n",
      "Warning: nan gradient found. The current loss is:  0.38331055641174316\n",
      "Warning: nan gradient found. The current loss is:  -0.0656435564160347\n",
      "Warning: nan gradient found. The current loss is:  -0.019064966589212418\n",
      "Warning: nan gradient found. The current loss is:  0.20131796598434448\n",
      "Warning: nan gradient found. The current loss is:  0.39053428173065186\n",
      "Warning: nan gradient found. The current loss is:  0.5531860589981079\n",
      "Warning: nan gradient found. The current loss is:  0.48390066623687744\n",
      "Warning: nan gradient found. The current loss is:  0.3410114049911499\n",
      "Warning: nan gradient found. The current loss is:  0.138163760304451\n",
      "Warning: nan gradient found. The current loss is:  0.29607078433036804\n",
      "Warning: nan gradient found. The current loss is:  0.10456661880016327\n",
      "Warning: nan gradient found. The current loss is:  -0.05311688035726547\n",
      "Warning: nan gradient found. The current loss is:  0.18474705517292023\n",
      "Warning: nan gradient found. The current loss is:  0.3970594108104706\n",
      "Warning: nan gradient found. The current loss is:  0.12327007949352264\n",
      "Warning: nan gradient found. The current loss is:  0.08064112067222595\n",
      "Warning: nan gradient found. The current loss is:  0.229116290807724\n",
      "Warning: nan gradient found. The current loss is:  0.4497760534286499\n",
      "Warning: nan gradient found. The current loss is:  0.5310917496681213\n",
      "Warning: nan gradient found. The current loss is:  0.7232571244239807\n",
      "Warning: nan gradient found. The current loss is:  0.06967166066169739\n",
      "Warning: nan gradient found. The current loss is:  0.3264561891555786\n",
      "Warning: nan gradient found. The current loss is:  0.2655346691608429\n",
      "Warning: nan gradient found. The current loss is:  0.5853253602981567\n",
      "Warning: nan gradient found. The current loss is:  0.2640446126461029\n",
      "Warning: nan gradient found. The current loss is:  0.30364638566970825\n",
      "Warning: nan gradient found. The current loss is:  0.362739235162735\n",
      "Warning: nan gradient found. The current loss is:  0.43939661979675293\n",
      "Warning: nan gradient found. The current loss is:  0.7710664868354797\n",
      "Warning: nan gradient found. The current loss is:  0.28937751054763794\n",
      "Warning: nan gradient found. The current loss is:  0.816417932510376\n",
      "Warning: nan gradient found. The current loss is:  0.2602524757385254\n",
      "Warning: nan gradient found. The current loss is:  0.2060873955488205\n",
      "Warning: nan gradient found. The current loss is:  0.5529793500900269\n",
      "Warning: nan gradient found. The current loss is:  0.8199754953384399\n",
      "Warning: nan gradient found. The current loss is:  0.034638091921806335\n",
      "Warning: nan gradient found. The current loss is:  0.6419931650161743\n",
      "Warning: nan gradient found. The current loss is:  0.41866999864578247\n",
      "Warning: nan gradient found. The current loss is:  0.35058414936065674\n",
      "Warning: nan gradient found. The current loss is:  0.476445734500885\n",
      "Warning: nan gradient found. The current loss is:  0.15214911103248596\n",
      "Current batch training loss: 0.152149  [819200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2177240550518036\n",
      "Warning: nan gradient found. The current loss is:  -0.07669955492019653\n",
      "Warning: nan gradient found. The current loss is:  0.42280784249305725\n",
      "Warning: nan gradient found. The current loss is:  0.4638102650642395\n",
      "Warning: nan gradient found. The current loss is:  0.15773877501487732\n",
      "Warning: nan gradient found. The current loss is:  0.2365083545446396\n",
      "Warning: nan gradient found. The current loss is:  0.17697301506996155\n",
      "Warning: nan gradient found. The current loss is:  0.3267970085144043\n",
      "Warning: nan gradient found. The current loss is:  0.39392557740211487\n",
      "Warning: nan gradient found. The current loss is:  0.09352387487888336\n",
      "Warning: nan gradient found. The current loss is:  0.09379352629184723\n",
      "Warning: nan gradient found. The current loss is:  0.5038425922393799\n",
      "Warning: nan gradient found. The current loss is:  0.5201698541641235\n",
      "Warning: nan gradient found. The current loss is:  0.7658663988113403\n",
      "Warning: nan gradient found. The current loss is:  0.37081801891326904\n",
      "Warning: nan gradient found. The current loss is:  0.2631716728210449\n",
      "Warning: nan gradient found. The current loss is:  0.6493956446647644\n",
      "Warning: nan gradient found. The current loss is:  1.4256951808929443\n",
      "Warning: nan gradient found. The current loss is:  0.4477517306804657\n",
      "Warning: nan gradient found. The current loss is:  0.5907328128814697\n",
      "Warning: nan gradient found. The current loss is:  0.2533659338951111\n",
      "Warning: nan gradient found. The current loss is:  1.026991844177246\n",
      "Warning: nan gradient found. The current loss is:  0.2789013385772705\n",
      "Warning: nan gradient found. The current loss is:  0.4352054297924042\n",
      "Warning: nan gradient found. The current loss is:  0.3555200695991516\n",
      "Warning: nan gradient found. The current loss is:  1.1247622966766357\n",
      "Warning: nan gradient found. The current loss is:  0.24601854383945465\n",
      "Warning: nan gradient found. The current loss is:  0.3528898060321808\n",
      "Warning: nan gradient found. The current loss is:  0.30610936880111694\n",
      "Warning: nan gradient found. The current loss is:  0.6885931491851807\n",
      "Warning: nan gradient found. The current loss is:  0.7085278630256653\n",
      "Warning: nan gradient found. The current loss is:  0.19169071316719055\n",
      "Warning: nan gradient found. The current loss is:  0.4192776083946228\n",
      "Warning: nan gradient found. The current loss is:  0.36377930641174316\n",
      "Warning: nan gradient found. The current loss is:  0.27073851227760315\n",
      "Warning: nan gradient found. The current loss is:  0.18707387149333954\n",
      "Warning: nan gradient found. The current loss is:  0.6528978943824768\n",
      "Warning: nan gradient found. The current loss is:  -0.054760754108428955\n",
      "Warning: nan gradient found. The current loss is:  0.4500419497489929\n",
      "Warning: nan gradient found. The current loss is:  0.3015749752521515\n",
      "Warning: nan gradient found. The current loss is:  0.20597004890441895\n",
      "Warning: nan gradient found. The current loss is:  0.4433990716934204\n",
      "Warning: nan gradient found. The current loss is:  0.5003196001052856\n",
      "Warning: nan gradient found. The current loss is:  0.39191168546676636\n",
      "Warning: nan gradient found. The current loss is:  0.40182390809059143\n",
      "Warning: nan gradient found. The current loss is:  0.7911531925201416\n",
      "Warning: nan gradient found. The current loss is:  0.4639716148376465\n",
      "Warning: nan gradient found. The current loss is:  0.028440959751605988\n",
      "Warning: nan gradient found. The current loss is:  0.4278612732887268\n",
      "Warning: nan gradient found. The current loss is:  0.0245977696031332\n",
      "Warning: nan gradient found. The current loss is:  0.4785429835319519\n",
      "Warning: nan gradient found. The current loss is:  0.38122454285621643\n",
      "Warning: nan gradient found. The current loss is:  0.42161208391189575\n",
      "Warning: nan gradient found. The current loss is:  1.1214261054992676\n",
      "Warning: nan gradient found. The current loss is:  0.34401416778564453\n",
      "Warning: nan gradient found. The current loss is:  0.4084215760231018\n",
      "Warning: nan gradient found. The current loss is:  0.49742233753204346\n",
      "Warning: nan gradient found. The current loss is:  0.2980656325817108\n",
      "Warning: nan gradient found. The current loss is:  0.4619421064853668\n",
      "Warning: nan gradient found. The current loss is:  0.24329540133476257\n",
      "Warning: nan gradient found. The current loss is:  0.8009971380233765\n",
      "Warning: nan gradient found. The current loss is:  0.8432164192199707\n",
      "Warning: nan gradient found. The current loss is:  0.8821296691894531\n",
      "Warning: nan gradient found. The current loss is:  0.01131293922662735\n",
      "Warning: nan gradient found. The current loss is:  0.34491464495658875\n",
      "Warning: nan gradient found. The current loss is:  0.4367852807044983\n",
      "Warning: nan gradient found. The current loss is:  0.3473198413848877\n",
      "Warning: nan gradient found. The current loss is:  -0.04354676976799965\n",
      "Warning: nan gradient found. The current loss is:  0.4842833876609802\n",
      "Warning: nan gradient found. The current loss is:  1.118518590927124\n",
      "Warning: nan gradient found. The current loss is:  0.1854395866394043\n",
      "Warning: nan gradient found. The current loss is:  0.4975937008857727\n",
      "Warning: nan gradient found. The current loss is:  0.3309509754180908\n",
      "Warning: nan gradient found. The current loss is:  0.17862394452095032\n",
      "Warning: nan gradient found. The current loss is:  0.2734246253967285\n",
      "Warning: nan gradient found. The current loss is:  0.75341796875\n",
      "Warning: nan gradient found. The current loss is:  0.6649469137191772\n",
      "Warning: nan gradient found. The current loss is:  0.27910086512565613\n",
      "Warning: nan gradient found. The current loss is:  0.28767815232276917\n",
      "Warning: nan gradient found. The current loss is:  0.3098515272140503\n",
      "Warning: nan gradient found. The current loss is:  0.5421528220176697\n",
      "Warning: nan gradient found. The current loss is:  0.2102956771850586\n",
      "Warning: nan gradient found. The current loss is:  0.26552683115005493\n",
      "Warning: nan gradient found. The current loss is:  0.2496320605278015\n",
      "Warning: nan gradient found. The current loss is:  0.7296452522277832\n",
      "Warning: nan gradient found. The current loss is:  0.37162840366363525\n",
      "Warning: nan gradient found. The current loss is:  0.341960072517395\n",
      "Warning: nan gradient found. The current loss is:  0.45727330446243286\n",
      "Warning: nan gradient found. The current loss is:  0.09040290862321854\n",
      "Warning: nan gradient found. The current loss is:  2.2605087757110596\n",
      "Warning: nan gradient found. The current loss is:  0.4512521028518677\n",
      "Warning: nan gradient found. The current loss is:  0.7873729467391968\n",
      "Warning: nan gradient found. The current loss is:  0.6229177117347717\n",
      "Warning: nan gradient found. The current loss is:  0.1377342939376831\n",
      "Warning: nan gradient found. The current loss is:  0.31739845871925354\n",
      "Warning: nan gradient found. The current loss is:  0.2827063798904419\n",
      "Warning: nan gradient found. The current loss is:  0.45418858528137207\n",
      "Warning: nan gradient found. The current loss is:  0.4035301208496094\n",
      "Warning: nan gradient found. The current loss is:  0.7261112332344055\n",
      "Warning: nan gradient found. The current loss is:  0.3776349127292633\n",
      "Current batch training loss: 0.377635  [844800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2608930170536041\n",
      "Warning: nan gradient found. The current loss is:  0.5997052788734436\n",
      "Warning: nan gradient found. The current loss is:  0.5231427550315857\n",
      "Warning: nan gradient found. The current loss is:  0.3684661388397217\n",
      "Warning: nan gradient found. The current loss is:  0.5176935195922852\n",
      "Warning: nan gradient found. The current loss is:  0.09643695503473282\n",
      "Warning: nan gradient found. The current loss is:  0.1965540200471878\n",
      "Warning: nan gradient found. The current loss is:  0.531798779964447\n",
      "Warning: nan gradient found. The current loss is:  0.6856449842453003\n",
      "Warning: nan gradient found. The current loss is:  0.3360854387283325\n",
      "Warning: nan gradient found. The current loss is:  0.361200213432312\n",
      "Warning: nan gradient found. The current loss is:  0.474850058555603\n",
      "Warning: nan gradient found. The current loss is:  0.406965047121048\n",
      "Warning: nan gradient found. The current loss is:  0.6695632934570312\n",
      "Warning: nan gradient found. The current loss is:  0.16770906746387482\n",
      "Warning: nan gradient found. The current loss is:  0.7570556402206421\n",
      "Warning: nan gradient found. The current loss is:  0.7043266296386719\n",
      "Warning: nan gradient found. The current loss is:  0.44784414768218994\n",
      "Warning: nan gradient found. The current loss is:  0.8455681204795837\n",
      "Warning: nan gradient found. The current loss is:  0.7191457152366638\n",
      "Warning: nan gradient found. The current loss is:  0.6877450942993164\n",
      "Warning: nan gradient found. The current loss is:  0.6418268084526062\n",
      "Warning: nan gradient found. The current loss is:  0.4777911305427551\n",
      "Warning: nan gradient found. The current loss is:  0.24349169433116913\n",
      "Warning: nan gradient found. The current loss is:  0.4643695056438446\n",
      "Warning: nan gradient found. The current loss is:  0.5426708459854126\n",
      "Warning: nan gradient found. The current loss is:  0.485746294260025\n",
      "Warning: nan gradient found. The current loss is:  0.589901328086853\n",
      "Warning: nan gradient found. The current loss is:  0.07463295757770538\n",
      "Warning: nan gradient found. The current loss is:  0.999350368976593\n",
      "Warning: nan gradient found. The current loss is:  0.3619747459888458\n",
      "Warning: nan gradient found. The current loss is:  0.029681358486413956\n",
      "Warning: nan gradient found. The current loss is:  0.7233926653862\n",
      "Warning: nan gradient found. The current loss is:  0.48698803782463074\n",
      "Warning: nan gradient found. The current loss is:  0.056259967386722565\n",
      "Warning: nan gradient found. The current loss is:  0.2711796462535858\n",
      "Warning: nan gradient found. The current loss is:  0.1118604987859726\n",
      "Warning: nan gradient found. The current loss is:  0.45096728205680847\n",
      "Warning: nan gradient found. The current loss is:  0.7713335156440735\n",
      "Warning: nan gradient found. The current loss is:  0.1649688184261322\n",
      "Warning: nan gradient found. The current loss is:  0.3733948767185211\n",
      "Warning: nan gradient found. The current loss is:  0.17958417534828186\n",
      "Warning: nan gradient found. The current loss is:  0.23143988847732544\n",
      "Warning: nan gradient found. The current loss is:  0.6330701112747192\n",
      "Warning: nan gradient found. The current loss is:  -0.05220891535282135\n",
      "Warning: nan gradient found. The current loss is:  0.1252988874912262\n",
      "Warning: nan gradient found. The current loss is:  0.638458251953125\n",
      "Warning: nan gradient found. The current loss is:  0.2957191467285156\n",
      "Warning: nan gradient found. The current loss is:  0.20793163776397705\n",
      "Warning: nan gradient found. The current loss is:  0.33008748292922974\n",
      "Warning: nan gradient found. The current loss is:  0.6348004341125488\n",
      "Warning: nan gradient found. The current loss is:  0.4797371029853821\n",
      "Warning: nan gradient found. The current loss is:  0.5239753127098083\n",
      "Warning: nan gradient found. The current loss is:  0.32815152406692505\n",
      "Warning: nan gradient found. The current loss is:  0.4719979166984558\n",
      "Warning: nan gradient found. The current loss is:  0.2811034917831421\n",
      "Warning: nan gradient found. The current loss is:  1.0289727449417114\n",
      "Warning: nan gradient found. The current loss is:  0.25637176632881165\n",
      "Warning: nan gradient found. The current loss is:  0.46417802572250366\n",
      "Warning: nan gradient found. The current loss is:  0.9877371788024902\n",
      "Warning: nan gradient found. The current loss is:  0.6336343288421631\n",
      "Warning: nan gradient found. The current loss is:  0.3392864465713501\n",
      "Warning: nan gradient found. The current loss is:  0.25316131114959717\n",
      "Warning: nan gradient found. The current loss is:  0.9644536972045898\n",
      "Warning: nan gradient found. The current loss is:  0.26737654209136963\n",
      "Warning: nan gradient found. The current loss is:  0.29639479517936707\n",
      "Warning: nan gradient found. The current loss is:  0.2656863331794739\n",
      "Warning: nan gradient found. The current loss is:  0.09409719705581665\n",
      "Warning: nan gradient found. The current loss is:  0.4129226505756378\n",
      "Warning: nan gradient found. The current loss is:  0.8075741529464722\n",
      "Warning: nan gradient found. The current loss is:  0.3634882867336273\n",
      "Warning: nan gradient found. The current loss is:  0.36580678820610046\n",
      "Warning: nan gradient found. The current loss is:  0.7306485176086426\n",
      "Warning: nan gradient found. The current loss is:  1.0095224380493164\n",
      "Warning: nan gradient found. The current loss is:  0.37138447165489197\n",
      "Warning: nan gradient found. The current loss is:  1.6167314052581787\n",
      "Warning: nan gradient found. The current loss is:  0.5281304121017456\n",
      "Warning: nan gradient found. The current loss is:  0.9542288184165955\n",
      "Warning: nan gradient found. The current loss is:  0.5476356744766235\n",
      "Warning: nan gradient found. The current loss is:  0.19450008869171143\n",
      "Warning: nan gradient found. The current loss is:  0.14191041886806488\n",
      "Warning: nan gradient found. The current loss is:  0.6779048442840576\n",
      "Warning: nan gradient found. The current loss is:  0.6634954810142517\n",
      "Warning: nan gradient found. The current loss is:  0.5629986524581909\n",
      "Warning: nan gradient found. The current loss is:  2.68845272064209\n",
      "Warning: nan gradient found. The current loss is:  0.4327203631401062\n",
      "Warning: nan gradient found. The current loss is:  0.43952375650405884\n",
      "Warning: nan gradient found. The current loss is:  0.5231226682662964\n",
      "Warning: nan gradient found. The current loss is:  0.4167449474334717\n",
      "Warning: nan gradient found. The current loss is:  0.7824037075042725\n",
      "Warning: nan gradient found. The current loss is:  0.9325169324874878\n",
      "Warning: nan gradient found. The current loss is:  0.6180704832077026\n",
      "Warning: nan gradient found. The current loss is:  0.3943929672241211\n",
      "Warning: nan gradient found. The current loss is:  0.4946531057357788\n",
      "Warning: nan gradient found. The current loss is:  0.07354309409856796\n",
      "Warning: nan gradient found. The current loss is:  0.23311984539031982\n",
      "Warning: nan gradient found. The current loss is:  0.1077323853969574\n",
      "Warning: nan gradient found. The current loss is:  0.615342915058136\n",
      "Warning: nan gradient found. The current loss is:  0.36469316482543945\n",
      "Warning: nan gradient found. The current loss is:  0.3728567361831665\n",
      "Current batch training loss: 0.372857  [870400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4299890100955963\n",
      "Warning: nan gradient found. The current loss is:  0.3138139843940735\n",
      "Warning: nan gradient found. The current loss is:  1.3318270444869995\n",
      "Warning: nan gradient found. The current loss is:  0.20614419877529144\n",
      "Warning: nan gradient found. The current loss is:  0.3040623366832733\n",
      "Warning: nan gradient found. The current loss is:  0.377680242061615\n",
      "Warning: nan gradient found. The current loss is:  0.510193407535553\n",
      "Warning: nan gradient found. The current loss is:  0.1650235652923584\n",
      "Warning: nan gradient found. The current loss is:  0.2771797478199005\n",
      "Warning: nan gradient found. The current loss is:  0.7565007209777832\n",
      "Warning: nan gradient found. The current loss is:  0.447781503200531\n",
      "Warning: nan gradient found. The current loss is:  0.14269861578941345\n",
      "Warning: nan gradient found. The current loss is:  0.6914352178573608\n",
      "Warning: nan gradient found. The current loss is:  -0.02566663920879364\n",
      "Warning: nan gradient found. The current loss is:  0.3130052983760834\n",
      "Warning: nan gradient found. The current loss is:  0.07526619732379913\n",
      "Warning: nan gradient found. The current loss is:  0.8713475465774536\n",
      "Warning: nan gradient found. The current loss is:  0.12176764011383057\n",
      "Warning: nan gradient found. The current loss is:  -0.01273849606513977\n",
      "Warning: nan gradient found. The current loss is:  0.2493027001619339\n",
      "Warning: nan gradient found. The current loss is:  0.44988375902175903\n",
      "Warning: nan gradient found. The current loss is:  0.5739161372184753\n",
      "Warning: nan gradient found. The current loss is:  0.29450690746307373\n",
      "Warning: nan gradient found. The current loss is:  0.24058189988136292\n",
      "Warning: nan gradient found. The current loss is:  0.17917606234550476\n",
      "Warning: nan gradient found. The current loss is:  0.5528504848480225\n",
      "Warning: nan gradient found. The current loss is:  0.6205425262451172\n",
      "Warning: nan gradient found. The current loss is:  0.5625401735305786\n",
      "Warning: nan gradient found. The current loss is:  1.056917667388916\n",
      "Warning: nan gradient found. The current loss is:  0.8860092759132385\n",
      "Warning: nan gradient found. The current loss is:  0.2634575366973877\n",
      "Warning: nan gradient found. The current loss is:  0.3561917841434479\n",
      "Warning: nan gradient found. The current loss is:  0.5163496136665344\n",
      "Warning: nan gradient found. The current loss is:  0.20117779076099396\n",
      "Warning: nan gradient found. The current loss is:  0.2686203122138977\n",
      "Warning: nan gradient found. The current loss is:  0.47302329540252686\n",
      "Warning: nan gradient found. The current loss is:  0.2566823959350586\n",
      "Warning: nan gradient found. The current loss is:  0.08945931494235992\n",
      "Warning: nan gradient found. The current loss is:  0.4538726508617401\n",
      "Warning: nan gradient found. The current loss is:  0.07396483421325684\n",
      "Warning: nan gradient found. The current loss is:  0.8634132146835327\n",
      "Warning: nan gradient found. The current loss is:  0.6560352444648743\n",
      "Warning: nan gradient found. The current loss is:  -0.03812048211693764\n",
      "Warning: nan gradient found. The current loss is:  0.9339731931686401\n",
      "Warning: nan gradient found. The current loss is:  0.34314659237861633\n",
      "Warning: nan gradient found. The current loss is:  0.2690230906009674\n",
      "Warning: nan gradient found. The current loss is:  0.7804380655288696\n",
      "Warning: nan gradient found. The current loss is:  0.461548388004303\n",
      "Warning: nan gradient found. The current loss is:  0.14653249084949493\n",
      "Warning: nan gradient found. The current loss is:  0.4011530876159668\n",
      "Warning: nan gradient found. The current loss is:  1.0770562887191772\n",
      "Warning: nan gradient found. The current loss is:  0.6405841112136841\n",
      "Warning: nan gradient found. The current loss is:  0.5534031987190247\n",
      "Warning: nan gradient found. The current loss is:  0.6599813103675842\n",
      "Warning: nan gradient found. The current loss is:  0.4372605085372925\n",
      "Warning: nan gradient found. The current loss is:  0.8032611012458801\n",
      "Warning: nan gradient found. The current loss is:  1.3671458959579468\n",
      "Warning: nan gradient found. The current loss is:  0.1662900745868683\n",
      "Warning: nan gradient found. The current loss is:  0.14252710342407227\n",
      "Warning: nan gradient found. The current loss is:  0.1576562523841858\n",
      "Warning: nan gradient found. The current loss is:  0.30365538597106934\n",
      "Warning: nan gradient found. The current loss is:  0.597382128238678\n",
      "Warning: nan gradient found. The current loss is:  0.47017550468444824\n",
      "Warning: nan gradient found. The current loss is:  0.27207034826278687\n",
      "Warning: nan gradient found. The current loss is:  0.44270989298820496\n",
      "Warning: nan gradient found. The current loss is:  0.32183170318603516\n",
      "Warning: nan gradient found. The current loss is:  0.10405013710260391\n",
      "Warning: nan gradient found. The current loss is:  0.6691851615905762\n",
      "Warning: nan gradient found. The current loss is:  0.48207154870033264\n",
      "Warning: nan gradient found. The current loss is:  0.31747007369995117\n",
      "Warning: nan gradient found. The current loss is:  0.4071795344352722\n",
      "Warning: nan gradient found. The current loss is:  0.9607385396957397\n",
      "Warning: nan gradient found. The current loss is:  0.37612563371658325\n",
      "Warning: nan gradient found. The current loss is:  0.42383119463920593\n",
      "Warning: nan gradient found. The current loss is:  0.6832848787307739\n",
      "Warning: nan gradient found. The current loss is:  0.12358399480581284\n",
      "Warning: nan gradient found. The current loss is:  0.5400136709213257\n",
      "Warning: nan gradient found. The current loss is:  0.5309150815010071\n",
      "Warning: nan gradient found. The current loss is:  0.38096651434898376\n",
      "Warning: nan gradient found. The current loss is:  0.5478792786598206\n",
      "Warning: nan gradient found. The current loss is:  0.48381975293159485\n",
      "Warning: nan gradient found. The current loss is:  2.943075180053711\n",
      "Warning: nan gradient found. The current loss is:  0.3707210421562195\n",
      "Warning: nan gradient found. The current loss is:  0.054658494889736176\n",
      "Warning: nan gradient found. The current loss is:  0.015040028840303421\n",
      "Warning: nan gradient found. The current loss is:  0.32980096340179443\n",
      "Warning: nan gradient found. The current loss is:  0.6129969358444214\n",
      "Warning: nan gradient found. The current loss is:  0.37177774310112\n",
      "Warning: nan gradient found. The current loss is:  0.37069517374038696\n",
      "Warning: nan gradient found. The current loss is:  0.34336042404174805\n",
      "Warning: nan gradient found. The current loss is:  0.51693195104599\n",
      "Warning: nan gradient found. The current loss is:  0.22485888004302979\n",
      "Warning: nan gradient found. The current loss is:  0.2999193072319031\n",
      "Warning: nan gradient found. The current loss is:  0.48892641067504883\n",
      "Warning: nan gradient found. The current loss is:  0.418836772441864\n",
      "Warning: nan gradient found. The current loss is:  0.21361617743968964\n",
      "Warning: nan gradient found. The current loss is:  0.27019596099853516\n",
      "Warning: nan gradient found. The current loss is:  0.4298596978187561\n",
      "Warning: nan gradient found. The current loss is:  0.37906283140182495\n",
      "Warning: nan gradient found. The current loss is:  0.37682968378067017\n",
      "Current batch training loss: 0.376830  [896000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.47433197498321533\n",
      "Warning: nan gradient found. The current loss is:  0.13888758420944214\n",
      "Warning: nan gradient found. The current loss is:  0.5024792551994324\n",
      "Warning: nan gradient found. The current loss is:  0.18987198173999786\n",
      "Warning: nan gradient found. The current loss is:  0.4274578094482422\n",
      "Warning: nan gradient found. The current loss is:  0.051724717020988464\n",
      "Warning: nan gradient found. The current loss is:  0.2606947422027588\n",
      "Warning: nan gradient found. The current loss is:  0.5237306952476501\n",
      "Warning: nan gradient found. The current loss is:  0.15130583941936493\n",
      "Warning: nan gradient found. The current loss is:  0.4579056203365326\n",
      "Warning: nan gradient found. The current loss is:  0.3675687611103058\n",
      "Warning: nan gradient found. The current loss is:  0.3931022584438324\n",
      "Warning: nan gradient found. The current loss is:  0.5110723972320557\n",
      "Warning: nan gradient found. The current loss is:  0.2977330684661865\n",
      "Warning: nan gradient found. The current loss is:  0.1984703689813614\n",
      "Warning: nan gradient found. The current loss is:  0.7730041742324829\n",
      "Warning: nan gradient found. The current loss is:  0.5055028200149536\n",
      "Warning: nan gradient found. The current loss is:  0.35297438502311707\n",
      "Warning: nan gradient found. The current loss is:  0.3968386650085449\n",
      "Warning: nan gradient found. The current loss is:  0.7400709986686707\n",
      "Warning: nan gradient found. The current loss is:  0.5823490619659424\n",
      "Warning: nan gradient found. The current loss is:  0.10270316153764725\n",
      "Warning: nan gradient found. The current loss is:  0.9944325685501099\n",
      "Warning: nan gradient found. The current loss is:  0.3944846987724304\n",
      "Warning: nan gradient found. The current loss is:  0.5733478665351868\n",
      "Warning: nan gradient found. The current loss is:  0.6485182046890259\n",
      "Warning: nan gradient found. The current loss is:  0.6411898732185364\n",
      "Warning: nan gradient found. The current loss is:  0.7016022205352783\n",
      "Warning: nan gradient found. The current loss is:  0.9864002466201782\n",
      "Warning: nan gradient found. The current loss is:  0.6132761240005493\n",
      "Warning: nan gradient found. The current loss is:  0.41825154423713684\n",
      "Warning: nan gradient found. The current loss is:  0.551532506942749\n",
      "Warning: nan gradient found. The current loss is:  0.15893487632274628\n",
      "Warning: nan gradient found. The current loss is:  0.2363695502281189\n",
      "Warning: nan gradient found. The current loss is:  -0.16367293894290924\n",
      "Warning: nan gradient found. The current loss is:  0.25126883387565613\n",
      "Warning: nan gradient found. The current loss is:  0.1583864539861679\n",
      "Warning: nan gradient found. The current loss is:  0.3995186388492584\n",
      "Warning: nan gradient found. The current loss is:  0.9398237466812134\n",
      "Warning: nan gradient found. The current loss is:  0.41846951842308044\n",
      "Warning: nan gradient found. The current loss is:  0.4077070355415344\n",
      "Warning: nan gradient found. The current loss is:  0.09138228744268417\n",
      "Warning: nan gradient found. The current loss is:  0.21685796976089478\n",
      "Warning: nan gradient found. The current loss is:  0.7991305589675903\n",
      "Warning: nan gradient found. The current loss is:  0.3794209957122803\n",
      "Warning: nan gradient found. The current loss is:  0.38078227639198303\n",
      "Warning: nan gradient found. The current loss is:  0.4560387134552002\n",
      "Warning: nan gradient found. The current loss is:  0.7521416544914246\n",
      "Warning: nan gradient found. The current loss is:  0.3730819821357727\n",
      "Warning: nan gradient found. The current loss is:  0.5564595460891724\n",
      "Warning: nan gradient found. The current loss is:  0.2953507602214813\n",
      "Warning: nan gradient found. The current loss is:  0.17111489176750183\n",
      "Warning: nan gradient found. The current loss is:  0.26067402958869934\n",
      "Warning: nan gradient found. The current loss is:  -0.013407111167907715\n",
      "Warning: nan gradient found. The current loss is:  0.23785662651062012\n",
      "Warning: nan gradient found. The current loss is:  0.4935546815395355\n",
      "Warning: nan gradient found. The current loss is:  0.5455871820449829\n",
      "Warning: nan gradient found. The current loss is:  0.5236124992370605\n",
      "Warning: nan gradient found. The current loss is:  0.45454689860343933\n",
      "Warning: nan gradient found. The current loss is:  0.4092978835105896\n",
      "Warning: nan gradient found. The current loss is:  0.13683576881885529\n",
      "Warning: nan gradient found. The current loss is:  0.7283283472061157\n",
      "Warning: nan gradient found. The current loss is:  0.9700242280960083\n",
      "Warning: nan gradient found. The current loss is:  0.4983985722064972\n",
      "Warning: nan gradient found. The current loss is:  0.6271162033081055\n",
      "Warning: nan gradient found. The current loss is:  0.30815017223358154\n",
      "Warning: nan gradient found. The current loss is:  0.24629083275794983\n",
      "Warning: nan gradient found. The current loss is:  0.4625793993473053\n",
      "Warning: nan gradient found. The current loss is:  0.5730298161506653\n",
      "Warning: nan gradient found. The current loss is:  0.181718647480011\n",
      "Warning: nan gradient found. The current loss is:  0.1667083203792572\n",
      "Warning: nan gradient found. The current loss is:  1.7394397258758545\n",
      "Warning: nan gradient found. The current loss is:  0.33129197359085083\n",
      "Warning: nan gradient found. The current loss is:  0.761817455291748\n",
      "Warning: nan gradient found. The current loss is:  0.5833107233047485\n",
      "Warning: nan gradient found. The current loss is:  0.339699923992157\n",
      "Warning: nan gradient found. The current loss is:  0.2602040767669678\n",
      "Warning: nan gradient found. The current loss is:  0.7462214231491089\n",
      "Warning: nan gradient found. The current loss is:  0.401300311088562\n",
      "Warning: nan gradient found. The current loss is:  0.25623229146003723\n",
      "Warning: nan gradient found. The current loss is:  0.4885483384132385\n",
      "Warning: nan gradient found. The current loss is:  0.2863336205482483\n",
      "Warning: nan gradient found. The current loss is:  0.7745202779769897\n",
      "Warning: nan gradient found. The current loss is:  0.7649966478347778\n",
      "Warning: nan gradient found. The current loss is:  0.9291554689407349\n",
      "Warning: nan gradient found. The current loss is:  1.066921591758728\n",
      "Warning: nan gradient found. The current loss is:  0.6315270662307739\n",
      "Warning: nan gradient found. The current loss is:  0.7981722354888916\n",
      "Warning: nan gradient found. The current loss is:  0.6004559397697449\n",
      "Warning: nan gradient found. The current loss is:  0.44444066286087036\n",
      "Warning: nan gradient found. The current loss is:  0.36917412281036377\n",
      "Warning: nan gradient found. The current loss is:  1.466880440711975\n",
      "Warning: nan gradient found. The current loss is:  0.47901666164398193\n",
      "Warning: nan gradient found. The current loss is:  0.6046820878982544\n",
      "Warning: nan gradient found. The current loss is:  0.5869709253311157\n",
      "Warning: nan gradient found. The current loss is:  0.3706580102443695\n",
      "Warning: nan gradient found. The current loss is:  0.28982114791870117\n",
      "Warning: nan gradient found. The current loss is:  0.21020092070102692\n",
      "Warning: nan gradient found. The current loss is:  0.34210848808288574\n",
      "Warning: nan gradient found. The current loss is:  0.08132033050060272\n",
      "Current batch training loss: 0.081320  [921600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.4059898853302002\n",
      "Warning: nan gradient found. The current loss is:  0.22136692702770233\n",
      "Warning: nan gradient found. The current loss is:  0.5219630002975464\n",
      "Warning: nan gradient found. The current loss is:  0.07421925663948059\n",
      "Warning: nan gradient found. The current loss is:  0.4955272376537323\n",
      "Warning: nan gradient found. The current loss is:  0.3314245343208313\n",
      "Warning: nan gradient found. The current loss is:  0.6244610548019409\n",
      "Warning: nan gradient found. The current loss is:  0.34755557775497437\n",
      "Warning: nan gradient found. The current loss is:  0.37784332036972046\n",
      "Warning: nan gradient found. The current loss is:  0.4346722960472107\n",
      "Warning: nan gradient found. The current loss is:  0.5000511407852173\n",
      "Warning: nan gradient found. The current loss is:  0.6993148326873779\n",
      "Warning: nan gradient found. The current loss is:  0.17117741703987122\n",
      "Warning: nan gradient found. The current loss is:  0.33775776624679565\n",
      "Warning: nan gradient found. The current loss is:  0.2765308916568756\n",
      "Warning: nan gradient found. The current loss is:  0.4682319164276123\n",
      "Warning: nan gradient found. The current loss is:  0.31411492824554443\n",
      "Warning: nan gradient found. The current loss is:  0.2535592317581177\n",
      "Warning: nan gradient found. The current loss is:  0.6217015981674194\n",
      "Warning: nan gradient found. The current loss is:  0.5618650913238525\n",
      "Warning: nan gradient found. The current loss is:  0.36401429772377014\n",
      "Warning: nan gradient found. The current loss is:  0.34368619322776794\n",
      "Warning: nan gradient found. The current loss is:  0.2751668691635132\n",
      "Warning: nan gradient found. The current loss is:  1.0098130702972412\n",
      "Warning: nan gradient found. The current loss is:  0.5349744558334351\n",
      "Warning: nan gradient found. The current loss is:  0.006623629480600357\n",
      "Warning: nan gradient found. The current loss is:  0.4129980206489563\n",
      "Warning: nan gradient found. The current loss is:  0.46362659335136414\n",
      "Warning: nan gradient found. The current loss is:  0.5456463694572449\n",
      "Warning: nan gradient found. The current loss is:  0.5191260576248169\n",
      "Warning: nan gradient found. The current loss is:  0.12333455681800842\n",
      "Warning: nan gradient found. The current loss is:  -0.09528425335884094\n",
      "Warning: nan gradient found. The current loss is:  0.6914387941360474\n",
      "Warning: nan gradient found. The current loss is:  0.550240159034729\n",
      "Warning: nan gradient found. The current loss is:  0.5455691814422607\n",
      "Warning: nan gradient found. The current loss is:  0.13922172784805298\n",
      "Warning: nan gradient found. The current loss is:  0.49791648983955383\n",
      "Warning: nan gradient found. The current loss is:  1.1107819080352783\n",
      "Warning: nan gradient found. The current loss is:  0.26026105880737305\n",
      "Warning: nan gradient found. The current loss is:  0.15499667823314667\n",
      "Warning: nan gradient found. The current loss is:  0.8059321045875549\n",
      "Warning: nan gradient found. The current loss is:  0.26176974177360535\n",
      "Warning: nan gradient found. The current loss is:  0.5212411880493164\n",
      "Warning: nan gradient found. The current loss is:  0.42465740442276\n",
      "Warning: nan gradient found. The current loss is:  0.3510552942752838\n",
      "Warning: nan gradient found. The current loss is:  0.4262170195579529\n",
      "Warning: nan gradient found. The current loss is:  0.6290104389190674\n",
      "Warning: nan gradient found. The current loss is:  0.8278059363365173\n",
      "Warning: nan gradient found. The current loss is:  0.9204720854759216\n",
      "Warning: nan gradient found. The current loss is:  0.10361169278621674\n",
      "Warning: nan gradient found. The current loss is:  0.15007902681827545\n",
      "Warning: nan gradient found. The current loss is:  0.0652732402086258\n",
      "Warning: nan gradient found. The current loss is:  0.42749539017677307\n",
      "Warning: nan gradient found. The current loss is:  0.9533374309539795\n",
      "Warning: nan gradient found. The current loss is:  0.24446189403533936\n",
      "Warning: nan gradient found. The current loss is:  0.09898467361927032\n",
      "Warning: nan gradient found. The current loss is:  0.1480584293603897\n",
      "Warning: nan gradient found. The current loss is:  0.3825608193874359\n",
      "Warning: nan gradient found. The current loss is:  0.4836934804916382\n",
      "Warning: nan gradient found. The current loss is:  0.40708380937576294\n",
      "Warning: nan gradient found. The current loss is:  0.4292266368865967\n",
      "Warning: nan gradient found. The current loss is:  0.5184835195541382\n",
      "Warning: nan gradient found. The current loss is:  0.4794657230377197\n",
      "Warning: nan gradient found. The current loss is:  0.17445197701454163\n",
      "Warning: nan gradient found. The current loss is:  0.36696115136146545\n",
      "Warning: nan gradient found. The current loss is:  0.13841620087623596\n",
      "Warning: nan gradient found. The current loss is:  0.4075923562049866\n",
      "Warning: nan gradient found. The current loss is:  0.7241798639297485\n",
      "Warning: nan gradient found. The current loss is:  0.27302247285842896\n",
      "Warning: nan gradient found. The current loss is:  0.628476619720459\n",
      "Warning: nan gradient found. The current loss is:  0.17777834832668304\n",
      "Warning: nan gradient found. The current loss is:  0.2713533341884613\n",
      "Warning: nan gradient found. The current loss is:  0.0388372540473938\n",
      "Warning: nan gradient found. The current loss is:  0.3088359534740448\n",
      "Warning: nan gradient found. The current loss is:  0.7092028856277466\n",
      "Warning: nan gradient found. The current loss is:  0.006497088819742203\n",
      "Warning: nan gradient found. The current loss is:  0.22589346766471863\n",
      "Warning: nan gradient found. The current loss is:  0.817454993724823\n",
      "Warning: nan gradient found. The current loss is:  0.1395421028137207\n",
      "Warning: nan gradient found. The current loss is:  -0.15063241124153137\n",
      "Warning: nan gradient found. The current loss is:  0.15907999873161316\n",
      "Warning: nan gradient found. The current loss is:  0.24388903379440308\n",
      "Warning: nan gradient found. The current loss is:  0.13895101845264435\n",
      "Warning: nan gradient found. The current loss is:  0.3954126238822937\n",
      "Warning: nan gradient found. The current loss is:  0.3018924295902252\n",
      "Warning: nan gradient found. The current loss is:  0.10050638020038605\n",
      "Warning: nan gradient found. The current loss is:  0.2629357576370239\n",
      "Warning: nan gradient found. The current loss is:  0.6443420052528381\n",
      "Warning: nan gradient found. The current loss is:  0.29246801137924194\n",
      "Warning: nan gradient found. The current loss is:  0.33546939492225647\n",
      "Warning: nan gradient found. The current loss is:  0.4219054579734802\n",
      "Warning: nan gradient found. The current loss is:  0.137518972158432\n",
      "Warning: nan gradient found. The current loss is:  0.5864262580871582\n",
      "Warning: nan gradient found. The current loss is:  1.474635362625122\n",
      "Warning: nan gradient found. The current loss is:  0.6738989353179932\n",
      "Warning: nan gradient found. The current loss is:  0.8738540410995483\n",
      "Warning: nan gradient found. The current loss is:  0.8635746240615845\n",
      "Warning: nan gradient found. The current loss is:  0.8761047720909119\n",
      "Warning: nan gradient found. The current loss is:  0.13055595755577087\n",
      "Warning: nan gradient found. The current loss is:  0.6856341361999512\n",
      "Current batch training loss: 0.685634  [947200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.10306736081838608\n",
      "Warning: nan gradient found. The current loss is:  0.5339186787605286\n",
      "Warning: nan gradient found. The current loss is:  0.5973874926567078\n",
      "Warning: nan gradient found. The current loss is:  0.23047545552253723\n",
      "Warning: nan gradient found. The current loss is:  0.654499888420105\n",
      "Warning: nan gradient found. The current loss is:  0.6624224781990051\n",
      "Warning: nan gradient found. The current loss is:  0.2851601541042328\n",
      "Warning: nan gradient found. The current loss is:  0.25994065403938293\n",
      "Warning: nan gradient found. The current loss is:  0.56965571641922\n",
      "Warning: nan gradient found. The current loss is:  0.5930757522583008\n",
      "Warning: nan gradient found. The current loss is:  1.1985163688659668\n",
      "Warning: nan gradient found. The current loss is:  -0.06325117498636246\n",
      "Warning: nan gradient found. The current loss is:  0.12375997006893158\n",
      "Warning: nan gradient found. The current loss is:  0.10724799335002899\n",
      "Warning: nan gradient found. The current loss is:  0.525375247001648\n",
      "Warning: nan gradient found. The current loss is:  0.7164368629455566\n",
      "Warning: nan gradient found. The current loss is:  0.9674869179725647\n",
      "Warning: nan gradient found. The current loss is:  0.09632404148578644\n",
      "Warning: nan gradient found. The current loss is:  0.7865164279937744\n",
      "Warning: nan gradient found. The current loss is:  0.3752044141292572\n",
      "Warning: nan gradient found. The current loss is:  0.28910380601882935\n",
      "Warning: nan gradient found. The current loss is:  0.30725908279418945\n",
      "Warning: nan gradient found. The current loss is:  0.6738201379776001\n",
      "Warning: nan gradient found. The current loss is:  0.17225688695907593\n",
      "Warning: nan gradient found. The current loss is:  0.23446957767009735\n",
      "Warning: nan gradient found. The current loss is:  0.7587916254997253\n",
      "Warning: nan gradient found. The current loss is:  0.4324004650115967\n",
      "Warning: nan gradient found. The current loss is:  0.6146994233131409\n",
      "Warning: nan gradient found. The current loss is:  0.9831040501594543\n",
      "Warning: nan gradient found. The current loss is:  0.3117416501045227\n",
      "Warning: nan gradient found. The current loss is:  0.22885730862617493\n",
      "Warning: nan gradient found. The current loss is:  0.38480257987976074\n",
      "Warning: nan gradient found. The current loss is:  0.9981611371040344\n",
      "Warning: nan gradient found. The current loss is:  0.27616435289382935\n",
      "Warning: nan gradient found. The current loss is:  0.40710949897766113\n",
      "Warning: nan gradient found. The current loss is:  0.44135135412216187\n",
      "Warning: nan gradient found. The current loss is:  0.6657871603965759\n",
      "Warning: nan gradient found. The current loss is:  0.14846578240394592\n",
      "Warning: nan gradient found. The current loss is:  0.042173076421022415\n",
      "Warning: nan gradient found. The current loss is:  0.8990226984024048\n",
      "Warning: nan gradient found. The current loss is:  0.3015063405036926\n",
      "Warning: nan gradient found. The current loss is:  0.7102200984954834\n",
      "Warning: nan gradient found. The current loss is:  0.5693912506103516\n",
      "Warning: nan gradient found. The current loss is:  0.2482568919658661\n",
      "Warning: nan gradient found. The current loss is:  1.6162124872207642\n",
      "Warning: nan gradient found. The current loss is:  0.21040205657482147\n",
      "Warning: nan gradient found. The current loss is:  0.6364339590072632\n",
      "Warning: nan gradient found. The current loss is:  0.6136753559112549\n",
      "Warning: nan gradient found. The current loss is:  0.4839155077934265\n",
      "Warning: nan gradient found. The current loss is:  0.7009467482566833\n",
      "Warning: nan gradient found. The current loss is:  -0.012413844466209412\n",
      "Warning: nan gradient found. The current loss is:  0.6940853595733643\n",
      "Warning: nan gradient found. The current loss is:  0.7852736711502075\n",
      "Warning: nan gradient found. The current loss is:  0.08152100443840027\n",
      "Warning: nan gradient found. The current loss is:  0.5248191952705383\n",
      "Warning: nan gradient found. The current loss is:  0.5813857316970825\n",
      "Warning: nan gradient found. The current loss is:  0.2892524302005768\n",
      "Warning: nan gradient found. The current loss is:  0.4853324294090271\n",
      "Warning: nan gradient found. The current loss is:  0.3575439453125\n",
      "Warning: nan gradient found. The current loss is:  0.3262696862220764\n",
      "Warning: nan gradient found. The current loss is:  0.49802982807159424\n",
      "Warning: nan gradient found. The current loss is:  0.1824084222316742\n",
      "Warning: nan gradient found. The current loss is:  0.38004037737846375\n",
      "Warning: nan gradient found. The current loss is:  -0.0031903088092803955\n",
      "Warning: nan gradient found. The current loss is:  0.31488704681396484\n",
      "Warning: nan gradient found. The current loss is:  0.9658854603767395\n",
      "Warning: nan gradient found. The current loss is:  0.7632542848587036\n",
      "Warning: nan gradient found. The current loss is:  0.7330682873725891\n",
      "Warning: nan gradient found. The current loss is:  0.43943333625793457\n",
      "Warning: nan gradient found. The current loss is:  0.3142073154449463\n",
      "Warning: nan gradient found. The current loss is:  0.3158038258552551\n",
      "Warning: nan gradient found. The current loss is:  0.6003416776657104\n",
      "Warning: nan gradient found. The current loss is:  0.16105888783931732\n",
      "Warning: nan gradient found. The current loss is:  0.5425562262535095\n",
      "Warning: nan gradient found. The current loss is:  0.22145815193653107\n",
      "Warning: nan gradient found. The current loss is:  0.5591020584106445\n",
      "Warning: nan gradient found. The current loss is:  0.507150411605835\n",
      "Warning: nan gradient found. The current loss is:  0.545452356338501\n",
      "Warning: nan gradient found. The current loss is:  0.24222342669963837\n",
      "Warning: nan gradient found. The current loss is:  0.3102097809314728\n",
      "Warning: nan gradient found. The current loss is:  0.9148144721984863\n",
      "Warning: nan gradient found. The current loss is:  0.39365118741989136\n",
      "Warning: nan gradient found. The current loss is:  0.5033793449401855\n",
      "Warning: nan gradient found. The current loss is:  0.11524584144353867\n",
      "Warning: nan gradient found. The current loss is:  0.19136154651641846\n",
      "Warning: nan gradient found. The current loss is:  1.2479627132415771\n",
      "Warning: nan gradient found. The current loss is:  0.5719362497329712\n",
      "Warning: nan gradient found. The current loss is:  0.7500293254852295\n",
      "Warning: nan gradient found. The current loss is:  0.5795114636421204\n",
      "Warning: nan gradient found. The current loss is:  0.30278313159942627\n",
      "Warning: nan gradient found. The current loss is:  0.639264702796936\n",
      "Warning: nan gradient found. The current loss is:  1.2748417854309082\n",
      "Warning: nan gradient found. The current loss is:  0.14114674925804138\n",
      "Warning: nan gradient found. The current loss is:  0.48034563660621643\n",
      "Warning: nan gradient found. The current loss is:  0.2788374125957489\n",
      "Warning: nan gradient found. The current loss is:  0.8869466781616211\n",
      "Warning: nan gradient found. The current loss is:  0.11314067989587784\n",
      "Warning: nan gradient found. The current loss is:  0.6127135753631592\n",
      "Warning: nan gradient found. The current loss is:  0.23235686123371124\n",
      "Warning: nan gradient found. The current loss is:  0.8536919355392456\n",
      "Current batch training loss: 0.853692  [972800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2916973829269409\n",
      "Warning: nan gradient found. The current loss is:  0.4381197392940521\n",
      "Warning: nan gradient found. The current loss is:  0.35444965958595276\n",
      "Warning: nan gradient found. The current loss is:  0.2544315457344055\n",
      "Warning: nan gradient found. The current loss is:  0.26169294118881226\n",
      "Warning: nan gradient found. The current loss is:  0.25797995924949646\n",
      "Warning: nan gradient found. The current loss is:  2.012514591217041\n",
      "Warning: nan gradient found. The current loss is:  0.2693403959274292\n",
      "Warning: nan gradient found. The current loss is:  0.5575103759765625\n",
      "Warning: nan gradient found. The current loss is:  0.15929661691188812\n",
      "Warning: nan gradient found. The current loss is:  0.0962587296962738\n",
      "Warning: nan gradient found. The current loss is:  0.26622891426086426\n",
      "Warning: nan gradient found. The current loss is:  0.4461444616317749\n",
      "Warning: nan gradient found. The current loss is:  -0.00140463188290596\n",
      "Warning: nan gradient found. The current loss is:  0.21252575516700745\n",
      "Warning: nan gradient found. The current loss is:  0.10768017917871475\n",
      "Warning: nan gradient found. The current loss is:  0.32480698823928833\n",
      "Warning: nan gradient found. The current loss is:  0.4638340473175049\n",
      "Warning: nan gradient found. The current loss is:  0.4706411361694336\n",
      "Warning: nan gradient found. The current loss is:  0.4356938600540161\n",
      "Warning: nan gradient found. The current loss is:  1.0734617710113525\n",
      "Warning: nan gradient found. The current loss is:  0.744807243347168\n",
      "Warning: nan gradient found. The current loss is:  -0.04406309872865677\n",
      "Warning: nan gradient found. The current loss is:  0.42916786670684814\n",
      "Warning: nan gradient found. The current loss is:  0.3245629072189331\n",
      "Warning: nan gradient found. The current loss is:  0.08453989773988724\n",
      "Warning: nan gradient found. The current loss is:  0.40210017561912537\n",
      "Warning: nan gradient found. The current loss is:  0.378775417804718\n",
      "Warning: nan gradient found. The current loss is:  0.2912169098854065\n",
      "Warning: nan gradient found. The current loss is:  0.6067413687705994\n",
      "Warning: nan gradient found. The current loss is:  0.5807753205299377\n",
      "Warning: nan gradient found. The current loss is:  0.4520920217037201\n",
      "Warning: nan gradient found. The current loss is:  0.1283755898475647\n",
      "Warning: nan gradient found. The current loss is:  0.42282921075820923\n",
      "Warning: nan gradient found. The current loss is:  0.24483707547187805\n",
      "Warning: nan gradient found. The current loss is:  0.5480664372444153\n",
      "Warning: nan gradient found. The current loss is:  0.597018301486969\n",
      "Warning: nan gradient found. The current loss is:  0.774175763130188\n",
      "Warning: nan gradient found. The current loss is:  0.19628223776817322\n",
      "Warning: nan gradient found. The current loss is:  0.11359792947769165\n",
      "Warning: nan gradient found. The current loss is:  0.3189832866191864\n",
      "Warning: nan gradient found. The current loss is:  0.6885784864425659\n",
      "Warning: nan gradient found. The current loss is:  0.5816227197647095\n",
      "Warning: nan gradient found. The current loss is:  0.5261267423629761\n",
      "Warning: nan gradient found. The current loss is:  0.6273253560066223\n",
      "Warning: nan gradient found. The current loss is:  0.6469329595565796\n",
      "Warning: nan gradient found. The current loss is:  0.46904319524765015\n",
      "Warning: nan gradient found. The current loss is:  0.6091052889823914\n",
      "Warning: nan gradient found. The current loss is:  0.16360579431056976\n",
      "Warning: nan gradient found. The current loss is:  0.10268523544073105\n",
      "Warning: nan gradient found. The current loss is:  1.0138963460922241\n",
      "Warning: nan gradient found. The current loss is:  0.5018128156661987\n",
      "Warning: nan gradient found. The current loss is:  0.3159641921520233\n",
      "Warning: nan gradient found. The current loss is:  0.3076273500919342\n",
      "Warning: nan gradient found. The current loss is:  0.7762515544891357\n",
      "Warning: nan gradient found. The current loss is:  0.25664395093917847\n",
      "Warning: nan gradient found. The current loss is:  0.6171313524246216\n",
      "Warning: nan gradient found. The current loss is:  0.22928796708583832\n",
      "Warning: nan gradient found. The current loss is:  0.9150220155715942\n",
      "Warning: nan gradient found. The current loss is:  0.2082318514585495\n",
      "Warning: nan gradient found. The current loss is:  0.6145368814468384\n",
      "Warning: nan gradient found. The current loss is:  0.8118480443954468\n",
      "Warning: nan gradient found. The current loss is:  0.2878822088241577\n",
      "Warning: nan gradient found. The current loss is:  0.2582912743091583\n",
      "Warning: nan gradient found. The current loss is:  0.5174527168273926\n",
      "Warning: nan gradient found. The current loss is:  0.5344921946525574\n",
      "Warning: nan gradient found. The current loss is:  1.2559435367584229\n",
      "Warning: nan gradient found. The current loss is:  0.5187287330627441\n",
      "Warning: nan gradient found. The current loss is:  0.05464264377951622\n",
      "Warning: nan gradient found. The current loss is:  0.5673539042472839\n",
      "Warning: nan gradient found. The current loss is:  0.32165762782096863\n",
      "Warning: nan gradient found. The current loss is:  0.7955809235572815\n",
      "Warning: nan gradient found. The current loss is:  0.5651348233222961\n",
      "Warning: nan gradient found. The current loss is:  1.1556533575057983\n",
      "Warning: nan gradient found. The current loss is:  0.9897339940071106\n",
      "Warning: nan gradient found. The current loss is:  0.7617985606193542\n",
      "Warning: nan gradient found. The current loss is:  0.3475852906703949\n",
      "Warning: nan gradient found. The current loss is:  0.4507943391799927\n",
      "Warning: nan gradient found. The current loss is:  0.4220685362815857\n",
      "Warning: nan gradient found. The current loss is:  0.25645729899406433\n",
      "Warning: nan gradient found. The current loss is:  0.7121936082839966\n",
      "Warning: nan gradient found. The current loss is:  0.21611160039901733\n",
      "Warning: nan gradient found. The current loss is:  0.32700932025909424\n",
      "Warning: nan gradient found. The current loss is:  0.6859284043312073\n",
      "Warning: nan gradient found. The current loss is:  0.9441273808479309\n",
      "Warning: nan gradient found. The current loss is:  0.2797805368900299\n",
      "Warning: nan gradient found. The current loss is:  0.4931795001029968\n",
      "Warning: nan gradient found. The current loss is:  0.4845295548439026\n",
      "Warning: nan gradient found. The current loss is:  0.3455408811569214\n",
      "Warning: nan gradient found. The current loss is:  0.3149057626724243\n",
      "Warning: nan gradient found. The current loss is:  0.5345487594604492\n",
      "Warning: nan gradient found. The current loss is:  0.8786299228668213\n",
      "Warning: nan gradient found. The current loss is:  1.191943883895874\n",
      "Warning: nan gradient found. The current loss is:  0.24155987799167633\n",
      "Warning: nan gradient found. The current loss is:  0.7024705410003662\n",
      "Warning: nan gradient found. The current loss is:  0.6974573135375977\n",
      "Warning: nan gradient found. The current loss is:  0.5697391629219055\n",
      "Warning: nan gradient found. The current loss is:  1.049515962600708\n",
      "Warning: nan gradient found. The current loss is:  0.5647248029708862\n",
      "Current batch training loss: 0.564725  [998400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6734628081321716\n",
      "Warning: nan gradient found. The current loss is:  0.5026308298110962\n",
      "Warning: nan gradient found. The current loss is:  0.9381576776504517\n",
      "Warning: nan gradient found. The current loss is:  -0.010385096073150635\n",
      "Warning: nan gradient found. The current loss is:  0.5649551153182983\n",
      "Warning: nan gradient found. The current loss is:  0.714154839515686\n",
      "Warning: nan gradient found. The current loss is:  0.20218497514724731\n",
      "Warning: nan gradient found. The current loss is:  0.2840632200241089\n",
      "Warning: nan gradient found. The current loss is:  0.25387412309646606\n",
      "Warning: nan gradient found. The current loss is:  -0.06903210282325745\n",
      "Warning: nan gradient found. The current loss is:  0.6008655428886414\n",
      "Warning: nan gradient found. The current loss is:  0.9288908243179321\n",
      "Warning: nan gradient found. The current loss is:  0.399472713470459\n",
      "Warning: nan gradient found. The current loss is:  1.1955387592315674\n",
      "Warning: nan gradient found. The current loss is:  0.7565213441848755\n",
      "Warning: nan gradient found. The current loss is:  0.762953519821167\n",
      "Warning: nan gradient found. The current loss is:  0.4085714519023895\n",
      "Warning: nan gradient found. The current loss is:  0.7246688604354858\n",
      "Warning: nan gradient found. The current loss is:  0.5066239237785339\n",
      "Warning: nan gradient found. The current loss is:  0.4266953468322754\n",
      "Warning: nan gradient found. The current loss is:  0.38051319122314453\n",
      "Warning: nan gradient found. The current loss is:  1.5824521780014038\n",
      "Warning: nan gradient found. The current loss is:  1.5148611068725586\n",
      "Warning: nan gradient found. The current loss is:  0.3907609283924103\n",
      "Warning: nan gradient found. The current loss is:  0.8787720203399658\n",
      "Warning: nan gradient found. The current loss is:  0.6603592038154602\n",
      "Warning: nan gradient found. The current loss is:  0.025303950533270836\n",
      "Warning: nan gradient found. The current loss is:  0.6125590205192566\n",
      "Warning: nan gradient found. The current loss is:  0.20882783830165863\n",
      "Warning: nan gradient found. The current loss is:  0.7017215490341187\n",
      "Warning: nan gradient found. The current loss is:  0.6691174507141113\n",
      "Warning: nan gradient found. The current loss is:  0.17122593522071838\n",
      "Warning: nan gradient found. The current loss is:  0.4912424683570862\n",
      "Warning: nan gradient found. The current loss is:  0.14973722398281097\n",
      "Warning: nan gradient found. The current loss is:  0.5279117822647095\n",
      "Warning: nan gradient found. The current loss is:  0.5788130760192871\n",
      "Warning: nan gradient found. The current loss is:  0.7047238349914551\n",
      "Warning: nan gradient found. The current loss is:  0.29585909843444824\n",
      "Warning: nan gradient found. The current loss is:  0.7680163383483887\n",
      "Warning: nan gradient found. The current loss is:  0.23192302882671356\n",
      "Warning: nan gradient found. The current loss is:  0.5400176644325256\n",
      "Warning: nan gradient found. The current loss is:  0.2881983518600464\n",
      "Warning: nan gradient found. The current loss is:  0.21988102793693542\n",
      "Warning: nan gradient found. The current loss is:  0.31815898418426514\n",
      "Warning: nan gradient found. The current loss is:  0.8246995210647583\n",
      "Warning: nan gradient found. The current loss is:  0.925398588180542\n",
      "Warning: nan gradient found. The current loss is:  0.7553631663322449\n",
      "Warning: nan gradient found. The current loss is:  0.5428112745285034\n",
      "Warning: nan gradient found. The current loss is:  0.7448204159736633\n",
      "Warning: nan gradient found. The current loss is:  0.46361401677131653\n",
      "Warning: nan gradient found. The current loss is:  0.4005403518676758\n",
      "Warning: nan gradient found. The current loss is:  0.41246867179870605\n",
      "Warning: nan gradient found. The current loss is:  0.7488524913787842\n",
      "Warning: nan gradient found. The current loss is:  1.5808416604995728\n",
      "Warning: nan gradient found. The current loss is:  0.7264108061790466\n",
      "Warning: nan gradient found. The current loss is:  0.6749580502510071\n",
      "Warning: nan gradient found. The current loss is:  0.1686183214187622\n",
      "Warning: nan gradient found. The current loss is:  0.6332724094390869\n",
      "Warning: nan gradient found. The current loss is:  0.20283879339694977\n",
      "Warning: nan gradient found. The current loss is:  1.068420171737671\n",
      "Warning: nan gradient found. The current loss is:  0.282284677028656\n",
      "Warning: nan gradient found. The current loss is:  0.802159309387207\n",
      "Warning: nan gradient found. The current loss is:  0.7134783864021301\n",
      "Warning: nan gradient found. The current loss is:  0.5709162950515747\n",
      "Warning: nan gradient found. The current loss is:  0.32621729373931885\n",
      "Warning: nan gradient found. The current loss is:  0.18495039641857147\n",
      "Warning: nan gradient found. The current loss is:  0.8660104274749756\n",
      "Warning: nan gradient found. The current loss is:  1.099632978439331\n",
      "Warning: nan gradient found. The current loss is:  1.668586254119873\n",
      "Warning: nan gradient found. The current loss is:  0.24418208003044128\n",
      "Warning: nan gradient found. The current loss is:  1.2974472045898438\n",
      "Warning: nan gradient found. The current loss is:  0.632548451423645\n",
      "Warning: nan gradient found. The current loss is:  0.9072144627571106\n",
      "Warning: nan gradient found. The current loss is:  0.3678503632545471\n",
      "Warning: nan gradient found. The current loss is:  1.0076918601989746\n",
      "Warning: nan gradient found. The current loss is:  0.8580693006515503\n",
      "Warning: nan gradient found. The current loss is:  0.6024738550186157\n",
      "Warning: nan gradient found. The current loss is:  0.5421974658966064\n",
      "Warning: nan gradient found. The current loss is:  0.5417878031730652\n",
      "Warning: nan gradient found. The current loss is:  0.5061455965042114\n",
      "Warning: nan gradient found. The current loss is:  0.2471809834241867\n",
      "Warning: nan gradient found. The current loss is:  0.9376683831214905\n",
      "Warning: nan gradient found. The current loss is:  0.4017866253852844\n",
      "Warning: nan gradient found. The current loss is:  2.261000871658325\n",
      "Warning: nan gradient found. The current loss is:  0.4084867835044861\n",
      "Warning: nan gradient found. The current loss is:  0.29236119985580444\n",
      "Warning: nan gradient found. The current loss is:  2.39144229888916\n",
      "Warning: nan gradient found. The current loss is:  0.6490764021873474\n",
      "Warning: nan gradient found. The current loss is:  0.9514591693878174\n",
      "Warning: nan gradient found. The current loss is:  1.9239462614059448\n",
      "Warning: nan gradient found. The current loss is:  0.38933810591697693\n",
      "Warning: nan gradient found. The current loss is:  0.6682340502738953\n",
      "Warning: nan gradient found. The current loss is:  0.5135920643806458\n",
      "Warning: nan gradient found. The current loss is:  0.5820910930633545\n",
      "Warning: nan gradient found. The current loss is:  0.5743448138237\n",
      "Warning: nan gradient found. The current loss is:  0.2827773094177246\n",
      "Warning: nan gradient found. The current loss is:  0.10460173338651657\n",
      "Warning: nan gradient found. The current loss is:  0.3701211214065552\n",
      "Warning: nan gradient found. The current loss is:  1.3961060047149658\n",
      "Warning: nan gradient found. The current loss is:  0.8739879131317139\n",
      "Current batch training loss: 0.873988  [1024000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6458580493927002\n",
      "Warning: nan gradient found. The current loss is:  0.8726688623428345\n",
      "Warning: nan gradient found. The current loss is:  0.21469220519065857\n",
      "Warning: nan gradient found. The current loss is:  0.748683750629425\n",
      "Warning: nan gradient found. The current loss is:  0.35333481431007385\n",
      "Warning: nan gradient found. The current loss is:  0.2980974614620209\n",
      "Warning: nan gradient found. The current loss is:  0.7345165014266968\n",
      "Warning: nan gradient found. The current loss is:  0.1605704426765442\n",
      "Warning: nan gradient found. The current loss is:  0.8470792174339294\n",
      "Warning: nan gradient found. The current loss is:  0.3481624722480774\n",
      "Warning: nan gradient found. The current loss is:  0.7753061056137085\n",
      "Warning: nan gradient found. The current loss is:  0.5113070011138916\n",
      "Warning: nan gradient found. The current loss is:  0.5586961507797241\n",
      "Warning: nan gradient found. The current loss is:  0.928663969039917\n",
      "Warning: nan gradient found. The current loss is:  0.3770042359828949\n",
      "Warning: nan gradient found. The current loss is:  0.8934773802757263\n",
      "Warning: nan gradient found. The current loss is:  0.27058112621307373\n",
      "Warning: nan gradient found. The current loss is:  1.3663620948791504\n",
      "Warning: nan gradient found. The current loss is:  0.3423747420310974\n",
      "Warning: nan gradient found. The current loss is:  0.37941664457321167\n",
      "Warning: nan gradient found. The current loss is:  0.1558934897184372\n",
      "Warning: nan gradient found. The current loss is:  1.040013313293457\n",
      "Warning: nan gradient found. The current loss is:  0.09932994842529297\n",
      "Warning: nan gradient found. The current loss is:  0.8677335977554321\n",
      "Warning: nan gradient found. The current loss is:  0.6496039628982544\n",
      "Warning: nan gradient found. The current loss is:  0.8308594822883606\n",
      "Warning: nan gradient found. The current loss is:  0.43432456254959106\n",
      "Warning: nan gradient found. The current loss is:  1.1546201705932617\n",
      "Warning: nan gradient found. The current loss is:  0.6216475963592529\n",
      "Warning: nan gradient found. The current loss is:  1.8638148307800293\n",
      "Warning: nan gradient found. The current loss is:  0.5953118801116943\n",
      "Warning: nan gradient found. The current loss is:  0.1666058450937271\n",
      "Warning: nan gradient found. The current loss is:  -0.022673413157463074\n",
      "Warning: nan gradient found. The current loss is:  0.8115659356117249\n",
      "Warning: nan gradient found. The current loss is:  0.3541923463344574\n",
      "Warning: nan gradient found. The current loss is:  0.2990855574607849\n",
      "Warning: nan gradient found. The current loss is:  0.19199441373348236\n",
      "Warning: nan gradient found. The current loss is:  0.5185214281082153\n",
      "Warning: nan gradient found. The current loss is:  0.5303475856781006\n",
      "Warning: nan gradient found. The current loss is:  0.5049871802330017\n",
      "Warning: nan gradient found. The current loss is:  0.3853648006916046\n",
      "Warning: nan gradient found. The current loss is:  0.29270830750465393\n",
      "Warning: nan gradient found. The current loss is:  0.33420056104660034\n",
      "Warning: nan gradient found. The current loss is:  1.0827394723892212\n",
      "Warning: nan gradient found. The current loss is:  0.5704202651977539\n",
      "Warning: nan gradient found. The current loss is:  0.5549477338790894\n",
      "Warning: nan gradient found. The current loss is:  1.9407765865325928\n",
      "Warning: nan gradient found. The current loss is:  0.7538263201713562\n",
      "Warning: nan gradient found. The current loss is:  0.36231690645217896\n",
      "Warning: nan gradient found. The current loss is:  0.5802186131477356\n",
      "Warning: nan gradient found. The current loss is:  0.40979593992233276\n",
      "Warning: nan gradient found. The current loss is:  0.5827711820602417\n",
      "Warning: nan gradient found. The current loss is:  0.5879812240600586\n",
      "Warning: nan gradient found. The current loss is:  1.693122148513794\n",
      "Warning: nan gradient found. The current loss is:  1.0040132999420166\n",
      "Warning: nan gradient found. The current loss is:  0.4606884717941284\n",
      "Warning: nan gradient found. The current loss is:  0.552958071231842\n",
      "Warning: nan gradient found. The current loss is:  0.8763329982757568\n",
      "Warning: nan gradient found. The current loss is:  1.1975996494293213\n",
      "Warning: nan gradient found. The current loss is:  0.35956254601478577\n",
      "Warning: nan gradient found. The current loss is:  0.4965022802352905\n",
      "Warning: nan gradient found. The current loss is:  0.8947482109069824\n",
      "Warning: nan gradient found. The current loss is:  0.3289237320423126\n",
      "Warning: nan gradient found. The current loss is:  0.3461858332157135\n",
      "Warning: nan gradient found. The current loss is:  0.5727218389511108\n",
      "Warning: nan gradient found. The current loss is:  0.6275730729103088\n",
      "Warning: nan gradient found. The current loss is:  0.6198180913925171\n",
      "Warning: nan gradient found. The current loss is:  0.9682012796401978\n",
      "Warning: nan gradient found. The current loss is:  0.2916443943977356\n",
      "Warning: nan gradient found. The current loss is:  0.7254507541656494\n",
      "Warning: nan gradient found. The current loss is:  0.45066648721694946\n",
      "Warning: nan gradient found. The current loss is:  1.0467541217803955\n",
      "Warning: nan gradient found. The current loss is:  0.5649639368057251\n",
      "Warning: nan gradient found. The current loss is:  0.7540401816368103\n",
      "Warning: nan gradient found. The current loss is:  0.9477221369743347\n",
      "Warning: nan gradient found. The current loss is:  0.657239556312561\n",
      "Warning: nan gradient found. The current loss is:  1.426144003868103\n",
      "Warning: nan gradient found. The current loss is:  0.3202090859413147\n",
      "Warning: nan gradient found. The current loss is:  0.9624263644218445\n",
      "Warning: nan gradient found. The current loss is:  0.2822495996952057\n",
      "Warning: nan gradient found. The current loss is:  0.3140752613544464\n",
      "Warning: nan gradient found. The current loss is:  0.6328004598617554\n",
      "Warning: nan gradient found. The current loss is:  0.5950454473495483\n",
      "Warning: nan gradient found. The current loss is:  0.23132213950157166\n",
      "Warning: nan gradient found. The current loss is:  0.6656421422958374\n",
      "Warning: nan gradient found. The current loss is:  0.5873638987541199\n",
      "Warning: nan gradient found. The current loss is:  0.5181451439857483\n",
      "Warning: nan gradient found. The current loss is:  1.020334005355835\n",
      "Warning: nan gradient found. The current loss is:  0.49502062797546387\n",
      "Warning: nan gradient found. The current loss is:  0.7404013276100159\n",
      "Warning: nan gradient found. The current loss is:  0.623039960861206\n",
      "Warning: nan gradient found. The current loss is:  0.6402512192726135\n",
      "Warning: nan gradient found. The current loss is:  0.8785009384155273\n",
      "Warning: nan gradient found. The current loss is:  0.3756074905395508\n",
      "Warning: nan gradient found. The current loss is:  0.4459703266620636\n",
      "Warning: nan gradient found. The current loss is:  1.0666857957839966\n",
      "Warning: nan gradient found. The current loss is:  0.3306935429573059\n",
      "Warning: nan gradient found. The current loss is:  0.4821285009384155\n",
      "Warning: nan gradient found. The current loss is:  0.5215986967086792\n",
      "Warning: nan gradient found. The current loss is:  0.40750187635421753\n",
      "Current batch training loss: 0.407502  [1049600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8795455694198608\n",
      "Warning: nan gradient found. The current loss is:  0.16421259939670563\n",
      "Warning: nan gradient found. The current loss is:  0.6530185341835022\n",
      "Warning: nan gradient found. The current loss is:  0.681667149066925\n",
      "Warning: nan gradient found. The current loss is:  0.6553192138671875\n",
      "Warning: nan gradient found. The current loss is:  0.6377145051956177\n",
      "Warning: nan gradient found. The current loss is:  0.5911549925804138\n",
      "Warning: nan gradient found. The current loss is:  0.7303071022033691\n",
      "Warning: nan gradient found. The current loss is:  0.45431673526763916\n",
      "Warning: nan gradient found. The current loss is:  0.4818550944328308\n",
      "Warning: nan gradient found. The current loss is:  1.0118145942687988\n",
      "Warning: nan gradient found. The current loss is:  0.268196702003479\n",
      "Warning: nan gradient found. The current loss is:  1.20000159740448\n",
      "Warning: nan gradient found. The current loss is:  0.46411073207855225\n",
      "Warning: nan gradient found. The current loss is:  0.2575942873954773\n",
      "Warning: nan gradient found. The current loss is:  0.23642253875732422\n",
      "Warning: nan gradient found. The current loss is:  -0.07331738620996475\n",
      "Warning: nan gradient found. The current loss is:  0.34365543723106384\n",
      "Warning: nan gradient found. The current loss is:  0.6149651408195496\n",
      "Warning: nan gradient found. The current loss is:  0.6623332500457764\n",
      "Warning: nan gradient found. The current loss is:  0.39072877168655396\n",
      "Warning: nan gradient found. The current loss is:  0.8212548494338989\n",
      "Warning: nan gradient found. The current loss is:  0.4800090193748474\n",
      "Warning: nan gradient found. The current loss is:  0.3734462261199951\n",
      "Warning: nan gradient found. The current loss is:  0.6500835418701172\n",
      "Warning: nan gradient found. The current loss is:  0.9147143363952637\n",
      "Warning: nan gradient found. The current loss is:  0.855361819267273\n",
      "Warning: nan gradient found. The current loss is:  0.9031927585601807\n",
      "Warning: nan gradient found. The current loss is:  0.32484596967697144\n",
      "Warning: nan gradient found. The current loss is:  1.4697608947753906\n",
      "Warning: nan gradient found. The current loss is:  0.9621541500091553\n",
      "Warning: nan gradient found. The current loss is:  0.7352115511894226\n",
      "Warning: nan gradient found. The current loss is:  0.5586814284324646\n",
      "Warning: nan gradient found. The current loss is:  0.7729021906852722\n",
      "Warning: nan gradient found. The current loss is:  0.25737088918685913\n",
      "Warning: nan gradient found. The current loss is:  0.06079760566353798\n",
      "Warning: nan gradient found. The current loss is:  0.43069660663604736\n",
      "Warning: nan gradient found. The current loss is:  0.7026467323303223\n",
      "Warning: nan gradient found. The current loss is:  0.5743615627288818\n",
      "Warning: nan gradient found. The current loss is:  0.28219154477119446\n",
      "Warning: nan gradient found. The current loss is:  0.5746351480484009\n",
      "Warning: nan gradient found. The current loss is:  1.1504342555999756\n",
      "Warning: nan gradient found. The current loss is:  0.6238319277763367\n",
      "Warning: nan gradient found. The current loss is:  2.038461685180664\n",
      "Warning: nan gradient found. The current loss is:  0.20750969648361206\n",
      "Warning: nan gradient found. The current loss is:  2.2919528484344482\n",
      "Warning: nan gradient found. The current loss is:  0.3496309816837311\n",
      "Warning: nan gradient found. The current loss is:  0.6473175287246704\n",
      "Warning: nan gradient found. The current loss is:  0.9258371591567993\n",
      "Warning: nan gradient found. The current loss is:  0.08896401524543762\n",
      "Warning: nan gradient found. The current loss is:  0.28600335121154785\n",
      "Warning: nan gradient found. The current loss is:  0.649018406867981\n",
      "Warning: nan gradient found. The current loss is:  0.6265532970428467\n",
      "Warning: nan gradient found. The current loss is:  0.3749167025089264\n",
      "Warning: nan gradient found. The current loss is:  -0.012608058750629425\n",
      "Warning: nan gradient found. The current loss is:  1.4342072010040283\n",
      "Warning: nan gradient found. The current loss is:  0.6892059445381165\n",
      "Warning: nan gradient found. The current loss is:  0.6892604827880859\n",
      "Warning: nan gradient found. The current loss is:  0.29074472188949585\n",
      "Warning: nan gradient found. The current loss is:  0.5320286154747009\n",
      "Warning: nan gradient found. The current loss is:  0.8222008347511292\n",
      "Warning: nan gradient found. The current loss is:  0.43885183334350586\n",
      "Warning: nan gradient found. The current loss is:  0.6988805532455444\n",
      "Warning: nan gradient found. The current loss is:  0.9477516412734985\n",
      "Warning: nan gradient found. The current loss is:  0.9291987419128418\n",
      "Warning: nan gradient found. The current loss is:  0.8403173089027405\n",
      "Warning: nan gradient found. The current loss is:  0.22902977466583252\n",
      "Warning: nan gradient found. The current loss is:  0.5040649175643921\n",
      "Warning: nan gradient found. The current loss is:  0.7369346618652344\n",
      "Warning: nan gradient found. The current loss is:  0.6767090559005737\n",
      "Warning: nan gradient found. The current loss is:  0.46531301736831665\n",
      "Warning: nan gradient found. The current loss is:  0.7566471099853516\n",
      "Warning: nan gradient found. The current loss is:  0.7925314903259277\n",
      "Warning: nan gradient found. The current loss is:  0.5128827095031738\n",
      "Warning: nan gradient found. The current loss is:  0.43073973059654236\n",
      "Warning: nan gradient found. The current loss is:  0.7045245170593262\n",
      "Warning: nan gradient found. The current loss is:  0.6264321804046631\n",
      "Warning: nan gradient found. The current loss is:  0.12804335355758667\n",
      "Warning: nan gradient found. The current loss is:  0.29230114817619324\n",
      "Warning: nan gradient found. The current loss is:  0.6712167263031006\n",
      "Warning: nan gradient found. The current loss is:  0.17619505524635315\n",
      "Warning: nan gradient found. The current loss is:  0.7900236248970032\n",
      "Warning: nan gradient found. The current loss is:  0.4166369140148163\n",
      "Warning: nan gradient found. The current loss is:  0.8433096408843994\n",
      "Warning: nan gradient found. The current loss is:  0.5578408241271973\n",
      "Warning: nan gradient found. The current loss is:  0.21786841750144958\n",
      "Warning: nan gradient found. The current loss is:  0.8780181407928467\n",
      "Warning: nan gradient found. The current loss is:  0.6340833902359009\n",
      "Warning: nan gradient found. The current loss is:  0.7994393706321716\n",
      "Warning: nan gradient found. The current loss is:  0.9518531560897827\n",
      "Warning: nan gradient found. The current loss is:  0.18541765213012695\n",
      "Warning: nan gradient found. The current loss is:  0.3107685446739197\n",
      "Warning: nan gradient found. The current loss is:  0.45414793491363525\n",
      "Warning: nan gradient found. The current loss is:  0.12688060104846954\n",
      "Warning: nan gradient found. The current loss is:  0.9739772081375122\n",
      "Warning: nan gradient found. The current loss is:  0.37083470821380615\n",
      "Warning: nan gradient found. The current loss is:  0.7181863188743591\n",
      "Warning: nan gradient found. The current loss is:  0.6017518043518066\n",
      "Warning: nan gradient found. The current loss is:  0.7812998294830322\n",
      "Warning: nan gradient found. The current loss is:  1.8334585428237915\n",
      "Current batch training loss: 1.833459  [1075200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.26015254855155945\n",
      "Warning: nan gradient found. The current loss is:  0.5884285569190979\n",
      "Warning: nan gradient found. The current loss is:  0.604243278503418\n",
      "Warning: nan gradient found. The current loss is:  0.4210139513015747\n",
      "Warning: nan gradient found. The current loss is:  0.47219139337539673\n",
      "Warning: nan gradient found. The current loss is:  0.7188019752502441\n",
      "Warning: nan gradient found. The current loss is:  0.07089130580425262\n",
      "Warning: nan gradient found. The current loss is:  0.6424925327301025\n",
      "Warning: nan gradient found. The current loss is:  0.5910857915878296\n",
      "Warning: nan gradient found. The current loss is:  0.6637996435165405\n",
      "Warning: nan gradient found. The current loss is:  1.1313048601150513\n",
      "Warning: nan gradient found. The current loss is:  0.4580492377281189\n",
      "Warning: nan gradient found. The current loss is:  0.3535976707935333\n",
      "Warning: nan gradient found. The current loss is:  1.384824514389038\n",
      "Warning: nan gradient found. The current loss is:  0.3809724450111389\n",
      "Warning: nan gradient found. The current loss is:  0.2877720594406128\n",
      "Warning: nan gradient found. The current loss is:  0.6455749273300171\n",
      "Warning: nan gradient found. The current loss is:  0.5803577899932861\n",
      "Warning: nan gradient found. The current loss is:  2.4829037189483643\n",
      "Warning: nan gradient found. The current loss is:  -0.02212434448301792\n",
      "Warning: nan gradient found. The current loss is:  1.6010322570800781\n",
      "Warning: nan gradient found. The current loss is:  0.7143560647964478\n",
      "Warning: nan gradient found. The current loss is:  0.5811291933059692\n",
      "Warning: nan gradient found. The current loss is:  0.2814287543296814\n",
      "Warning: nan gradient found. The current loss is:  0.4665261507034302\n",
      "Warning: nan gradient found. The current loss is:  0.6359249353408813\n",
      "Warning: nan gradient found. The current loss is:  0.5451481342315674\n",
      "Warning: nan gradient found. The current loss is:  0.15108706057071686\n",
      "Warning: nan gradient found. The current loss is:  0.31724584102630615\n",
      "Warning: nan gradient found. The current loss is:  0.4162510335445404\n",
      "Warning: nan gradient found. The current loss is:  0.644048810005188\n",
      "Warning: nan gradient found. The current loss is:  0.2408994883298874\n",
      "Warning: nan gradient found. The current loss is:  0.5264259576797485\n",
      "Warning: nan gradient found. The current loss is:  0.8032358884811401\n",
      "Warning: nan gradient found. The current loss is:  1.4074000120162964\n",
      "Warning: nan gradient found. The current loss is:  1.3684390783309937\n",
      "Warning: nan gradient found. The current loss is:  0.586583137512207\n",
      "Warning: nan gradient found. The current loss is:  -0.005808670073747635\n",
      "Warning: nan gradient found. The current loss is:  -0.019248496741056442\n",
      "Warning: nan gradient found. The current loss is:  0.4716421961784363\n",
      "Warning: nan gradient found. The current loss is:  0.7909703254699707\n",
      "Warning: nan gradient found. The current loss is:  0.6348097324371338\n",
      "Warning: nan gradient found. The current loss is:  0.6289609670639038\n",
      "Warning: nan gradient found. The current loss is:  0.5272868871688843\n",
      "Warning: nan gradient found. The current loss is:  0.9560110569000244\n",
      "Warning: nan gradient found. The current loss is:  0.6249129772186279\n",
      "Warning: nan gradient found. The current loss is:  0.5425817966461182\n",
      "Warning: nan gradient found. The current loss is:  0.4814457297325134\n",
      "Warning: nan gradient found. The current loss is:  0.3698911666870117\n",
      "Warning: nan gradient found. The current loss is:  0.5255424976348877\n",
      "Warning: nan gradient found. The current loss is:  -0.18114948272705078\n",
      "Warning: nan gradient found. The current loss is:  0.10633282363414764\n",
      "Warning: nan gradient found. The current loss is:  0.6884612441062927\n",
      "Warning: nan gradient found. The current loss is:  0.5566093921661377\n",
      "Warning: nan gradient found. The current loss is:  0.4396800696849823\n",
      "Warning: nan gradient found. The current loss is:  0.40457361936569214\n",
      "Warning: nan gradient found. The current loss is:  1.1745030879974365\n",
      "Warning: nan gradient found. The current loss is:  0.7755987048149109\n",
      "Warning: nan gradient found. The current loss is:  0.3241117298603058\n",
      "Warning: nan gradient found. The current loss is:  0.5605906248092651\n",
      "Warning: nan gradient found. The current loss is:  0.10290688276290894\n",
      "Warning: nan gradient found. The current loss is:  0.6962850689888\n",
      "Warning: nan gradient found. The current loss is:  0.5869300961494446\n",
      "Warning: nan gradient found. The current loss is:  1.5638598203659058\n",
      "Warning: nan gradient found. The current loss is:  0.9974875450134277\n",
      "Warning: nan gradient found. The current loss is:  0.6142918467521667\n",
      "Warning: nan gradient found. The current loss is:  0.5549763441085815\n",
      "Warning: nan gradient found. The current loss is:  1.1377449035644531\n",
      "Warning: nan gradient found. The current loss is:  0.026009485125541687\n",
      "Warning: nan gradient found. The current loss is:  0.4440464973449707\n",
      "Warning: nan gradient found. The current loss is:  0.8564814329147339\n",
      "Warning: nan gradient found. The current loss is:  0.46179667115211487\n",
      "Warning: nan gradient found. The current loss is:  0.5800812244415283\n",
      "Warning: nan gradient found. The current loss is:  1.1396628618240356\n",
      "Warning: nan gradient found. The current loss is:  0.37626153230667114\n",
      "Warning: nan gradient found. The current loss is:  0.521340548992157\n",
      "Warning: nan gradient found. The current loss is:  0.29480767250061035\n",
      "Warning: nan gradient found. The current loss is:  0.18327581882476807\n",
      "Warning: nan gradient found. The current loss is:  0.7377864122390747\n",
      "Warning: nan gradient found. The current loss is:  0.41554731130599976\n",
      "Warning: nan gradient found. The current loss is:  0.5471451282501221\n",
      "Warning: nan gradient found. The current loss is:  0.5850642919540405\n",
      "Warning: nan gradient found. The current loss is:  0.11365547776222229\n",
      "Warning: nan gradient found. The current loss is:  0.3976447880268097\n",
      "Warning: nan gradient found. The current loss is:  0.6749070882797241\n",
      "Warning: nan gradient found. The current loss is:  0.8092292547225952\n",
      "Warning: nan gradient found. The current loss is:  0.3894230127334595\n",
      "Warning: nan gradient found. The current loss is:  0.7050161361694336\n",
      "Warning: nan gradient found. The current loss is:  0.45674705505371094\n",
      "Warning: nan gradient found. The current loss is:  0.20340612530708313\n",
      "Warning: nan gradient found. The current loss is:  0.5184692144393921\n",
      "Warning: nan gradient found. The current loss is:  0.6390700340270996\n",
      "Warning: nan gradient found. The current loss is:  0.7316402792930603\n",
      "Warning: nan gradient found. The current loss is:  0.1014699712395668\n",
      "Warning: nan gradient found. The current loss is:  0.8387199640274048\n",
      "Warning: nan gradient found. The current loss is:  0.05238507688045502\n",
      "Warning: nan gradient found. The current loss is:  1.0560821294784546\n",
      "Warning: nan gradient found. The current loss is:  0.9017277359962463\n",
      "Warning: nan gradient found. The current loss is:  0.5231921672821045\n",
      "Warning: nan gradient found. The current loss is:  0.723447322845459\n",
      "Current batch training loss: 0.723447  [1100800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.857275128364563\n",
      "Warning: nan gradient found. The current loss is:  0.427298903465271\n",
      "Warning: nan gradient found. The current loss is:  0.42322537302970886\n",
      "Warning: nan gradient found. The current loss is:  0.6035209894180298\n",
      "Warning: nan gradient found. The current loss is:  0.45747172832489014\n",
      "Warning: nan gradient found. The current loss is:  0.1050640195608139\n",
      "Warning: nan gradient found. The current loss is:  0.3565962016582489\n",
      "Warning: nan gradient found. The current loss is:  0.391012579202652\n",
      "Warning: nan gradient found. The current loss is:  0.355590283870697\n",
      "Warning: nan gradient found. The current loss is:  0.3196548521518707\n",
      "Warning: nan gradient found. The current loss is:  0.30319520831108093\n",
      "Warning: nan gradient found. The current loss is:  0.1914549171924591\n",
      "Warning: nan gradient found. The current loss is:  1.0692857503890991\n",
      "Warning: nan gradient found. The current loss is:  0.2079663872718811\n",
      "Warning: nan gradient found. The current loss is:  1.440384864807129\n",
      "Warning: nan gradient found. The current loss is:  0.5330740213394165\n",
      "Warning: nan gradient found. The current loss is:  0.47990959882736206\n",
      "Warning: nan gradient found. The current loss is:  0.9611002206802368\n",
      "Warning: nan gradient found. The current loss is:  0.3775372803211212\n",
      "Warning: nan gradient found. The current loss is:  0.7877355813980103\n",
      "Warning: nan gradient found. The current loss is:  0.6192618012428284\n",
      "Warning: nan gradient found. The current loss is:  0.6394485831260681\n",
      "Warning: nan gradient found. The current loss is:  0.5998615026473999\n",
      "Warning: nan gradient found. The current loss is:  0.5679314136505127\n",
      "Warning: nan gradient found. The current loss is:  0.697185754776001\n",
      "Warning: nan gradient found. The current loss is:  0.2048872709274292\n",
      "Warning: nan gradient found. The current loss is:  0.7600437998771667\n",
      "Warning: nan gradient found. The current loss is:  1.0319111347198486\n",
      "Warning: nan gradient found. The current loss is:  0.453268438577652\n",
      "Warning: nan gradient found. The current loss is:  0.6694326400756836\n",
      "Warning: nan gradient found. The current loss is:  0.20986872911453247\n",
      "Warning: nan gradient found. The current loss is:  0.6770647764205933\n",
      "Warning: nan gradient found. The current loss is:  0.4003143906593323\n",
      "Warning: nan gradient found. The current loss is:  0.6381844282150269\n",
      "Warning: nan gradient found. The current loss is:  0.6514291763305664\n",
      "Warning: nan gradient found. The current loss is:  0.40507012605667114\n",
      "Warning: nan gradient found. The current loss is:  0.6493677496910095\n",
      "Warning: nan gradient found. The current loss is:  1.1987521648406982\n",
      "Warning: nan gradient found. The current loss is:  0.39440587162971497\n",
      "Warning: nan gradient found. The current loss is:  -0.0007434189319610596\n",
      "Warning: nan gradient found. The current loss is:  0.5297939777374268\n",
      "Warning: nan gradient found. The current loss is:  0.840706467628479\n",
      "Warning: nan gradient found. The current loss is:  0.47957170009613037\n",
      "Warning: nan gradient found. The current loss is:  0.8256840705871582\n",
      "Warning: nan gradient found. The current loss is:  0.34601879119873047\n",
      "Warning: nan gradient found. The current loss is:  0.2201806902885437\n",
      "Warning: nan gradient found. The current loss is:  0.2302699238061905\n",
      "Warning: nan gradient found. The current loss is:  1.3331764936447144\n",
      "Warning: nan gradient found. The current loss is:  0.31242573261260986\n",
      "Warning: nan gradient found. The current loss is:  0.342695027589798\n",
      "Warning: nan gradient found. The current loss is:  0.7842115163803101\n",
      "Warning: nan gradient found. The current loss is:  0.8431220054626465\n",
      "Warning: nan gradient found. The current loss is:  0.7469480037689209\n",
      "Warning: nan gradient found. The current loss is:  0.6051771640777588\n",
      "Warning: nan gradient found. The current loss is:  0.3795586824417114\n",
      "Warning: nan gradient found. The current loss is:  0.2971622049808502\n",
      "Warning: nan gradient found. The current loss is:  0.3346223831176758\n",
      "Warning: nan gradient found. The current loss is:  0.675032913684845\n",
      "Warning: nan gradient found. The current loss is:  0.3463376760482788\n",
      "Warning: nan gradient found. The current loss is:  0.39984387159347534\n",
      "Warning: nan gradient found. The current loss is:  0.10051319748163223\n",
      "Warning: nan gradient found. The current loss is:  0.27177858352661133\n",
      "Warning: nan gradient found. The current loss is:  0.44500401616096497\n",
      "Warning: nan gradient found. The current loss is:  1.0399951934814453\n",
      "Warning: nan gradient found. The current loss is:  0.881564736366272\n",
      "Warning: nan gradient found. The current loss is:  0.22364041209220886\n",
      "Warning: nan gradient found. The current loss is:  0.42998743057250977\n",
      "Warning: nan gradient found. The current loss is:  0.19603906571865082\n",
      "Warning: nan gradient found. The current loss is:  0.33588746190071106\n",
      "Warning: nan gradient found. The current loss is:  0.11752036213874817\n",
      "Warning: nan gradient found. The current loss is:  0.27842068672180176\n",
      "Warning: nan gradient found. The current loss is:  0.49198397994041443\n",
      "Warning: nan gradient found. The current loss is:  0.03543541207909584\n",
      "Warning: nan gradient found. The current loss is:  1.0425513982772827\n",
      "Warning: nan gradient found. The current loss is:  0.5499350428581238\n",
      "Warning: nan gradient found. The current loss is:  0.5004593133926392\n",
      "Warning: nan gradient found. The current loss is:  0.25588181614875793\n",
      "Warning: nan gradient found. The current loss is:  0.8315955996513367\n",
      "Warning: nan gradient found. The current loss is:  0.8101481199264526\n",
      "Warning: nan gradient found. The current loss is:  0.6930400729179382\n",
      "Warning: nan gradient found. The current loss is:  0.7420631647109985\n",
      "Warning: nan gradient found. The current loss is:  0.3051593601703644\n",
      "Warning: nan gradient found. The current loss is:  1.103158712387085\n",
      "Warning: nan gradient found. The current loss is:  0.49728405475616455\n",
      "Warning: nan gradient found. The current loss is:  0.29047340154647827\n",
      "Warning: nan gradient found. The current loss is:  0.6554558873176575\n",
      "Warning: nan gradient found. The current loss is:  1.3318603038787842\n",
      "Warning: nan gradient found. The current loss is:  0.3035220503807068\n",
      "Warning: nan gradient found. The current loss is:  0.6709152460098267\n",
      "Warning: nan gradient found. The current loss is:  0.4314621686935425\n",
      "Warning: nan gradient found. The current loss is:  0.5502578616142273\n",
      "Warning: nan gradient found. The current loss is:  0.663375198841095\n",
      "Warning: nan gradient found. The current loss is:  1.0172873735427856\n",
      "Warning: nan gradient found. The current loss is:  0.43603426218032837\n",
      "Warning: nan gradient found. The current loss is:  0.4190567433834076\n",
      "Warning: nan gradient found. The current loss is:  0.17841437458992004\n",
      "Warning: nan gradient found. The current loss is:  0.4844863712787628\n",
      "Warning: nan gradient found. The current loss is:  0.47162309288978577\n",
      "Warning: nan gradient found. The current loss is:  0.1858464628458023\n",
      "Warning: nan gradient found. The current loss is:  0.7522205114364624\n",
      "Current batch training loss: 0.752221  [1126400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.18488475680351257\n",
      "Warning: nan gradient found. The current loss is:  0.9526554346084595\n",
      "Warning: nan gradient found. The current loss is:  0.8491660356521606\n",
      "Warning: nan gradient found. The current loss is:  1.2204195261001587\n",
      "Warning: nan gradient found. The current loss is:  0.694303572177887\n",
      "Warning: nan gradient found. The current loss is:  -0.017133072018623352\n",
      "Warning: nan gradient found. The current loss is:  0.3829435110092163\n",
      "Warning: nan gradient found. The current loss is:  0.4263877868652344\n",
      "Warning: nan gradient found. The current loss is:  0.17760537564754486\n",
      "Warning: nan gradient found. The current loss is:  0.6167938113212585\n",
      "Warning: nan gradient found. The current loss is:  0.23526792228221893\n",
      "Warning: nan gradient found. The current loss is:  0.5702313780784607\n",
      "Warning: nan gradient found. The current loss is:  0.6078732013702393\n",
      "Warning: nan gradient found. The current loss is:  0.44165754318237305\n",
      "Warning: nan gradient found. The current loss is:  0.5759990215301514\n",
      "Warning: nan gradient found. The current loss is:  0.974478006362915\n",
      "Warning: nan gradient found. The current loss is:  1.0596669912338257\n",
      "Warning: nan gradient found. The current loss is:  0.3141099810600281\n",
      "Warning: nan gradient found. The current loss is:  1.3183304071426392\n",
      "Warning: nan gradient found. The current loss is:  0.9127691984176636\n",
      "Warning: nan gradient found. The current loss is:  0.222109854221344\n",
      "Warning: nan gradient found. The current loss is:  0.6196197867393494\n",
      "Warning: nan gradient found. The current loss is:  0.4770362079143524\n",
      "Warning: nan gradient found. The current loss is:  0.6485375165939331\n",
      "Warning: nan gradient found. The current loss is:  0.6898698806762695\n",
      "Warning: nan gradient found. The current loss is:  0.3100520074367523\n",
      "Warning: nan gradient found. The current loss is:  0.04900950938463211\n",
      "Warning: nan gradient found. The current loss is:  0.2904752790927887\n",
      "Warning: nan gradient found. The current loss is:  0.6458615064620972\n",
      "Warning: nan gradient found. The current loss is:  0.5320127010345459\n",
      "Warning: nan gradient found. The current loss is:  0.14599120616912842\n",
      "Warning: nan gradient found. The current loss is:  0.39665377140045166\n",
      "Warning: nan gradient found. The current loss is:  0.13262516260147095\n",
      "Warning: nan gradient found. The current loss is:  0.5329705476760864\n",
      "Warning: nan gradient found. The current loss is:  0.6807523965835571\n",
      "Warning: nan gradient found. The current loss is:  1.490964651107788\n",
      "Warning: nan gradient found. The current loss is:  1.5435736179351807\n",
      "Warning: nan gradient found. The current loss is:  0.26492840051651\n",
      "Warning: nan gradient found. The current loss is:  1.3327656984329224\n",
      "Warning: nan gradient found. The current loss is:  0.14857786893844604\n",
      "Warning: nan gradient found. The current loss is:  0.16839870810508728\n",
      "Warning: nan gradient found. The current loss is:  0.4731766879558563\n",
      "Warning: nan gradient found. The current loss is:  0.42966610193252563\n",
      "Warning: nan gradient found. The current loss is:  0.4128400981426239\n",
      "Warning: nan gradient found. The current loss is:  1.0054978132247925\n",
      "Warning: nan gradient found. The current loss is:  0.7131651043891907\n",
      "Warning: nan gradient found. The current loss is:  0.31189990043640137\n",
      "Warning: nan gradient found. The current loss is:  0.41274958848953247\n",
      "Warning: nan gradient found. The current loss is:  0.19476401805877686\n",
      "Warning: nan gradient found. The current loss is:  -0.06124669313430786\n",
      "Warning: nan gradient found. The current loss is:  0.8182390332221985\n",
      "Warning: nan gradient found. The current loss is:  0.7536718845367432\n",
      "Warning: nan gradient found. The current loss is:  0.6284122467041016\n",
      "Warning: nan gradient found. The current loss is:  0.30689653754234314\n",
      "Warning: nan gradient found. The current loss is:  0.9492323398590088\n",
      "Warning: nan gradient found. The current loss is:  0.6423464417457581\n",
      "Warning: nan gradient found. The current loss is:  0.5606089234352112\n",
      "Warning: nan gradient found. The current loss is:  0.3494897484779358\n",
      "Warning: nan gradient found. The current loss is:  0.3012445569038391\n",
      "Warning: nan gradient found. The current loss is:  0.3546163737773895\n",
      "Warning: nan gradient found. The current loss is:  0.4947813153266907\n",
      "Warning: nan gradient found. The current loss is:  0.5292416214942932\n",
      "Warning: nan gradient found. The current loss is:  0.5498100519180298\n",
      "Warning: nan gradient found. The current loss is:  0.7661846876144409\n",
      "Warning: nan gradient found. The current loss is:  0.1842063069343567\n",
      "Warning: nan gradient found. The current loss is:  0.6316298246383667\n",
      "Warning: nan gradient found. The current loss is:  0.5561234951019287\n",
      "Warning: nan gradient found. The current loss is:  0.6401509046554565\n",
      "Warning: nan gradient found. The current loss is:  0.9810112714767456\n",
      "Warning: nan gradient found. The current loss is:  0.8600343465805054\n",
      "Warning: nan gradient found. The current loss is:  0.08961686491966248\n",
      "Warning: nan gradient found. The current loss is:  0.5369176268577576\n",
      "Warning: nan gradient found. The current loss is:  0.295860230922699\n",
      "Warning: nan gradient found. The current loss is:  0.5340371131896973\n",
      "Warning: nan gradient found. The current loss is:  0.4922330975532532\n",
      "Warning: nan gradient found. The current loss is:  0.5314316749572754\n",
      "Warning: nan gradient found. The current loss is:  0.7612729072570801\n",
      "Warning: nan gradient found. The current loss is:  0.6620091199874878\n",
      "Warning: nan gradient found. The current loss is:  0.8698291778564453\n",
      "Warning: nan gradient found. The current loss is:  0.39797207713127136\n",
      "Warning: nan gradient found. The current loss is:  0.5248114466667175\n",
      "Warning: nan gradient found. The current loss is:  0.573428213596344\n",
      "Warning: nan gradient found. The current loss is:  2.0738942623138428\n",
      "Warning: nan gradient found. The current loss is:  0.4540112018585205\n",
      "Warning: nan gradient found. The current loss is:  0.2227165549993515\n",
      "Warning: nan gradient found. The current loss is:  0.8814226984977722\n",
      "Warning: nan gradient found. The current loss is:  0.3442132771015167\n",
      "Warning: nan gradient found. The current loss is:  0.22630853950977325\n",
      "Warning: nan gradient found. The current loss is:  0.2841143012046814\n",
      "Warning: nan gradient found. The current loss is:  0.555118203163147\n",
      "Warning: nan gradient found. The current loss is:  0.5564653277397156\n",
      "Warning: nan gradient found. The current loss is:  0.4726489782333374\n",
      "Warning: nan gradient found. The current loss is:  1.4265966415405273\n",
      "Warning: nan gradient found. The current loss is:  0.4828498959541321\n",
      "Warning: nan gradient found. The current loss is:  0.48923784494400024\n",
      "Warning: nan gradient found. The current loss is:  0.31457728147506714\n",
      "Warning: nan gradient found. The current loss is:  0.30915868282318115\n",
      "Warning: nan gradient found. The current loss is:  1.028959035873413\n",
      "Warning: nan gradient found. The current loss is:  0.9140541553497314\n",
      "Warning: nan gradient found. The current loss is:  0.4308990240097046\n",
      "Current batch training loss: 0.430899  [1152000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.403202086687088\n",
      "Warning: nan gradient found. The current loss is:  0.8509106636047363\n",
      "Warning: nan gradient found. The current loss is:  0.5558471083641052\n",
      "Warning: nan gradient found. The current loss is:  1.6730700731277466\n",
      "Warning: nan gradient found. The current loss is:  0.2690425515174866\n",
      "Warning: nan gradient found. The current loss is:  0.5176542401313782\n",
      "Warning: nan gradient found. The current loss is:  0.5456749200820923\n",
      "Warning: nan gradient found. The current loss is:  0.8382922410964966\n",
      "Warning: nan gradient found. The current loss is:  0.8422315120697021\n",
      "Warning: nan gradient found. The current loss is:  0.9415488243103027\n",
      "Warning: nan gradient found. The current loss is:  0.823325514793396\n",
      "Warning: nan gradient found. The current loss is:  1.3570477962493896\n",
      "Warning: nan gradient found. The current loss is:  1.0131850242614746\n",
      "Warning: nan gradient found. The current loss is:  0.25283193588256836\n",
      "Warning: nan gradient found. The current loss is:  0.5213912725448608\n",
      "Warning: nan gradient found. The current loss is:  0.11113946884870529\n",
      "Warning: nan gradient found. The current loss is:  0.641815185546875\n",
      "Warning: nan gradient found. The current loss is:  0.3922685384750366\n",
      "Warning: nan gradient found. The current loss is:  0.24441193044185638\n",
      "Warning: nan gradient found. The current loss is:  0.4134257137775421\n",
      "Warning: nan gradient found. The current loss is:  0.6200478076934814\n",
      "Warning: nan gradient found. The current loss is:  0.9676361083984375\n",
      "Warning: nan gradient found. The current loss is:  1.074942708015442\n",
      "Warning: nan gradient found. The current loss is:  0.30381080508232117\n",
      "Warning: nan gradient found. The current loss is:  0.709428071975708\n",
      "Warning: nan gradient found. The current loss is:  1.0358628034591675\n",
      "Warning: nan gradient found. The current loss is:  1.2277894020080566\n",
      "Warning: nan gradient found. The current loss is:  -0.053481534123420715\n",
      "Warning: nan gradient found. The current loss is:  0.6905040144920349\n",
      "Warning: nan gradient found. The current loss is:  0.3717970550060272\n",
      "Warning: nan gradient found. The current loss is:  0.7875838875770569\n",
      "Warning: nan gradient found. The current loss is:  0.6152952909469604\n",
      "Warning: nan gradient found. The current loss is:  0.5491788387298584\n",
      "Warning: nan gradient found. The current loss is:  0.6179969310760498\n",
      "Warning: nan gradient found. The current loss is:  0.41736701130867004\n",
      "Warning: nan gradient found. The current loss is:  0.593845546245575\n",
      "Warning: nan gradient found. The current loss is:  0.3844268023967743\n",
      "Warning: nan gradient found. The current loss is:  0.28134819865226746\n",
      "Warning: nan gradient found. The current loss is:  0.44521987438201904\n",
      "Warning: nan gradient found. The current loss is:  1.4288647174835205\n",
      "Warning: nan gradient found. The current loss is:  1.3785645961761475\n",
      "Warning: nan gradient found. The current loss is:  0.2880730926990509\n",
      "Warning: nan gradient found. The current loss is:  0.7120347023010254\n",
      "Warning: nan gradient found. The current loss is:  0.5173211097717285\n",
      "Warning: nan gradient found. The current loss is:  0.3369789719581604\n",
      "Warning: nan gradient found. The current loss is:  0.8312846422195435\n",
      "Warning: nan gradient found. The current loss is:  0.4997349977493286\n",
      "Warning: nan gradient found. The current loss is:  1.7284376621246338\n",
      "Warning: nan gradient found. The current loss is:  0.15823855996131897\n",
      "Warning: nan gradient found. The current loss is:  0.2742503881454468\n",
      "Warning: nan gradient found. The current loss is:  0.6994236707687378\n",
      "Warning: nan gradient found. The current loss is:  0.8117800951004028\n",
      "Warning: nan gradient found. The current loss is:  0.6831717491149902\n",
      "Warning: nan gradient found. The current loss is:  0.3879871368408203\n",
      "Warning: nan gradient found. The current loss is:  0.5890920162200928\n",
      "Warning: nan gradient found. The current loss is:  0.8402358293533325\n",
      "Warning: nan gradient found. The current loss is:  0.5560593605041504\n",
      "Warning: nan gradient found. The current loss is:  0.2284703403711319\n",
      "Warning: nan gradient found. The current loss is:  0.660588264465332\n",
      "Warning: nan gradient found. The current loss is:  0.21300148963928223\n",
      "Warning: nan gradient found. The current loss is:  0.15719537436962128\n",
      "Warning: nan gradient found. The current loss is:  0.5987244844436646\n",
      "Warning: nan gradient found. The current loss is:  0.4612511098384857\n",
      "Warning: nan gradient found. The current loss is:  0.5426302552223206\n",
      "Warning: nan gradient found. The current loss is:  0.465185284614563\n",
      "Warning: nan gradient found. The current loss is:  0.484780877828598\n",
      "Warning: nan gradient found. The current loss is:  0.5843684077262878\n",
      "Warning: nan gradient found. The current loss is:  0.39076173305511475\n",
      "Warning: nan gradient found. The current loss is:  1.0801384449005127\n",
      "Warning: nan gradient found. The current loss is:  0.5148059725761414\n",
      "Warning: nan gradient found. The current loss is:  0.6019630432128906\n",
      "Warning: nan gradient found. The current loss is:  0.6227556467056274\n",
      "Warning: nan gradient found. The current loss is:  0.5852607488632202\n",
      "Warning: nan gradient found. The current loss is:  0.08837990462779999\n",
      "Warning: nan gradient found. The current loss is:  0.43976539373397827\n",
      "Warning: nan gradient found. The current loss is:  0.5817786455154419\n",
      "Warning: nan gradient found. The current loss is:  0.5877257585525513\n",
      "Warning: nan gradient found. The current loss is:  0.3345871567726135\n",
      "Warning: nan gradient found. The current loss is:  0.6601923704147339\n",
      "Warning: nan gradient found. The current loss is:  0.7988587617874146\n",
      "Warning: nan gradient found. The current loss is:  0.860192060470581\n",
      "Warning: nan gradient found. The current loss is:  0.4153565764427185\n",
      "Warning: nan gradient found. The current loss is:  0.761696457862854\n",
      "Warning: nan gradient found. The current loss is:  0.28061968088150024\n",
      "Warning: nan gradient found. The current loss is:  0.28207841515541077\n",
      "Warning: nan gradient found. The current loss is:  0.11393286287784576\n",
      "Warning: nan gradient found. The current loss is:  1.086092233657837\n",
      "Warning: nan gradient found. The current loss is:  0.2287115752696991\n",
      "Warning: nan gradient found. The current loss is:  1.2292184829711914\n",
      "Warning: nan gradient found. The current loss is:  0.44637978076934814\n",
      "Warning: nan gradient found. The current loss is:  1.477879285812378\n",
      "Warning: nan gradient found. The current loss is:  0.15714886784553528\n",
      "Warning: nan gradient found. The current loss is:  0.6725704073905945\n",
      "Warning: nan gradient found. The current loss is:  0.24450922012329102\n",
      "Warning: nan gradient found. The current loss is:  0.5349463224411011\n",
      "Warning: nan gradient found. The current loss is:  0.9333866834640503\n",
      "Warning: nan gradient found. The current loss is:  0.0012300051748752594\n",
      "Warning: nan gradient found. The current loss is:  0.22126469016075134\n",
      "Warning: nan gradient found. The current loss is:  1.0507081747055054\n",
      "Warning: nan gradient found. The current loss is:  0.34371417760849\n",
      "Current batch training loss: 0.343714  [1177600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.31699979305267334\n",
      "Warning: nan gradient found. The current loss is:  0.23043477535247803\n",
      "Warning: nan gradient found. The current loss is:  0.8575116395950317\n",
      "Warning: nan gradient found. The current loss is:  -0.010700084269046783\n",
      "Warning: nan gradient found. The current loss is:  0.567385196685791\n",
      "Warning: nan gradient found. The current loss is:  0.43554943799972534\n",
      "Warning: nan gradient found. The current loss is:  1.0372123718261719\n",
      "Warning: nan gradient found. The current loss is:  0.9425119161605835\n",
      "Warning: nan gradient found. The current loss is:  0.27088290452957153\n",
      "Warning: nan gradient found. The current loss is:  0.7024447321891785\n",
      "Warning: nan gradient found. The current loss is:  0.8196773529052734\n",
      "Warning: nan gradient found. The current loss is:  0.49141013622283936\n",
      "Warning: nan gradient found. The current loss is:  0.9481738805770874\n",
      "Warning: nan gradient found. The current loss is:  0.3119303584098816\n",
      "Warning: nan gradient found. The current loss is:  1.3586547374725342\n",
      "Warning: nan gradient found. The current loss is:  0.7629164457321167\n",
      "Warning: nan gradient found. The current loss is:  1.1543781757354736\n",
      "Warning: nan gradient found. The current loss is:  0.4734043776988983\n",
      "Warning: nan gradient found. The current loss is:  0.4605315327644348\n",
      "Warning: nan gradient found. The current loss is:  1.2521997690200806\n",
      "Warning: nan gradient found. The current loss is:  0.5492119789123535\n",
      "Warning: nan gradient found. The current loss is:  0.7865570187568665\n",
      "Warning: nan gradient found. The current loss is:  0.5676273703575134\n",
      "Warning: nan gradient found. The current loss is:  -0.012926027178764343\n",
      "Warning: nan gradient found. The current loss is:  1.064892053604126\n",
      "Warning: nan gradient found. The current loss is:  0.5263606309890747\n",
      "Warning: nan gradient found. The current loss is:  0.3053377568721771\n",
      "Warning: nan gradient found. The current loss is:  0.3697737455368042\n",
      "Warning: nan gradient found. The current loss is:  0.3009818196296692\n",
      "Warning: nan gradient found. The current loss is:  0.3495340943336487\n",
      "Warning: nan gradient found. The current loss is:  0.6754355430603027\n",
      "Warning: nan gradient found. The current loss is:  0.36309826374053955\n",
      "Warning: nan gradient found. The current loss is:  0.2069399058818817\n",
      "Warning: nan gradient found. The current loss is:  0.09537257999181747\n",
      "Warning: nan gradient found. The current loss is:  0.5517059564590454\n",
      "Warning: nan gradient found. The current loss is:  0.6149898171424866\n",
      "Warning: nan gradient found. The current loss is:  0.6681992411613464\n",
      "Warning: nan gradient found. The current loss is:  0.8596847057342529\n",
      "Warning: nan gradient found. The current loss is:  0.34004154801368713\n",
      "Warning: nan gradient found. The current loss is:  0.8127524852752686\n",
      "Warning: nan gradient found. The current loss is:  0.31570354104042053\n",
      "Warning: nan gradient found. The current loss is:  0.9578925967216492\n",
      "Warning: nan gradient found. The current loss is:  0.734525203704834\n",
      "Warning: nan gradient found. The current loss is:  0.8037217855453491\n",
      "Warning: nan gradient found. The current loss is:  0.7836318612098694\n",
      "Warning: nan gradient found. The current loss is:  0.4297890365123749\n",
      "Warning: nan gradient found. The current loss is:  0.7451728582382202\n",
      "Warning: nan gradient found. The current loss is:  0.2606343626976013\n",
      "Warning: nan gradient found. The current loss is:  0.49078017473220825\n",
      "Warning: nan gradient found. The current loss is:  0.173117995262146\n",
      "Warning: nan gradient found. The current loss is:  0.4161115288734436\n",
      "Warning: nan gradient found. The current loss is:  0.5321152806282043\n",
      "Warning: nan gradient found. The current loss is:  0.43436098098754883\n",
      "Warning: nan gradient found. The current loss is:  1.0262891054153442\n",
      "Warning: nan gradient found. The current loss is:  0.43067270517349243\n",
      "Warning: nan gradient found. The current loss is:  0.7505979537963867\n",
      "Warning: nan gradient found. The current loss is:  0.3956671357154846\n",
      "Warning: nan gradient found. The current loss is:  0.6042633652687073\n",
      "Warning: nan gradient found. The current loss is:  0.6646146178245544\n",
      "Warning: nan gradient found. The current loss is:  1.0820330381393433\n",
      "Warning: nan gradient found. The current loss is:  0.1970081925392151\n",
      "Warning: nan gradient found. The current loss is:  0.4589712619781494\n",
      "Warning: nan gradient found. The current loss is:  0.8910289406776428\n",
      "Warning: nan gradient found. The current loss is:  0.46486377716064453\n",
      "Warning: nan gradient found. The current loss is:  0.6209530234336853\n",
      "Warning: nan gradient found. The current loss is:  2.522144079208374\n",
      "Warning: nan gradient found. The current loss is:  0.5608115792274475\n",
      "Warning: nan gradient found. The current loss is:  0.42143362760543823\n",
      "Warning: nan gradient found. The current loss is:  1.111508846282959\n",
      "Warning: nan gradient found. The current loss is:  0.29366061091423035\n",
      "Warning: nan gradient found. The current loss is:  0.45599687099456787\n",
      "Warning: nan gradient found. The current loss is:  0.6610242128372192\n",
      "Warning: nan gradient found. The current loss is:  1.644051194190979\n",
      "Warning: nan gradient found. The current loss is:  1.7944077253341675\n",
      "Warning: nan gradient found. The current loss is:  0.9685345888137817\n",
      "Warning: nan gradient found. The current loss is:  0.7561663389205933\n",
      "Warning: nan gradient found. The current loss is:  0.3099302351474762\n",
      "Warning: nan gradient found. The current loss is:  0.4980311691761017\n",
      "Warning: nan gradient found. The current loss is:  0.25673753023147583\n",
      "Warning: nan gradient found. The current loss is:  1.1886122226715088\n",
      "Warning: nan gradient found. The current loss is:  0.37567874789237976\n",
      "Warning: nan gradient found. The current loss is:  0.46320685744285583\n",
      "Warning: nan gradient found. The current loss is:  -0.025371214374899864\n",
      "Warning: nan gradient found. The current loss is:  0.5646528005599976\n",
      "Warning: nan gradient found. The current loss is:  0.20337247848510742\n",
      "Warning: nan gradient found. The current loss is:  0.8314449191093445\n",
      "Warning: nan gradient found. The current loss is:  0.37752416729927063\n",
      "Warning: nan gradient found. The current loss is:  0.9200345277786255\n",
      "Warning: nan gradient found. The current loss is:  0.24140456318855286\n",
      "Warning: nan gradient found. The current loss is:  1.36128830909729\n",
      "Warning: nan gradient found. The current loss is:  0.4349152147769928\n",
      "Warning: nan gradient found. The current loss is:  0.30233582854270935\n",
      "Warning: nan gradient found. The current loss is:  0.3814881443977356\n",
      "Warning: nan gradient found. The current loss is:  0.6056274771690369\n",
      "Warning: nan gradient found. The current loss is:  0.08826535940170288\n",
      "Warning: nan gradient found. The current loss is:  0.1404411792755127\n",
      "Warning: nan gradient found. The current loss is:  0.10434260964393616\n",
      "Warning: nan gradient found. The current loss is:  0.3467973470687866\n",
      "Warning: nan gradient found. The current loss is:  0.25555798411369324\n",
      "Warning: nan gradient found. The current loss is:  0.31865495443344116\n",
      "Current batch training loss: 0.318655  [1203200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.32159724831581116\n",
      "Warning: nan gradient found. The current loss is:  0.33608266711235046\n",
      "Warning: nan gradient found. The current loss is:  0.449490487575531\n",
      "Warning: nan gradient found. The current loss is:  0.6091006994247437\n",
      "Warning: nan gradient found. The current loss is:  0.9176835417747498\n",
      "Warning: nan gradient found. The current loss is:  2.2556748390197754\n",
      "Warning: nan gradient found. The current loss is:  0.5142823457717896\n",
      "Warning: nan gradient found. The current loss is:  0.89166259765625\n",
      "Warning: nan gradient found. The current loss is:  0.5142405033111572\n",
      "Warning: nan gradient found. The current loss is:  0.40742868185043335\n",
      "Warning: nan gradient found. The current loss is:  0.24755626916885376\n",
      "Warning: nan gradient found. The current loss is:  0.24451673030853271\n",
      "Warning: nan gradient found. The current loss is:  0.7735970616340637\n",
      "Warning: nan gradient found. The current loss is:  0.9182773232460022\n",
      "Warning: nan gradient found. The current loss is:  0.7038265466690063\n",
      "Warning: nan gradient found. The current loss is:  0.27248615026474\n",
      "Warning: nan gradient found. The current loss is:  1.2628005743026733\n",
      "Warning: nan gradient found. The current loss is:  0.8647927045822144\n",
      "Warning: nan gradient found. The current loss is:  0.5829992890357971\n",
      "Warning: nan gradient found. The current loss is:  0.27986854314804077\n",
      "Warning: nan gradient found. The current loss is:  0.6599651575088501\n",
      "Warning: nan gradient found. The current loss is:  0.3868270516395569\n",
      "Warning: nan gradient found. The current loss is:  0.7055646181106567\n",
      "Warning: nan gradient found. The current loss is:  0.8344448208808899\n",
      "Warning: nan gradient found. The current loss is:  0.866153359413147\n",
      "Warning: nan gradient found. The current loss is:  1.1173604726791382\n",
      "Warning: nan gradient found. The current loss is:  1.0793565511703491\n",
      "Warning: nan gradient found. The current loss is:  0.16893276572227478\n",
      "Warning: nan gradient found. The current loss is:  0.5701327323913574\n",
      "Warning: nan gradient found. The current loss is:  0.8533922433853149\n",
      "Warning: nan gradient found. The current loss is:  0.9746580719947815\n",
      "Warning: nan gradient found. The current loss is:  -0.05677250772714615\n",
      "Warning: nan gradient found. The current loss is:  0.5487565994262695\n",
      "Warning: nan gradient found. The current loss is:  -0.05895328149199486\n",
      "Warning: nan gradient found. The current loss is:  0.4633972644805908\n",
      "Warning: nan gradient found. The current loss is:  0.31663915514945984\n",
      "Warning: nan gradient found. The current loss is:  0.4623580574989319\n",
      "Warning: nan gradient found. The current loss is:  0.7086978554725647\n",
      "Warning: nan gradient found. The current loss is:  0.5323359370231628\n",
      "Warning: nan gradient found. The current loss is:  1.338289737701416\n",
      "Warning: nan gradient found. The current loss is:  0.5938596725463867\n",
      "Warning: nan gradient found. The current loss is:  0.6395519971847534\n",
      "Warning: nan gradient found. The current loss is:  0.28949466347694397\n",
      "Warning: nan gradient found. The current loss is:  0.5061272382736206\n",
      "Warning: nan gradient found. The current loss is:  0.5018807649612427\n",
      "Warning: nan gradient found. The current loss is:  0.46920883655548096\n",
      "Warning: nan gradient found. The current loss is:  0.26081061363220215\n",
      "Warning: nan gradient found. The current loss is:  0.5887532234191895\n",
      "Warning: nan gradient found. The current loss is:  0.7141736745834351\n",
      "Warning: nan gradient found. The current loss is:  0.7199767231941223\n",
      "Warning: nan gradient found. The current loss is:  0.5374611020088196\n",
      "Warning: nan gradient found. The current loss is:  0.37307289242744446\n",
      "Warning: nan gradient found. The current loss is:  2.451481819152832\n",
      "Warning: nan gradient found. The current loss is:  0.20343264937400818\n",
      "Warning: nan gradient found. The current loss is:  0.5658888220787048\n",
      "Warning: nan gradient found. The current loss is:  0.3598668575286865\n",
      "Warning: nan gradient found. The current loss is:  0.753252387046814\n",
      "Warning: nan gradient found. The current loss is:  1.061099886894226\n",
      "Warning: nan gradient found. The current loss is:  0.6492998600006104\n",
      "Warning: nan gradient found. The current loss is:  0.6424872875213623\n",
      "Warning: nan gradient found. The current loss is:  1.1936049461364746\n",
      "Warning: nan gradient found. The current loss is:  2.023944854736328\n",
      "Warning: nan gradient found. The current loss is:  0.5777402520179749\n",
      "Warning: nan gradient found. The current loss is:  0.40830087661743164\n",
      "Warning: nan gradient found. The current loss is:  0.8489476442337036\n",
      "Warning: nan gradient found. The current loss is:  0.9352390766143799\n",
      "Warning: nan gradient found. The current loss is:  0.8152580261230469\n",
      "Warning: nan gradient found. The current loss is:  0.010150425136089325\n",
      "Warning: nan gradient found. The current loss is:  0.5562477707862854\n",
      "Warning: nan gradient found. The current loss is:  0.95757657289505\n",
      "Warning: nan gradient found. The current loss is:  0.2849666178226471\n",
      "Warning: nan gradient found. The current loss is:  1.8426356315612793\n",
      "Warning: nan gradient found. The current loss is:  0.6201879978179932\n",
      "Warning: nan gradient found. The current loss is:  0.6513356566429138\n",
      "Warning: nan gradient found. The current loss is:  0.4268209934234619\n",
      "Warning: nan gradient found. The current loss is:  0.76069176197052\n",
      "Warning: nan gradient found. The current loss is:  0.7514615058898926\n",
      "Warning: nan gradient found. The current loss is:  1.156313180923462\n",
      "Warning: nan gradient found. The current loss is:  0.4136037230491638\n",
      "Warning: nan gradient found. The current loss is:  1.6041100025177002\n",
      "Warning: nan gradient found. The current loss is:  0.1265970915555954\n",
      "Warning: nan gradient found. The current loss is:  0.2904815673828125\n",
      "Warning: nan gradient found. The current loss is:  0.5708721876144409\n",
      "Warning: nan gradient found. The current loss is:  1.074934959411621\n",
      "Warning: nan gradient found. The current loss is:  0.6673718690872192\n",
      "Warning: nan gradient found. The current loss is:  0.5383169651031494\n",
      "Warning: nan gradient found. The current loss is:  0.8787791132926941\n",
      "Warning: nan gradient found. The current loss is:  0.6213942170143127\n",
      "Warning: nan gradient found. The current loss is:  1.1079132556915283\n",
      "Warning: nan gradient found. The current loss is:  0.5303781628608704\n",
      "Warning: nan gradient found. The current loss is:  0.4026893675327301\n",
      "Warning: nan gradient found. The current loss is:  0.18760426342487335\n",
      "Warning: nan gradient found. The current loss is:  0.4460151195526123\n",
      "Warning: nan gradient found. The current loss is:  0.3838886618614197\n",
      "Warning: nan gradient found. The current loss is:  1.4455981254577637\n",
      "Warning: nan gradient found. The current loss is:  0.295792818069458\n",
      "Warning: nan gradient found. The current loss is:  0.2057698369026184\n",
      "Warning: nan gradient found. The current loss is:  0.057324327528476715\n",
      "Warning: nan gradient found. The current loss is:  1.0047073364257812\n",
      "Warning: nan gradient found. The current loss is:  0.6420826315879822\n",
      "Current batch training loss: 0.642083  [1228800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.1130825281143188\n",
      "Warning: nan gradient found. The current loss is:  0.35152801871299744\n",
      "Warning: nan gradient found. The current loss is:  0.11364366114139557\n",
      "Warning: nan gradient found. The current loss is:  0.2981964647769928\n",
      "Warning: nan gradient found. The current loss is:  0.43609166145324707\n",
      "Warning: nan gradient found. The current loss is:  0.3638108968734741\n",
      "Warning: nan gradient found. The current loss is:  0.4520999789237976\n",
      "Warning: nan gradient found. The current loss is:  0.8375255465507507\n",
      "Warning: nan gradient found. The current loss is:  0.43883854150772095\n",
      "Warning: nan gradient found. The current loss is:  0.9045321941375732\n",
      "Warning: nan gradient found. The current loss is:  0.5470208525657654\n",
      "Warning: nan gradient found. The current loss is:  0.6041891574859619\n",
      "Warning: nan gradient found. The current loss is:  0.40139445662498474\n",
      "Warning: nan gradient found. The current loss is:  0.421706885099411\n",
      "Warning: nan gradient found. The current loss is:  0.1957864612340927\n",
      "Warning: nan gradient found. The current loss is:  0.512660026550293\n",
      "Warning: nan gradient found. The current loss is:  0.3552698493003845\n",
      "Warning: nan gradient found. The current loss is:  0.4896392822265625\n",
      "Warning: nan gradient found. The current loss is:  0.08889279514551163\n",
      "Warning: nan gradient found. The current loss is:  0.3595622777938843\n",
      "Warning: nan gradient found. The current loss is:  1.000839352607727\n",
      "Warning: nan gradient found. The current loss is:  0.4213109314441681\n",
      "Warning: nan gradient found. The current loss is:  1.3265917301177979\n",
      "Warning: nan gradient found. The current loss is:  1.392212152481079\n",
      "Warning: nan gradient found. The current loss is:  0.8580644726753235\n",
      "Warning: nan gradient found. The current loss is:  0.5313735008239746\n",
      "Warning: nan gradient found. The current loss is:  0.5270469188690186\n",
      "Warning: nan gradient found. The current loss is:  0.5480520129203796\n",
      "Warning: nan gradient found. The current loss is:  1.4450457096099854\n",
      "Warning: nan gradient found. The current loss is:  0.22752204537391663\n",
      "Warning: nan gradient found. The current loss is:  0.5290676951408386\n",
      "Warning: nan gradient found. The current loss is:  0.5834194421768188\n",
      "Warning: nan gradient found. The current loss is:  0.43977582454681396\n",
      "Warning: nan gradient found. The current loss is:  0.2711141109466553\n",
      "Warning: nan gradient found. The current loss is:  0.4018591344356537\n",
      "Warning: nan gradient found. The current loss is:  0.6506015658378601\n",
      "Warning: nan gradient found. The current loss is:  0.6267702579498291\n",
      "Warning: nan gradient found. The current loss is:  0.040660642087459564\n",
      "Warning: nan gradient found. The current loss is:  0.6213288307189941\n",
      "Warning: nan gradient found. The current loss is:  0.2491474449634552\n",
      "Warning: nan gradient found. The current loss is:  0.559563934803009\n",
      "Warning: nan gradient found. The current loss is:  0.31891587376594543\n",
      "Warning: nan gradient found. The current loss is:  0.027972832322120667\n",
      "Warning: nan gradient found. The current loss is:  0.7069546580314636\n",
      "Warning: nan gradient found. The current loss is:  0.5034466981887817\n",
      "Warning: nan gradient found. The current loss is:  0.9824212789535522\n",
      "Warning: nan gradient found. The current loss is:  0.7999304533004761\n",
      "Warning: nan gradient found. The current loss is:  0.36463916301727295\n",
      "Warning: nan gradient found. The current loss is:  0.740755558013916\n",
      "Warning: nan gradient found. The current loss is:  0.4182590842247009\n",
      "Warning: nan gradient found. The current loss is:  0.5809451341629028\n",
      "Warning: nan gradient found. The current loss is:  0.5752590298652649\n",
      "Warning: nan gradient found. The current loss is:  0.8418413400650024\n",
      "Warning: nan gradient found. The current loss is:  0.7160207033157349\n",
      "Warning: nan gradient found. The current loss is:  0.7639573812484741\n",
      "Warning: nan gradient found. The current loss is:  0.5282500982284546\n",
      "Warning: nan gradient found. The current loss is:  0.4175608158111572\n",
      "Warning: nan gradient found. The current loss is:  0.6219496130943298\n",
      "Warning: nan gradient found. The current loss is:  0.5494179725646973\n",
      "Warning: nan gradient found. The current loss is:  0.10836083441972733\n",
      "Warning: nan gradient found. The current loss is:  0.6909647583961487\n",
      "Warning: nan gradient found. The current loss is:  0.4181634485721588\n",
      "Warning: nan gradient found. The current loss is:  0.3335840106010437\n",
      "Warning: nan gradient found. The current loss is:  0.7912467122077942\n",
      "Warning: nan gradient found. The current loss is:  0.8431813716888428\n",
      "Warning: nan gradient found. The current loss is:  0.32438361644744873\n",
      "Warning: nan gradient found. The current loss is:  0.6384699940681458\n",
      "Warning: nan gradient found. The current loss is:  0.5334839224815369\n",
      "Warning: nan gradient found. The current loss is:  2.019359827041626\n",
      "Warning: nan gradient found. The current loss is:  0.29792320728302\n",
      "Warning: nan gradient found. The current loss is:  0.6553477048873901\n",
      "Warning: nan gradient found. The current loss is:  0.1688668131828308\n",
      "Warning: nan gradient found. The current loss is:  0.3982183337211609\n",
      "Warning: nan gradient found. The current loss is:  0.6996442079544067\n",
      "Warning: nan gradient found. The current loss is:  0.5788946151733398\n",
      "Warning: nan gradient found. The current loss is:  0.6875934600830078\n",
      "Warning: nan gradient found. The current loss is:  -0.0447831004858017\n",
      "Warning: nan gradient found. The current loss is:  1.3278769254684448\n",
      "Warning: nan gradient found. The current loss is:  1.1819074153900146\n",
      "Warning: nan gradient found. The current loss is:  0.5033174753189087\n",
      "Warning: nan gradient found. The current loss is:  0.7160980105400085\n",
      "Warning: nan gradient found. The current loss is:  0.9598226547241211\n",
      "Warning: nan gradient found. The current loss is:  1.6392545700073242\n",
      "Warning: nan gradient found. The current loss is:  0.22964781522750854\n",
      "Warning: nan gradient found. The current loss is:  0.045160453766584396\n",
      "Warning: nan gradient found. The current loss is:  0.5788158178329468\n",
      "Warning: nan gradient found. The current loss is:  0.7317560911178589\n",
      "Warning: nan gradient found. The current loss is:  0.2785959541797638\n",
      "Warning: nan gradient found. The current loss is:  0.9747724533081055\n",
      "Warning: nan gradient found. The current loss is:  0.43738746643066406\n",
      "Warning: nan gradient found. The current loss is:  0.6466686725616455\n",
      "Warning: nan gradient found. The current loss is:  0.9033949971199036\n",
      "Warning: nan gradient found. The current loss is:  0.9427712559700012\n",
      "Warning: nan gradient found. The current loss is:  0.6916182637214661\n",
      "Warning: nan gradient found. The current loss is:  0.6628997921943665\n",
      "Warning: nan gradient found. The current loss is:  0.31196334958076477\n",
      "Warning: nan gradient found. The current loss is:  0.3675684332847595\n",
      "Warning: nan gradient found. The current loss is:  0.1883874237537384\n",
      "Warning: nan gradient found. The current loss is:  0.2995690107345581\n",
      "Warning: nan gradient found. The current loss is:  0.0906890332698822\n",
      "Current batch training loss: 0.090689  [1254400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.3762452602386475\n",
      "Warning: nan gradient found. The current loss is:  0.8987731337547302\n",
      "Warning: nan gradient found. The current loss is:  0.4021065831184387\n",
      "Warning: nan gradient found. The current loss is:  0.06301987916231155\n",
      "Warning: nan gradient found. The current loss is:  1.0119034051895142\n",
      "Warning: nan gradient found. The current loss is:  0.3125308156013489\n",
      "Warning: nan gradient found. The current loss is:  0.6476110219955444\n",
      "Warning: nan gradient found. The current loss is:  0.888009250164032\n",
      "Warning: nan gradient found. The current loss is:  0.3709811568260193\n",
      "Warning: nan gradient found. The current loss is:  0.5561172962188721\n",
      "Warning: nan gradient found. The current loss is:  0.5729516744613647\n",
      "Warning: nan gradient found. The current loss is:  0.5699758529663086\n",
      "Warning: nan gradient found. The current loss is:  0.8725293874740601\n",
      "Warning: nan gradient found. The current loss is:  0.4137601852416992\n",
      "Warning: nan gradient found. The current loss is:  0.26226747035980225\n",
      "Warning: nan gradient found. The current loss is:  0.25446653366088867\n",
      "Warning: nan gradient found. The current loss is:  0.8805071115493774\n",
      "Warning: nan gradient found. The current loss is:  0.8792887330055237\n",
      "Warning: nan gradient found. The current loss is:  2.357818841934204\n",
      "Warning: nan gradient found. The current loss is:  0.7123994827270508\n",
      "Warning: nan gradient found. The current loss is:  0.5684070587158203\n",
      "Warning: nan gradient found. The current loss is:  0.5376213788986206\n",
      "Warning: nan gradient found. The current loss is:  0.4404427707195282\n",
      "Warning: nan gradient found. The current loss is:  0.32951804995536804\n",
      "Warning: nan gradient found. The current loss is:  0.8258616924285889\n",
      "Warning: nan gradient found. The current loss is:  1.1727756261825562\n",
      "Warning: nan gradient found. The current loss is:  0.8996464610099792\n",
      "Warning: nan gradient found. The current loss is:  0.39000964164733887\n",
      "Warning: nan gradient found. The current loss is:  0.3156352937221527\n",
      "Warning: nan gradient found. The current loss is:  1.0959810018539429\n",
      "Warning: nan gradient found. The current loss is:  1.1009303331375122\n",
      "Warning: nan gradient found. The current loss is:  0.2696436643600464\n",
      "Warning: nan gradient found. The current loss is:  0.5740876197814941\n",
      "Warning: nan gradient found. The current loss is:  0.7999884486198425\n",
      "Warning: nan gradient found. The current loss is:  0.30684348940849304\n",
      "Warning: nan gradient found. The current loss is:  0.1722172498703003\n",
      "Warning: nan gradient found. The current loss is:  0.035171329975128174\n",
      "Warning: nan gradient found. The current loss is:  0.2774677574634552\n",
      "Warning: nan gradient found. The current loss is:  0.3741428852081299\n",
      "Warning: nan gradient found. The current loss is:  0.2816642224788666\n",
      "Warning: nan gradient found. The current loss is:  0.22513139247894287\n",
      "Warning: nan gradient found. The current loss is:  0.2137766182422638\n",
      "Warning: nan gradient found. The current loss is:  0.867363452911377\n",
      "Warning: nan gradient found. The current loss is:  0.19147144258022308\n",
      "Warning: nan gradient found. The current loss is:  0.869047224521637\n",
      "Warning: nan gradient found. The current loss is:  0.6713484525680542\n",
      "Warning: nan gradient found. The current loss is:  0.33810049295425415\n",
      "Warning: nan gradient found. The current loss is:  1.8259117603302002\n",
      "Warning: nan gradient found. The current loss is:  0.6513411998748779\n",
      "Warning: nan gradient found. The current loss is:  0.09561555087566376\n",
      "Warning: nan gradient found. The current loss is:  0.5279433131217957\n",
      "Warning: nan gradient found. The current loss is:  0.23598265647888184\n",
      "Warning: nan gradient found. The current loss is:  0.8922597169876099\n",
      "Warning: nan gradient found. The current loss is:  1.2002140283584595\n",
      "Warning: nan gradient found. The current loss is:  0.5750446915626526\n",
      "Warning: nan gradient found. The current loss is:  0.7785045504570007\n",
      "Warning: nan gradient found. The current loss is:  0.32450902462005615\n",
      "Warning: nan gradient found. The current loss is:  0.35711199045181274\n",
      "Warning: nan gradient found. The current loss is:  0.7557134628295898\n",
      "Warning: nan gradient found. The current loss is:  0.8778302669525146\n",
      "Warning: nan gradient found. The current loss is:  0.5191731452941895\n",
      "Warning: nan gradient found. The current loss is:  0.6415963172912598\n",
      "Warning: nan gradient found. The current loss is:  0.9317883253097534\n",
      "Warning: nan gradient found. The current loss is:  0.7043153047561646\n",
      "Warning: nan gradient found. The current loss is:  0.5179569721221924\n",
      "Warning: nan gradient found. The current loss is:  0.8125126957893372\n",
      "Warning: nan gradient found. The current loss is:  0.3532688021659851\n",
      "Warning: nan gradient found. The current loss is:  1.001460313796997\n",
      "Warning: nan gradient found. The current loss is:  0.5288278460502625\n",
      "Warning: nan gradient found. The current loss is:  0.2587670087814331\n",
      "Warning: nan gradient found. The current loss is:  0.6713894605636597\n",
      "Warning: nan gradient found. The current loss is:  0.3908647298812866\n",
      "Warning: nan gradient found. The current loss is:  0.9185836911201477\n",
      "Warning: nan gradient found. The current loss is:  0.857991635799408\n",
      "Warning: nan gradient found. The current loss is:  0.6874156594276428\n",
      "Warning: nan gradient found. The current loss is:  0.31803444027900696\n",
      "Warning: nan gradient found. The current loss is:  0.0791388750076294\n",
      "Warning: nan gradient found. The current loss is:  0.3260001242160797\n",
      "Warning: nan gradient found. The current loss is:  0.9709106683731079\n",
      "Warning: nan gradient found. The current loss is:  0.6693141460418701\n",
      "Warning: nan gradient found. The current loss is:  0.6788926124572754\n",
      "Warning: nan gradient found. The current loss is:  0.9872397184371948\n",
      "Warning: nan gradient found. The current loss is:  0.24938391149044037\n",
      "Warning: nan gradient found. The current loss is:  0.6307483911514282\n",
      "Warning: nan gradient found. The current loss is:  0.733039915561676\n",
      "Warning: nan gradient found. The current loss is:  0.10710635781288147\n",
      "Warning: nan gradient found. The current loss is:  0.23989437520503998\n",
      "Warning: nan gradient found. The current loss is:  0.05733151733875275\n",
      "Warning: nan gradient found. The current loss is:  0.5817840695381165\n",
      "Warning: nan gradient found. The current loss is:  0.9625547528266907\n",
      "Warning: nan gradient found. The current loss is:  0.3304106593132019\n",
      "Warning: nan gradient found. The current loss is:  0.9117578864097595\n",
      "Warning: nan gradient found. The current loss is:  0.6918624043464661\n",
      "Warning: nan gradient found. The current loss is:  0.8145700693130493\n",
      "Warning: nan gradient found. The current loss is:  0.7028435468673706\n",
      "Warning: nan gradient found. The current loss is:  1.1323195695877075\n",
      "Warning: nan gradient found. The current loss is:  0.39997240900993347\n",
      "Warning: nan gradient found. The current loss is:  0.2317623347043991\n",
      "Warning: nan gradient found. The current loss is:  0.7383955121040344\n",
      "Warning: nan gradient found. The current loss is:  0.4880094528198242\n",
      "Current batch training loss: 0.488009  [1280000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4605688452720642\n",
      "Warning: nan gradient found. The current loss is:  1.59861159324646\n",
      "Warning: nan gradient found. The current loss is:  0.5419144034385681\n",
      "Warning: nan gradient found. The current loss is:  0.5430787801742554\n",
      "Warning: nan gradient found. The current loss is:  0.5975337624549866\n",
      "Warning: nan gradient found. The current loss is:  0.41135141253471375\n",
      "Warning: nan gradient found. The current loss is:  0.7336450815200806\n",
      "Warning: nan gradient found. The current loss is:  0.5757165551185608\n",
      "Warning: nan gradient found. The current loss is:  4.421424388885498\n",
      "Warning: nan gradient found. The current loss is:  0.3874242901802063\n",
      "Warning: nan gradient found. The current loss is:  0.7988101243972778\n",
      "Warning: nan gradient found. The current loss is:  0.4748108983039856\n",
      "Warning: nan gradient found. The current loss is:  0.6218438148498535\n",
      "Warning: nan gradient found. The current loss is:  0.5162838101387024\n",
      "Warning: nan gradient found. The current loss is:  0.39358094334602356\n",
      "Warning: nan gradient found. The current loss is:  0.6733303070068359\n",
      "Warning: nan gradient found. The current loss is:  0.528972327709198\n",
      "Warning: nan gradient found. The current loss is:  0.5123695731163025\n",
      "Warning: nan gradient found. The current loss is:  0.3445066809654236\n",
      "Warning: nan gradient found. The current loss is:  0.4252969026565552\n",
      "Warning: nan gradient found. The current loss is:  0.8045289516448975\n",
      "Warning: nan gradient found. The current loss is:  1.6426259279251099\n",
      "Warning: nan gradient found. The current loss is:  0.3380495607852936\n",
      "Warning: nan gradient found. The current loss is:  0.3059684634208679\n",
      "Warning: nan gradient found. The current loss is:  0.6787875890731812\n",
      "Warning: nan gradient found. The current loss is:  -0.0097469761967659\n",
      "Warning: nan gradient found. The current loss is:  0.1877877414226532\n",
      "Warning: nan gradient found. The current loss is:  0.593055009841919\n",
      "Warning: nan gradient found. The current loss is:  0.2234257012605667\n",
      "Warning: nan gradient found. The current loss is:  0.19104863703250885\n",
      "Warning: nan gradient found. The current loss is:  0.7658325433731079\n",
      "Warning: nan gradient found. The current loss is:  0.13424134254455566\n",
      "Warning: nan gradient found. The current loss is:  1.664581298828125\n",
      "Warning: nan gradient found. The current loss is:  0.17370101809501648\n",
      "Warning: nan gradient found. The current loss is:  0.32329443097114563\n",
      "Warning: nan gradient found. The current loss is:  0.714409351348877\n",
      "Warning: nan gradient found. The current loss is:  0.34830689430236816\n",
      "Warning: nan gradient found. The current loss is:  0.07425100356340408\n",
      "Warning: nan gradient found. The current loss is:  0.30368292331695557\n",
      "Warning: nan gradient found. The current loss is:  0.5318745970726013\n",
      "Warning: nan gradient found. The current loss is:  1.0866467952728271\n",
      "Warning: nan gradient found. The current loss is:  0.7366206645965576\n",
      "Warning: nan gradient found. The current loss is:  0.15969699621200562\n",
      "Warning: nan gradient found. The current loss is:  1.454555630683899\n",
      "Warning: nan gradient found. The current loss is:  0.31141987442970276\n",
      "Warning: nan gradient found. The current loss is:  0.11207543313503265\n",
      "Warning: nan gradient found. The current loss is:  0.6484078168869019\n",
      "Warning: nan gradient found. The current loss is:  0.5740790963172913\n",
      "Warning: nan gradient found. The current loss is:  0.297155499458313\n",
      "Warning: nan gradient found. The current loss is:  0.3945833146572113\n",
      "Warning: nan gradient found. The current loss is:  0.7099296450614929\n",
      "Warning: nan gradient found. The current loss is:  0.5094624161720276\n",
      "Warning: nan gradient found. The current loss is:  0.16537895798683167\n",
      "Warning: nan gradient found. The current loss is:  0.7096638083457947\n",
      "Warning: nan gradient found. The current loss is:  0.2098873257637024\n",
      "Warning: nan gradient found. The current loss is:  0.48426592350006104\n",
      "Warning: nan gradient found. The current loss is:  0.49267029762268066\n",
      "Warning: nan gradient found. The current loss is:  0.6158840656280518\n",
      "Warning: nan gradient found. The current loss is:  0.6993923187255859\n",
      "Warning: nan gradient found. The current loss is:  0.291908323764801\n",
      "Warning: nan gradient found. The current loss is:  0.28994473814964294\n",
      "Warning: nan gradient found. The current loss is:  0.09121137857437134\n",
      "Warning: nan gradient found. The current loss is:  0.9648032784461975\n",
      "Warning: nan gradient found. The current loss is:  0.4093814194202423\n",
      "Warning: nan gradient found. The current loss is:  0.06116390973329544\n",
      "Warning: nan gradient found. The current loss is:  0.5112918615341187\n",
      "Warning: nan gradient found. The current loss is:  0.6639015674591064\n",
      "Warning: nan gradient found. The current loss is:  0.345152884721756\n",
      "Warning: nan gradient found. The current loss is:  0.8349282741546631\n",
      "Warning: nan gradient found. The current loss is:  0.8888934850692749\n",
      "Warning: nan gradient found. The current loss is:  0.7163656949996948\n",
      "Warning: nan gradient found. The current loss is:  1.2596954107284546\n",
      "Warning: nan gradient found. The current loss is:  0.31162360310554504\n",
      "Warning: nan gradient found. The current loss is:  0.8859195709228516\n",
      "Warning: nan gradient found. The current loss is:  0.6792283058166504\n",
      "Warning: nan gradient found. The current loss is:  0.8062452077865601\n",
      "Warning: nan gradient found. The current loss is:  0.45916110277175903\n",
      "Warning: nan gradient found. The current loss is:  0.7450363636016846\n",
      "Warning: nan gradient found. The current loss is:  0.8909403085708618\n",
      "Warning: nan gradient found. The current loss is:  0.19548299908638\n",
      "Warning: nan gradient found. The current loss is:  0.7927435636520386\n",
      "Warning: nan gradient found. The current loss is:  0.29551205039024353\n",
      "Warning: nan gradient found. The current loss is:  0.8562575578689575\n",
      "Warning: nan gradient found. The current loss is:  0.7745651602745056\n",
      "Warning: nan gradient found. The current loss is:  0.6374109387397766\n",
      "Warning: nan gradient found. The current loss is:  0.6067976951599121\n",
      "Warning: nan gradient found. The current loss is:  0.11738044023513794\n",
      "Warning: nan gradient found. The current loss is:  0.5186282396316528\n",
      "Warning: nan gradient found. The current loss is:  0.6268880367279053\n",
      "Warning: nan gradient found. The current loss is:  1.7617460489273071\n",
      "Warning: nan gradient found. The current loss is:  0.3763780891895294\n",
      "Warning: nan gradient found. The current loss is:  1.0154980421066284\n",
      "Warning: nan gradient found. The current loss is:  0.35795921087265015\n",
      "Warning: nan gradient found. The current loss is:  0.7145382761955261\n",
      "Warning: nan gradient found. The current loss is:  1.9631654024124146\n",
      "Warning: nan gradient found. The current loss is:  0.22598129510879517\n",
      "Warning: nan gradient found. The current loss is:  0.08451498299837112\n",
      "Warning: nan gradient found. The current loss is:  0.031304195523262024\n",
      "Warning: nan gradient found. The current loss is:  0.11711451411247253\n",
      "Warning: nan gradient found. The current loss is:  0.829849123954773\n",
      "Current batch training loss: 0.829849  [1305600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8027667999267578\n",
      "Warning: nan gradient found. The current loss is:  0.5383486151695251\n",
      "Warning: nan gradient found. The current loss is:  0.24587087333202362\n",
      "Warning: nan gradient found. The current loss is:  0.5663259029388428\n",
      "Warning: nan gradient found. The current loss is:  0.7399406433105469\n",
      "Warning: nan gradient found. The current loss is:  0.3293672204017639\n",
      "Warning: nan gradient found. The current loss is:  0.3704083263874054\n",
      "Warning: nan gradient found. The current loss is:  0.6646581292152405\n",
      "Warning: nan gradient found. The current loss is:  0.3021101653575897\n",
      "Warning: nan gradient found. The current loss is:  1.095030665397644\n",
      "Warning: nan gradient found. The current loss is:  0.276198148727417\n",
      "Warning: nan gradient found. The current loss is:  2.6097517013549805\n",
      "Warning: nan gradient found. The current loss is:  0.14850692451000214\n",
      "Warning: nan gradient found. The current loss is:  0.6228536367416382\n",
      "Warning: nan gradient found. The current loss is:  0.32417258620262146\n",
      "Warning: nan gradient found. The current loss is:  1.200892448425293\n",
      "Warning: nan gradient found. The current loss is:  0.6696645021438599\n",
      "Warning: nan gradient found. The current loss is:  1.3856550455093384\n",
      "Warning: nan gradient found. The current loss is:  0.27681440114974976\n",
      "Warning: nan gradient found. The current loss is:  0.6372392177581787\n",
      "Warning: nan gradient found. The current loss is:  0.37867528200149536\n",
      "Warning: nan gradient found. The current loss is:  1.0973478555679321\n",
      "Warning: nan gradient found. The current loss is:  0.6367549300193787\n",
      "Warning: nan gradient found. The current loss is:  0.12485489249229431\n",
      "Warning: nan gradient found. The current loss is:  1.0880998373031616\n",
      "Warning: nan gradient found. The current loss is:  0.4239567518234253\n",
      "Warning: nan gradient found. The current loss is:  0.45998871326446533\n",
      "Warning: nan gradient found. The current loss is:  0.60599684715271\n",
      "Warning: nan gradient found. The current loss is:  0.32129836082458496\n",
      "Warning: nan gradient found. The current loss is:  0.7366948127746582\n",
      "Warning: nan gradient found. The current loss is:  0.8611355423927307\n",
      "Warning: nan gradient found. The current loss is:  0.44984132051467896\n",
      "Warning: nan gradient found. The current loss is:  0.28245794773101807\n",
      "Warning: nan gradient found. The current loss is:  0.37472227215766907\n",
      "Warning: nan gradient found. The current loss is:  0.6272485852241516\n",
      "Warning: nan gradient found. The current loss is:  0.544407308101654\n",
      "Warning: nan gradient found. The current loss is:  0.6798259615898132\n",
      "Warning: nan gradient found. The current loss is:  0.1462605744600296\n",
      "Warning: nan gradient found. The current loss is:  0.38783496618270874\n",
      "Warning: nan gradient found. The current loss is:  0.6954451203346252\n",
      "Warning: nan gradient found. The current loss is:  0.5155007243156433\n",
      "Warning: nan gradient found. The current loss is:  0.2027486264705658\n",
      "Warning: nan gradient found. The current loss is:  0.4039934575557709\n",
      "Warning: nan gradient found. The current loss is:  0.30694109201431274\n",
      "Warning: nan gradient found. The current loss is:  2.0393199920654297\n",
      "Warning: nan gradient found. The current loss is:  -0.05100618302822113\n",
      "Warning: nan gradient found. The current loss is:  0.21143066883087158\n",
      "Warning: nan gradient found. The current loss is:  0.8004032969474792\n",
      "Warning: nan gradient found. The current loss is:  0.5831444263458252\n",
      "Warning: nan gradient found. The current loss is:  0.6008100509643555\n",
      "Warning: nan gradient found. The current loss is:  0.5027064085006714\n",
      "Warning: nan gradient found. The current loss is:  0.08903887122869492\n",
      "Warning: nan gradient found. The current loss is:  1.5069044828414917\n",
      "Warning: nan gradient found. The current loss is:  0.14835339784622192\n",
      "Warning: nan gradient found. The current loss is:  0.43067467212677\n",
      "Warning: nan gradient found. The current loss is:  0.5283127427101135\n",
      "Warning: nan gradient found. The current loss is:  0.4260273277759552\n",
      "Warning: nan gradient found. The current loss is:  0.5374829769134521\n",
      "Warning: nan gradient found. The current loss is:  0.5514476895332336\n",
      "Warning: nan gradient found. The current loss is:  0.8979463577270508\n",
      "Warning: nan gradient found. The current loss is:  0.20097056031227112\n",
      "Warning: nan gradient found. The current loss is:  0.5284664630889893\n",
      "Warning: nan gradient found. The current loss is:  1.0569124221801758\n",
      "Warning: nan gradient found. The current loss is:  0.362725168466568\n",
      "Warning: nan gradient found. The current loss is:  1.0562530755996704\n",
      "Warning: nan gradient found. The current loss is:  0.8240899443626404\n",
      "Warning: nan gradient found. The current loss is:  0.3272915482521057\n",
      "Warning: nan gradient found. The current loss is:  0.7886262536048889\n",
      "Warning: nan gradient found. The current loss is:  0.45510557293891907\n",
      "Warning: nan gradient found. The current loss is:  0.985942006111145\n",
      "Warning: nan gradient found. The current loss is:  0.46851682662963867\n",
      "Warning: nan gradient found. The current loss is:  0.6866381168365479\n",
      "Warning: nan gradient found. The current loss is:  1.1234153509140015\n",
      "Warning: nan gradient found. The current loss is:  0.799584150314331\n",
      "Warning: nan gradient found. The current loss is:  0.7395977973937988\n",
      "Warning: nan gradient found. The current loss is:  0.4729093611240387\n",
      "Warning: nan gradient found. The current loss is:  0.11159606277942657\n",
      "Warning: nan gradient found. The current loss is:  0.514414370059967\n",
      "Warning: nan gradient found. The current loss is:  0.4532490670681\n",
      "Warning: nan gradient found. The current loss is:  0.9500076770782471\n",
      "Warning: nan gradient found. The current loss is:  0.3303837776184082\n",
      "Warning: nan gradient found. The current loss is:  0.3221677839756012\n",
      "Warning: nan gradient found. The current loss is:  0.664793848991394\n",
      "Warning: nan gradient found. The current loss is:  0.4907757639884949\n",
      "Warning: nan gradient found. The current loss is:  0.16288937628269196\n",
      "Warning: nan gradient found. The current loss is:  0.34579452872276306\n",
      "Warning: nan gradient found. The current loss is:  0.4893159568309784\n",
      "Warning: nan gradient found. The current loss is:  0.25208163261413574\n",
      "Warning: nan gradient found. The current loss is:  0.432608425617218\n",
      "Warning: nan gradient found. The current loss is:  0.7521393299102783\n",
      "Warning: nan gradient found. The current loss is:  0.05178169906139374\n",
      "Warning: nan gradient found. The current loss is:  1.5474998950958252\n",
      "Warning: nan gradient found. The current loss is:  0.8029003143310547\n",
      "Warning: nan gradient found. The current loss is:  0.864017128944397\n",
      "Warning: nan gradient found. The current loss is:  1.201404094696045\n",
      "Warning: nan gradient found. The current loss is:  0.8697160482406616\n",
      "Warning: nan gradient found. The current loss is:  0.13065195083618164\n",
      "Warning: nan gradient found. The current loss is:  0.6253949403762817\n",
      "Warning: nan gradient found. The current loss is:  0.5870469212532043\n",
      "Warning: nan gradient found. The current loss is:  0.6052215099334717\n",
      "Current batch training loss: 0.605222  [1331200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6047061681747437\n",
      "Warning: nan gradient found. The current loss is:  0.4569346308708191\n",
      "Warning: nan gradient found. The current loss is:  0.5594963431358337\n",
      "Warning: nan gradient found. The current loss is:  0.586678683757782\n",
      "Warning: nan gradient found. The current loss is:  0.4765416383743286\n",
      "Warning: nan gradient found. The current loss is:  0.4749506115913391\n",
      "Warning: nan gradient found. The current loss is:  1.382570505142212\n",
      "Warning: nan gradient found. The current loss is:  0.9478460550308228\n",
      "Warning: nan gradient found. The current loss is:  0.008178796619176865\n",
      "Warning: nan gradient found. The current loss is:  1.1496226787567139\n",
      "Warning: nan gradient found. The current loss is:  0.493677020072937\n",
      "Warning: nan gradient found. The current loss is:  1.5242259502410889\n",
      "Warning: nan gradient found. The current loss is:  0.26445886492729187\n",
      "Warning: nan gradient found. The current loss is:  0.7068521976470947\n",
      "Warning: nan gradient found. The current loss is:  0.6212499141693115\n",
      "Warning: nan gradient found. The current loss is:  0.2159741073846817\n",
      "Warning: nan gradient found. The current loss is:  0.42218801379203796\n",
      "Warning: nan gradient found. The current loss is:  0.5755554437637329\n",
      "Warning: nan gradient found. The current loss is:  0.784945011138916\n",
      "Warning: nan gradient found. The current loss is:  0.9481505751609802\n",
      "Warning: nan gradient found. The current loss is:  0.6486025452613831\n",
      "Warning: nan gradient found. The current loss is:  1.0961768627166748\n",
      "Warning: nan gradient found. The current loss is:  0.37061288952827454\n",
      "Warning: nan gradient found. The current loss is:  0.4179636240005493\n",
      "Warning: nan gradient found. The current loss is:  0.30888593196868896\n",
      "Warning: nan gradient found. The current loss is:  0.7947609424591064\n",
      "Warning: nan gradient found. The current loss is:  0.7718579769134521\n",
      "Warning: nan gradient found. The current loss is:  0.20240214467048645\n",
      "Warning: nan gradient found. The current loss is:  0.512661874294281\n",
      "Warning: nan gradient found. The current loss is:  1.1891305446624756\n",
      "Warning: nan gradient found. The current loss is:  0.5664045810699463\n",
      "Warning: nan gradient found. The current loss is:  1.1272497177124023\n",
      "Warning: nan gradient found. The current loss is:  0.9966509342193604\n",
      "Warning: nan gradient found. The current loss is:  0.3592803180217743\n",
      "Warning: nan gradient found. The current loss is:  0.5392837524414062\n",
      "Warning: nan gradient found. The current loss is:  1.0664441585540771\n",
      "Warning: nan gradient found. The current loss is:  0.5182594656944275\n",
      "Warning: nan gradient found. The current loss is:  0.7467676401138306\n",
      "Warning: nan gradient found. The current loss is:  0.48541101813316345\n",
      "Warning: nan gradient found. The current loss is:  0.43272075057029724\n",
      "Warning: nan gradient found. The current loss is:  0.43704086542129517\n",
      "Warning: nan gradient found. The current loss is:  0.23642146587371826\n",
      "Warning: nan gradient found. The current loss is:  0.7663309574127197\n",
      "Warning: nan gradient found. The current loss is:  0.4251907467842102\n",
      "Warning: nan gradient found. The current loss is:  0.4307166039943695\n",
      "Warning: nan gradient found. The current loss is:  0.5304788947105408\n",
      "Warning: nan gradient found. The current loss is:  0.4040347933769226\n",
      "Warning: nan gradient found. The current loss is:  0.6951464414596558\n",
      "Warning: nan gradient found. The current loss is:  0.27888554334640503\n",
      "Warning: nan gradient found. The current loss is:  0.28005868196487427\n",
      "Warning: nan gradient found. The current loss is:  0.8030179738998413\n",
      "Warning: nan gradient found. The current loss is:  0.6350304484367371\n",
      "Warning: nan gradient found. The current loss is:  0.47645309567451477\n",
      "Warning: nan gradient found. The current loss is:  0.3691501021385193\n",
      "Warning: nan gradient found. The current loss is:  0.5865755677223206\n",
      "Warning: nan gradient found. The current loss is:  0.7208344340324402\n",
      "Warning: nan gradient found. The current loss is:  0.18276558816432953\n",
      "Warning: nan gradient found. The current loss is:  0.38647428154945374\n",
      "Warning: nan gradient found. The current loss is:  0.48766282200813293\n",
      "Warning: nan gradient found. The current loss is:  1.1310369968414307\n",
      "Warning: nan gradient found. The current loss is:  0.25581055879592896\n",
      "Warning: nan gradient found. The current loss is:  0.5378702878952026\n",
      "Warning: nan gradient found. The current loss is:  0.4760916829109192\n",
      "Warning: nan gradient found. The current loss is:  2.9532830715179443\n",
      "Warning: nan gradient found. The current loss is:  0.8065103888511658\n",
      "Warning: nan gradient found. The current loss is:  0.8651584386825562\n",
      "Warning: nan gradient found. The current loss is:  0.840538501739502\n",
      "Warning: nan gradient found. The current loss is:  0.7051887512207031\n",
      "Warning: nan gradient found. The current loss is:  0.8960112929344177\n",
      "Warning: nan gradient found. The current loss is:  0.46972930431365967\n",
      "Warning: nan gradient found. The current loss is:  0.7922793030738831\n",
      "Warning: nan gradient found. The current loss is:  0.15682224929332733\n",
      "Warning: nan gradient found. The current loss is:  0.7092637419700623\n",
      "Warning: nan gradient found. The current loss is:  0.5569063425064087\n",
      "Warning: nan gradient found. The current loss is:  0.5619261860847473\n",
      "Warning: nan gradient found. The current loss is:  0.4631701707839966\n",
      "Warning: nan gradient found. The current loss is:  0.7341126799583435\n",
      "Warning: nan gradient found. The current loss is:  0.385337233543396\n",
      "Warning: nan gradient found. The current loss is:  0.958003044128418\n",
      "Warning: nan gradient found. The current loss is:  0.4111548662185669\n",
      "Warning: nan gradient found. The current loss is:  0.39765891432762146\n",
      "Warning: nan gradient found. The current loss is:  0.13482126593589783\n",
      "Warning: nan gradient found. The current loss is:  0.13325192034244537\n",
      "Warning: nan gradient found. The current loss is:  2.136742115020752\n",
      "Warning: nan gradient found. The current loss is:  0.8020325899124146\n",
      "Warning: nan gradient found. The current loss is:  0.4421953558921814\n",
      "Warning: nan gradient found. The current loss is:  0.35801517963409424\n",
      "Warning: nan gradient found. The current loss is:  0.7471568584442139\n",
      "Warning: nan gradient found. The current loss is:  0.35492685437202454\n",
      "Warning: nan gradient found. The current loss is:  0.8747451901435852\n",
      "Warning: nan gradient found. The current loss is:  1.5694670677185059\n",
      "Warning: nan gradient found. The current loss is:  0.32181984186172485\n",
      "Warning: nan gradient found. The current loss is:  0.2951090335845947\n",
      "Warning: nan gradient found. The current loss is:  0.9423562288284302\n",
      "Warning: nan gradient found. The current loss is:  0.5708949565887451\n",
      "Warning: nan gradient found. The current loss is:  1.0994362831115723\n",
      "Warning: nan gradient found. The current loss is:  0.4013689458370209\n",
      "Warning: nan gradient found. The current loss is:  0.17842796444892883\n",
      "Warning: nan gradient found. The current loss is:  1.2371315956115723\n",
      "Warning: nan gradient found. The current loss is:  0.49432671070098877\n",
      "Current batch training loss: 0.494327  [1356800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4817926585674286\n",
      "Warning: nan gradient found. The current loss is:  0.6725456118583679\n",
      "Warning: nan gradient found. The current loss is:  0.44525814056396484\n",
      "Warning: nan gradient found. The current loss is:  0.3083743155002594\n",
      "Warning: nan gradient found. The current loss is:  0.3709549307823181\n",
      "Warning: nan gradient found. The current loss is:  0.33292627334594727\n",
      "Warning: nan gradient found. The current loss is:  -0.017641868442296982\n",
      "Warning: nan gradient found. The current loss is:  1.3814735412597656\n",
      "Warning: nan gradient found. The current loss is:  0.7556483149528503\n",
      "Warning: nan gradient found. The current loss is:  0.5021854639053345\n",
      "Warning: nan gradient found. The current loss is:  0.3406865894794464\n",
      "Warning: nan gradient found. The current loss is:  0.7018252015113831\n",
      "Warning: nan gradient found. The current loss is:  0.9265936613082886\n",
      "Warning: nan gradient found. The current loss is:  1.0303926467895508\n",
      "Warning: nan gradient found. The current loss is:  0.7740304470062256\n",
      "Warning: nan gradient found. The current loss is:  0.745252251625061\n",
      "Warning: nan gradient found. The current loss is:  0.25200891494750977\n",
      "Warning: nan gradient found. The current loss is:  0.20397886633872986\n",
      "Warning: nan gradient found. The current loss is:  0.7753949165344238\n",
      "Warning: nan gradient found. The current loss is:  0.5363385677337646\n",
      "Warning: nan gradient found. The current loss is:  0.534294605255127\n",
      "Warning: nan gradient found. The current loss is:  0.2701779007911682\n",
      "Warning: nan gradient found. The current loss is:  0.34997084736824036\n",
      "Warning: nan gradient found. The current loss is:  0.47133931517601013\n",
      "Warning: nan gradient found. The current loss is:  0.972198486328125\n",
      "Warning: nan gradient found. The current loss is:  0.2139817476272583\n",
      "Warning: nan gradient found. The current loss is:  0.29520612955093384\n",
      "Warning: nan gradient found. The current loss is:  0.6480884552001953\n",
      "Warning: nan gradient found. The current loss is:  0.6229103803634644\n",
      "Warning: nan gradient found. The current loss is:  0.3035433888435364\n",
      "Warning: nan gradient found. The current loss is:  0.45221370458602905\n",
      "Warning: nan gradient found. The current loss is:  0.7006785273551941\n",
      "Warning: nan gradient found. The current loss is:  0.8297148942947388\n",
      "Warning: nan gradient found. The current loss is:  1.2929821014404297\n",
      "Warning: nan gradient found. The current loss is:  0.698938250541687\n",
      "Warning: nan gradient found. The current loss is:  0.1873689889907837\n",
      "Warning: nan gradient found. The current loss is:  0.7719332575798035\n",
      "Warning: nan gradient found. The current loss is:  0.6073556542396545\n",
      "Warning: nan gradient found. The current loss is:  0.5093902349472046\n",
      "Warning: nan gradient found. The current loss is:  0.7984914183616638\n",
      "Warning: nan gradient found. The current loss is:  0.18594253063201904\n",
      "Warning: nan gradient found. The current loss is:  0.7742223143577576\n",
      "Warning: nan gradient found. The current loss is:  0.3270688056945801\n",
      "Warning: nan gradient found. The current loss is:  0.15646681189537048\n",
      "Warning: nan gradient found. The current loss is:  0.706017017364502\n",
      "Warning: nan gradient found. The current loss is:  0.6327086091041565\n",
      "Warning: nan gradient found. The current loss is:  0.39740878343582153\n",
      "Warning: nan gradient found. The current loss is:  0.9419412612915039\n",
      "Warning: nan gradient found. The current loss is:  1.074141025543213\n",
      "Warning: nan gradient found. The current loss is:  0.4554155170917511\n",
      "Warning: nan gradient found. The current loss is:  0.38340187072753906\n",
      "Warning: nan gradient found. The current loss is:  0.48102641105651855\n",
      "Warning: nan gradient found. The current loss is:  0.4728848338127136\n",
      "Warning: nan gradient found. The current loss is:  0.6596378087997437\n",
      "Warning: nan gradient found. The current loss is:  0.43409988284111023\n",
      "Warning: nan gradient found. The current loss is:  0.3871196508407593\n",
      "Warning: nan gradient found. The current loss is:  0.283404678106308\n",
      "Warning: nan gradient found. The current loss is:  0.41428810358047485\n",
      "Warning: nan gradient found. The current loss is:  -0.004222361370921135\n",
      "Warning: nan gradient found. The current loss is:  0.13667303323745728\n",
      "Warning: nan gradient found. The current loss is:  0.2535683810710907\n",
      "Warning: nan gradient found. The current loss is:  0.9238731861114502\n",
      "Warning: nan gradient found. The current loss is:  0.30154651403427124\n",
      "Warning: nan gradient found. The current loss is:  0.7006071209907532\n",
      "Warning: nan gradient found. The current loss is:  0.37516260147094727\n",
      "Warning: nan gradient found. The current loss is:  0.7612995505332947\n",
      "Warning: nan gradient found. The current loss is:  0.4824013113975525\n",
      "Warning: nan gradient found. The current loss is:  0.41590604186058044\n",
      "Warning: nan gradient found. The current loss is:  0.1907162219285965\n",
      "Warning: nan gradient found. The current loss is:  0.5216368436813354\n",
      "Warning: nan gradient found. The current loss is:  0.23599842190742493\n",
      "Warning: nan gradient found. The current loss is:  0.3849788308143616\n",
      "Warning: nan gradient found. The current loss is:  1.087151050567627\n",
      "Warning: nan gradient found. The current loss is:  0.48695680499076843\n",
      "Warning: nan gradient found. The current loss is:  0.4843209981918335\n",
      "Warning: nan gradient found. The current loss is:  0.8957623839378357\n",
      "Warning: nan gradient found. The current loss is:  0.19450555741786957\n",
      "Warning: nan gradient found. The current loss is:  0.9489693641662598\n",
      "Warning: nan gradient found. The current loss is:  0.5355496406555176\n",
      "Warning: nan gradient found. The current loss is:  -0.008117679506540298\n",
      "Warning: nan gradient found. The current loss is:  0.13307447731494904\n",
      "Warning: nan gradient found. The current loss is:  0.611461877822876\n",
      "Warning: nan gradient found. The current loss is:  0.7892081141471863\n",
      "Warning: nan gradient found. The current loss is:  0.7186511158943176\n",
      "Warning: nan gradient found. The current loss is:  1.3156626224517822\n",
      "Warning: nan gradient found. The current loss is:  1.0187076330184937\n",
      "Warning: nan gradient found. The current loss is:  0.19972644746303558\n",
      "Warning: nan gradient found. The current loss is:  1.0203428268432617\n",
      "Warning: nan gradient found. The current loss is:  0.646226167678833\n",
      "Warning: nan gradient found. The current loss is:  0.4711533784866333\n",
      "Warning: nan gradient found. The current loss is:  0.7886499762535095\n",
      "Warning: nan gradient found. The current loss is:  0.044675856828689575\n",
      "Warning: nan gradient found. The current loss is:  0.3689872622489929\n",
      "Warning: nan gradient found. The current loss is:  0.8455770015716553\n",
      "Warning: nan gradient found. The current loss is:  -0.1109551414847374\n",
      "Warning: nan gradient found. The current loss is:  0.8791837096214294\n",
      "Warning: nan gradient found. The current loss is:  0.5459401607513428\n",
      "Warning: nan gradient found. The current loss is:  0.5278840661048889\n",
      "Warning: nan gradient found. The current loss is:  0.190558522939682\n",
      "Warning: nan gradient found. The current loss is:  0.15275119245052338\n",
      "Current batch training loss: 0.152751  [1382400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.30799323320388794\n",
      "Warning: nan gradient found. The current loss is:  0.2790762484073639\n",
      "Warning: nan gradient found. The current loss is:  0.6071560978889465\n",
      "Warning: nan gradient found. The current loss is:  0.30144405364990234\n",
      "Warning: nan gradient found. The current loss is:  0.13628457486629486\n",
      "Warning: nan gradient found. The current loss is:  0.47468772530555725\n",
      "Warning: nan gradient found. The current loss is:  0.44550251960754395\n",
      "Warning: nan gradient found. The current loss is:  0.323150098323822\n",
      "Warning: nan gradient found. The current loss is:  0.5352548360824585\n",
      "Warning: nan gradient found. The current loss is:  0.26555925607681274\n",
      "Warning: nan gradient found. The current loss is:  0.5640490055084229\n",
      "Warning: nan gradient found. The current loss is:  0.9052464962005615\n",
      "Warning: nan gradient found. The current loss is:  1.1862943172454834\n",
      "Warning: nan gradient found. The current loss is:  1.9568095207214355\n",
      "Warning: nan gradient found. The current loss is:  0.11826081573963165\n",
      "Warning: nan gradient found. The current loss is:  0.5423142910003662\n",
      "Warning: nan gradient found. The current loss is:  0.5797520279884338\n",
      "Warning: nan gradient found. The current loss is:  0.25408121943473816\n",
      "Warning: nan gradient found. The current loss is:  0.9080815315246582\n",
      "Warning: nan gradient found. The current loss is:  0.4135255217552185\n",
      "Warning: nan gradient found. The current loss is:  0.6301745772361755\n",
      "Warning: nan gradient found. The current loss is:  0.37107613682746887\n",
      "Warning: nan gradient found. The current loss is:  1.0005955696105957\n",
      "Warning: nan gradient found. The current loss is:  1.5060527324676514\n",
      "Warning: nan gradient found. The current loss is:  0.793978750705719\n",
      "Warning: nan gradient found. The current loss is:  0.564341127872467\n",
      "Warning: nan gradient found. The current loss is:  0.5130874514579773\n",
      "Warning: nan gradient found. The current loss is:  0.5904197096824646\n",
      "Warning: nan gradient found. The current loss is:  0.5433565378189087\n",
      "Warning: nan gradient found. The current loss is:  0.8185069561004639\n",
      "Warning: nan gradient found. The current loss is:  0.6958886384963989\n",
      "Warning: nan gradient found. The current loss is:  0.7067252993583679\n",
      "Warning: nan gradient found. The current loss is:  0.5705181956291199\n",
      "Warning: nan gradient found. The current loss is:  0.7282333374023438\n",
      "Warning: nan gradient found. The current loss is:  0.42781925201416016\n",
      "Warning: nan gradient found. The current loss is:  0.9916398525238037\n",
      "Warning: nan gradient found. The current loss is:  0.11338379979133606\n",
      "Warning: nan gradient found. The current loss is:  0.5409718751907349\n",
      "Warning: nan gradient found. The current loss is:  1.2043602466583252\n",
      "Warning: nan gradient found. The current loss is:  0.3050544857978821\n",
      "Warning: nan gradient found. The current loss is:  0.45837798714637756\n",
      "Warning: nan gradient found. The current loss is:  0.7137203216552734\n",
      "Warning: nan gradient found. The current loss is:  0.2028501182794571\n",
      "Warning: nan gradient found. The current loss is:  0.41317951679229736\n",
      "Warning: nan gradient found. The current loss is:  0.29907968640327454\n",
      "Warning: nan gradient found. The current loss is:  0.6874473094940186\n",
      "Warning: nan gradient found. The current loss is:  0.4639225900173187\n",
      "Warning: nan gradient found. The current loss is:  0.608860433101654\n",
      "Warning: nan gradient found. The current loss is:  1.0671911239624023\n",
      "Warning: nan gradient found. The current loss is:  0.14022599160671234\n",
      "Warning: nan gradient found. The current loss is:  0.6101321578025818\n",
      "Warning: nan gradient found. The current loss is:  0.37123215198516846\n",
      "Warning: nan gradient found. The current loss is:  0.4924757480621338\n",
      "Warning: nan gradient found. The current loss is:  0.5418548583984375\n",
      "Warning: nan gradient found. The current loss is:  1.113579273223877\n",
      "Warning: nan gradient found. The current loss is:  0.7227355241775513\n",
      "Warning: nan gradient found. The current loss is:  0.33934149146080017\n",
      "Warning: nan gradient found. The current loss is:  0.5080124139785767\n",
      "Warning: nan gradient found. The current loss is:  0.27281612157821655\n",
      "Warning: nan gradient found. The current loss is:  2.4819869995117188\n",
      "Warning: nan gradient found. The current loss is:  0.20753727853298187\n",
      "Warning: nan gradient found. The current loss is:  0.8341121673583984\n",
      "Warning: nan gradient found. The current loss is:  0.647428572177887\n",
      "Warning: nan gradient found. The current loss is:  0.3280980885028839\n",
      "Warning: nan gradient found. The current loss is:  1.172803282737732\n",
      "Warning: nan gradient found. The current loss is:  0.4163241386413574\n",
      "Warning: nan gradient found. The current loss is:  0.5869372487068176\n",
      "Warning: nan gradient found. The current loss is:  0.5988417863845825\n",
      "Warning: nan gradient found. The current loss is:  0.8616479635238647\n",
      "Warning: nan gradient found. The current loss is:  0.7846179008483887\n",
      "Warning: nan gradient found. The current loss is:  0.693023681640625\n",
      "Warning: nan gradient found. The current loss is:  0.757224440574646\n",
      "Warning: nan gradient found. The current loss is:  0.39884722232818604\n",
      "Warning: nan gradient found. The current loss is:  0.09585259109735489\n",
      "Warning: nan gradient found. The current loss is:  0.4009639024734497\n",
      "Warning: nan gradient found. The current loss is:  0.1506975293159485\n",
      "Warning: nan gradient found. The current loss is:  0.8369630575180054\n",
      "Warning: nan gradient found. The current loss is:  1.0577120780944824\n",
      "Warning: nan gradient found. The current loss is:  0.9989125728607178\n",
      "Warning: nan gradient found. The current loss is:  0.8866113424301147\n",
      "Warning: nan gradient found. The current loss is:  0.05732638016343117\n",
      "Warning: nan gradient found. The current loss is:  0.3513995409011841\n",
      "Warning: nan gradient found. The current loss is:  0.20939570665359497\n",
      "Warning: nan gradient found. The current loss is:  0.3002040386199951\n",
      "Warning: nan gradient found. The current loss is:  0.39102742075920105\n",
      "Warning: nan gradient found. The current loss is:  0.4577373266220093\n",
      "Warning: nan gradient found. The current loss is:  0.12455299496650696\n",
      "Warning: nan gradient found. The current loss is:  0.840468168258667\n",
      "Warning: nan gradient found. The current loss is:  0.11299704760313034\n",
      "Warning: nan gradient found. The current loss is:  0.42341119050979614\n",
      "Warning: nan gradient found. The current loss is:  0.8521704077720642\n",
      "Warning: nan gradient found. The current loss is:  0.2037143111228943\n",
      "Warning: nan gradient found. The current loss is:  0.6306637525558472\n",
      "Warning: nan gradient found. The current loss is:  1.0839537382125854\n",
      "Warning: nan gradient found. The current loss is:  0.47821831703186035\n",
      "Warning: nan gradient found. The current loss is:  1.0604774951934814\n",
      "Warning: nan gradient found. The current loss is:  0.6687800884246826\n",
      "Warning: nan gradient found. The current loss is:  0.4452824592590332\n",
      "Warning: nan gradient found. The current loss is:  0.2986571788787842\n",
      "Warning: nan gradient found. The current loss is:  1.086562991142273\n",
      "Current batch training loss: 1.086563  [1408000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4131171405315399\n",
      "Warning: nan gradient found. The current loss is:  0.5226997137069702\n",
      "Warning: nan gradient found. The current loss is:  0.840932309627533\n",
      "Warning: nan gradient found. The current loss is:  0.3068947494029999\n",
      "Warning: nan gradient found. The current loss is:  0.1668640524148941\n",
      "Warning: nan gradient found. The current loss is:  0.14789140224456787\n",
      "Warning: nan gradient found. The current loss is:  0.13338682055473328\n",
      "Warning: nan gradient found. The current loss is:  0.042364656925201416\n",
      "Warning: nan gradient found. The current loss is:  0.6731112003326416\n",
      "Warning: nan gradient found. The current loss is:  0.9368306398391724\n",
      "Warning: nan gradient found. The current loss is:  1.1671690940856934\n",
      "Warning: nan gradient found. The current loss is:  0.4836556315422058\n",
      "Warning: nan gradient found. The current loss is:  0.38721269369125366\n",
      "Warning: nan gradient found. The current loss is:  0.4241862893104553\n",
      "Warning: nan gradient found. The current loss is:  0.39945676922798157\n",
      "Warning: nan gradient found. The current loss is:  0.23599527776241302\n",
      "Warning: nan gradient found. The current loss is:  0.618388295173645\n",
      "Warning: nan gradient found. The current loss is:  0.9541730880737305\n",
      "Warning: nan gradient found. The current loss is:  0.6592364311218262\n",
      "Warning: nan gradient found. The current loss is:  0.2654474377632141\n",
      "Warning: nan gradient found. The current loss is:  0.8466796278953552\n",
      "Warning: nan gradient found. The current loss is:  0.5438864827156067\n",
      "Warning: nan gradient found. The current loss is:  1.426415205001831\n",
      "Warning: nan gradient found. The current loss is:  0.7994776368141174\n",
      "Warning: nan gradient found. The current loss is:  0.4369661509990692\n",
      "Warning: nan gradient found. The current loss is:  0.6227374076843262\n",
      "Warning: nan gradient found. The current loss is:  0.8301919102668762\n",
      "Warning: nan gradient found. The current loss is:  0.4474754333496094\n",
      "Warning: nan gradient found. The current loss is:  0.28992414474487305\n",
      "Warning: nan gradient found. The current loss is:  0.39149045944213867\n",
      "Warning: nan gradient found. The current loss is:  0.4844930171966553\n",
      "Warning: nan gradient found. The current loss is:  0.6950416564941406\n",
      "Warning: nan gradient found. The current loss is:  0.2695951461791992\n",
      "Warning: nan gradient found. The current loss is:  0.3567526936531067\n",
      "Warning: nan gradient found. The current loss is:  0.5665494203567505\n",
      "Warning: nan gradient found. The current loss is:  0.9622766971588135\n",
      "Warning: nan gradient found. The current loss is:  0.45844775438308716\n",
      "Warning: nan gradient found. The current loss is:  0.4063471853733063\n",
      "Warning: nan gradient found. The current loss is:  0.30860015749931335\n",
      "Warning: nan gradient found. The current loss is:  0.539778470993042\n",
      "Warning: nan gradient found. The current loss is:  0.6222196817398071\n",
      "Warning: nan gradient found. The current loss is:  0.29940134286880493\n",
      "Warning: nan gradient found. The current loss is:  0.583466649055481\n",
      "Warning: nan gradient found. The current loss is:  0.618277907371521\n",
      "Warning: nan gradient found. The current loss is:  0.6370970010757446\n",
      "Warning: nan gradient found. The current loss is:  0.9686733484268188\n",
      "Warning: nan gradient found. The current loss is:  0.585887610912323\n",
      "Warning: nan gradient found. The current loss is:  0.6554259061813354\n",
      "Warning: nan gradient found. The current loss is:  0.6458491086959839\n",
      "Warning: nan gradient found. The current loss is:  0.21705570816993713\n",
      "Warning: nan gradient found. The current loss is:  0.2016039341688156\n",
      "Warning: nan gradient found. The current loss is:  0.37687167525291443\n",
      "Warning: nan gradient found. The current loss is:  0.530571699142456\n",
      "Warning: nan gradient found. The current loss is:  0.3465626835823059\n",
      "Warning: nan gradient found. The current loss is:  0.5458437204360962\n",
      "Warning: nan gradient found. The current loss is:  0.5713387131690979\n",
      "Warning: nan gradient found. The current loss is:  0.5344556570053101\n",
      "Warning: nan gradient found. The current loss is:  0.33605366945266724\n",
      "Warning: nan gradient found. The current loss is:  0.5586144924163818\n",
      "Warning: nan gradient found. The current loss is:  0.28837910294532776\n",
      "Warning: nan gradient found. The current loss is:  0.5113239288330078\n",
      "Warning: nan gradient found. The current loss is:  0.11062908172607422\n",
      "Warning: nan gradient found. The current loss is:  0.7558721899986267\n",
      "Warning: nan gradient found. The current loss is:  0.5191342830657959\n",
      "Warning: nan gradient found. The current loss is:  0.8230257630348206\n",
      "Warning: nan gradient found. The current loss is:  0.575816810131073\n",
      "Warning: nan gradient found. The current loss is:  0.4089571237564087\n",
      "Warning: nan gradient found. The current loss is:  0.8236169815063477\n",
      "Warning: nan gradient found. The current loss is:  0.5247757434844971\n",
      "Warning: nan gradient found. The current loss is:  0.4572012722492218\n",
      "Warning: nan gradient found. The current loss is:  0.874385416507721\n",
      "Warning: nan gradient found. The current loss is:  0.7213324308395386\n",
      "Warning: nan gradient found. The current loss is:  1.0663206577301025\n",
      "Warning: nan gradient found. The current loss is:  0.8440802097320557\n",
      "Warning: nan gradient found. The current loss is:  0.40334630012512207\n",
      "Warning: nan gradient found. The current loss is:  0.2738379240036011\n",
      "Warning: nan gradient found. The current loss is:  0.44790294766426086\n",
      "Warning: nan gradient found. The current loss is:  1.362346887588501\n",
      "Warning: nan gradient found. The current loss is:  0.5297772884368896\n",
      "Warning: nan gradient found. The current loss is:  0.524249792098999\n",
      "Warning: nan gradient found. The current loss is:  0.5299226641654968\n",
      "Warning: nan gradient found. The current loss is:  0.6513631343841553\n",
      "Warning: nan gradient found. The current loss is:  0.7665771245956421\n",
      "Warning: nan gradient found. The current loss is:  0.19503164291381836\n",
      "Warning: nan gradient found. The current loss is:  0.41939929127693176\n",
      "Warning: nan gradient found. The current loss is:  0.45822739601135254\n",
      "Warning: nan gradient found. The current loss is:  0.5356912016868591\n",
      "Warning: nan gradient found. The current loss is:  0.028282560408115387\n",
      "Warning: nan gradient found. The current loss is:  0.8358041644096375\n",
      "Warning: nan gradient found. The current loss is:  1.0864077806472778\n",
      "Warning: nan gradient found. The current loss is:  0.4918055534362793\n",
      "Warning: nan gradient found. The current loss is:  0.45518070459365845\n",
      "Warning: nan gradient found. The current loss is:  0.5754244327545166\n",
      "Warning: nan gradient found. The current loss is:  0.3467327058315277\n",
      "Warning: nan gradient found. The current loss is:  0.7332719564437866\n",
      "Warning: nan gradient found. The current loss is:  0.5189692378044128\n",
      "Warning: nan gradient found. The current loss is:  0.7655351161956787\n",
      "Warning: nan gradient found. The current loss is:  0.498155802488327\n",
      "Warning: nan gradient found. The current loss is:  2.8946757316589355\n",
      "Warning: nan gradient found. The current loss is:  0.25958821177482605\n",
      "Current batch training loss: 0.259588  [1433600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.3052932024002075\n",
      "Warning: nan gradient found. The current loss is:  0.4305844306945801\n",
      "Warning: nan gradient found. The current loss is:  1.1399812698364258\n",
      "Warning: nan gradient found. The current loss is:  0.6869555711746216\n",
      "Warning: nan gradient found. The current loss is:  0.6699175238609314\n",
      "Warning: nan gradient found. The current loss is:  0.2243730127811432\n",
      "Warning: nan gradient found. The current loss is:  0.536491870880127\n",
      "Warning: nan gradient found. The current loss is:  0.6817715167999268\n",
      "Warning: nan gradient found. The current loss is:  0.779028058052063\n",
      "Warning: nan gradient found. The current loss is:  0.7172548174858093\n",
      "Warning: nan gradient found. The current loss is:  0.9895630478858948\n",
      "Warning: nan gradient found. The current loss is:  1.0957436561584473\n",
      "Warning: nan gradient found. The current loss is:  0.40794166922569275\n",
      "Warning: nan gradient found. The current loss is:  1.561946153640747\n",
      "Warning: nan gradient found. The current loss is:  0.6268569231033325\n",
      "Warning: nan gradient found. The current loss is:  0.8119082450866699\n",
      "Warning: nan gradient found. The current loss is:  0.8306680917739868\n",
      "Warning: nan gradient found. The current loss is:  0.6558396220207214\n",
      "Warning: nan gradient found. The current loss is:  1.315852165222168\n",
      "Warning: nan gradient found. The current loss is:  0.7043150067329407\n",
      "Warning: nan gradient found. The current loss is:  0.6179790496826172\n",
      "Warning: nan gradient found. The current loss is:  0.6713957786560059\n",
      "Warning: nan gradient found. The current loss is:  1.9308078289031982\n",
      "Warning: nan gradient found. The current loss is:  0.836543083190918\n",
      "Warning: nan gradient found. The current loss is:  0.6990956664085388\n",
      "Warning: nan gradient found. The current loss is:  0.600581169128418\n",
      "Warning: nan gradient found. The current loss is:  0.4967295527458191\n",
      "Warning: nan gradient found. The current loss is:  0.06340885162353516\n",
      "Warning: nan gradient found. The current loss is:  0.08615671098232269\n",
      "Warning: nan gradient found. The current loss is:  0.1792929619550705\n",
      "Warning: nan gradient found. The current loss is:  0.537616491317749\n",
      "Warning: nan gradient found. The current loss is:  1.3132612705230713\n",
      "Warning: nan gradient found. The current loss is:  0.48862653970718384\n",
      "Warning: nan gradient found. The current loss is:  1.0040245056152344\n",
      "Warning: nan gradient found. The current loss is:  0.39156490564346313\n",
      "Warning: nan gradient found. The current loss is:  0.061373721808195114\n",
      "Warning: nan gradient found. The current loss is:  0.5902577042579651\n",
      "Warning: nan gradient found. The current loss is:  0.5298374891281128\n",
      "Warning: nan gradient found. The current loss is:  0.6356134414672852\n",
      "Warning: nan gradient found. The current loss is:  0.4806824028491974\n",
      "Warning: nan gradient found. The current loss is:  0.0998380035161972\n",
      "Warning: nan gradient found. The current loss is:  0.3520376682281494\n",
      "Warning: nan gradient found. The current loss is:  0.23687443137168884\n",
      "Warning: nan gradient found. The current loss is:  0.8260406255722046\n",
      "Warning: nan gradient found. The current loss is:  0.14305175840854645\n",
      "Warning: nan gradient found. The current loss is:  0.7428081035614014\n",
      "Warning: nan gradient found. The current loss is:  0.3783580958843231\n",
      "Warning: nan gradient found. The current loss is:  0.5285979509353638\n",
      "Warning: nan gradient found. The current loss is:  0.5681576132774353\n",
      "Warning: nan gradient found. The current loss is:  0.4858112335205078\n",
      "Warning: nan gradient found. The current loss is:  0.5219433307647705\n",
      "Warning: nan gradient found. The current loss is:  0.7369014620780945\n",
      "Warning: nan gradient found. The current loss is:  0.6326841115951538\n",
      "Warning: nan gradient found. The current loss is:  0.3640331029891968\n",
      "Warning: nan gradient found. The current loss is:  0.3524024486541748\n",
      "Warning: nan gradient found. The current loss is:  0.1073697954416275\n",
      "Warning: nan gradient found. The current loss is:  0.6187467575073242\n",
      "Warning: nan gradient found. The current loss is:  0.06464865803718567\n",
      "Warning: nan gradient found. The current loss is:  0.5424153208732605\n",
      "Warning: nan gradient found. The current loss is:  0.9193387031555176\n",
      "Warning: nan gradient found. The current loss is:  0.28265124559402466\n",
      "Warning: nan gradient found. The current loss is:  0.6240845918655396\n",
      "Warning: nan gradient found. The current loss is:  1.035536766052246\n",
      "Warning: nan gradient found. The current loss is:  0.3679269552230835\n",
      "Warning: nan gradient found. The current loss is:  0.25258198380470276\n",
      "Warning: nan gradient found. The current loss is:  0.28026556968688965\n",
      "Warning: nan gradient found. The current loss is:  0.2436751276254654\n",
      "Warning: nan gradient found. The current loss is:  0.47407475113868713\n",
      "Warning: nan gradient found. The current loss is:  0.29217758774757385\n",
      "Warning: nan gradient found. The current loss is:  0.4924432039260864\n",
      "Warning: nan gradient found. The current loss is:  0.05501794070005417\n",
      "Warning: nan gradient found. The current loss is:  0.8189190626144409\n",
      "Warning: nan gradient found. The current loss is:  0.13095353543758392\n",
      "Warning: nan gradient found. The current loss is:  2.238168716430664\n",
      "Warning: nan gradient found. The current loss is:  0.3009420335292816\n",
      "Warning: nan gradient found. The current loss is:  0.4524068832397461\n",
      "Warning: nan gradient found. The current loss is:  0.5926027297973633\n",
      "Warning: nan gradient found. The current loss is:  0.9471292495727539\n",
      "Warning: nan gradient found. The current loss is:  0.9355320930480957\n",
      "Warning: nan gradient found. The current loss is:  0.43560266494750977\n",
      "Warning: nan gradient found. The current loss is:  0.6518974900245667\n",
      "Warning: nan gradient found. The current loss is:  0.6021303534507751\n",
      "Warning: nan gradient found. The current loss is:  0.7423990964889526\n",
      "Warning: nan gradient found. The current loss is:  0.5097706317901611\n",
      "Warning: nan gradient found. The current loss is:  0.1502324789762497\n",
      "Warning: nan gradient found. The current loss is:  0.4164527952671051\n",
      "Warning: nan gradient found. The current loss is:  0.4773617386817932\n",
      "Warning: nan gradient found. The current loss is:  0.06874895095825195\n",
      "Warning: nan gradient found. The current loss is:  0.3594794273376465\n",
      "Warning: nan gradient found. The current loss is:  1.1221272945404053\n",
      "Warning: nan gradient found. The current loss is:  0.72322678565979\n",
      "Warning: nan gradient found. The current loss is:  0.003353886306285858\n",
      "Warning: nan gradient found. The current loss is:  0.5884701609611511\n",
      "Warning: nan gradient found. The current loss is:  0.8700092434883118\n",
      "Warning: nan gradient found. The current loss is:  0.4328495264053345\n",
      "Warning: nan gradient found. The current loss is:  0.3814130425453186\n",
      "Warning: nan gradient found. The current loss is:  0.02539907395839691\n",
      "Warning: nan gradient found. The current loss is:  0.521887481212616\n",
      "Warning: nan gradient found. The current loss is:  0.9812873601913452\n",
      "Warning: nan gradient found. The current loss is:  0.35977572202682495\n",
      "Current batch training loss: 0.359776  [1459200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.13264960050582886\n",
      "Warning: nan gradient found. The current loss is:  1.164095401763916\n",
      "Warning: nan gradient found. The current loss is:  -0.06964852660894394\n",
      "Warning: nan gradient found. The current loss is:  0.8214698433876038\n",
      "Warning: nan gradient found. The current loss is:  0.6241577863693237\n",
      "Warning: nan gradient found. The current loss is:  0.6347814798355103\n",
      "Warning: nan gradient found. The current loss is:  1.2861075401306152\n",
      "Warning: nan gradient found. The current loss is:  0.3426094651222229\n",
      "Warning: nan gradient found. The current loss is:  1.39139723777771\n",
      "Warning: nan gradient found. The current loss is:  0.609463095664978\n",
      "Warning: nan gradient found. The current loss is:  0.5967356562614441\n",
      "Warning: nan gradient found. The current loss is:  0.5108944773674011\n",
      "Warning: nan gradient found. The current loss is:  1.1157231330871582\n",
      "Warning: nan gradient found. The current loss is:  0.8042609691619873\n",
      "Warning: nan gradient found. The current loss is:  0.2383740097284317\n",
      "Warning: nan gradient found. The current loss is:  0.6958303451538086\n",
      "Warning: nan gradient found. The current loss is:  0.4133930802345276\n",
      "Warning: nan gradient found. The current loss is:  0.0868048369884491\n",
      "Warning: nan gradient found. The current loss is:  0.8353078961372375\n",
      "Warning: nan gradient found. The current loss is:  0.5140884518623352\n",
      "Warning: nan gradient found. The current loss is:  0.369485080242157\n",
      "Warning: nan gradient found. The current loss is:  0.4908044934272766\n",
      "Warning: nan gradient found. The current loss is:  0.3900502920150757\n",
      "Warning: nan gradient found. The current loss is:  0.701210618019104\n",
      "Warning: nan gradient found. The current loss is:  0.22683921456336975\n",
      "Warning: nan gradient found. The current loss is:  0.2273949831724167\n",
      "Warning: nan gradient found. The current loss is:  0.46851110458374023\n",
      "Warning: nan gradient found. The current loss is:  0.7383847236633301\n",
      "Warning: nan gradient found. The current loss is:  0.5224951505661011\n",
      "Warning: nan gradient found. The current loss is:  0.6753071546554565\n",
      "Warning: nan gradient found. The current loss is:  0.49907034635543823\n",
      "Warning: nan gradient found. The current loss is:  0.7280325293540955\n",
      "Warning: nan gradient found. The current loss is:  1.7340128421783447\n",
      "Warning: nan gradient found. The current loss is:  1.316666603088379\n",
      "Warning: nan gradient found. The current loss is:  1.1372478008270264\n",
      "Warning: nan gradient found. The current loss is:  0.6229305267333984\n",
      "Warning: nan gradient found. The current loss is:  0.5016251802444458\n",
      "Warning: nan gradient found. The current loss is:  0.9977022409439087\n",
      "Warning: nan gradient found. The current loss is:  0.3714497685432434\n",
      "Warning: nan gradient found. The current loss is:  0.6015653014183044\n",
      "Warning: nan gradient found. The current loss is:  0.20427334308624268\n",
      "Warning: nan gradient found. The current loss is:  0.5827717781066895\n",
      "Warning: nan gradient found. The current loss is:  1.9643362760543823\n",
      "Warning: nan gradient found. The current loss is:  0.802107036113739\n",
      "Warning: nan gradient found. The current loss is:  0.81312096118927\n",
      "Warning: nan gradient found. The current loss is:  0.6821733713150024\n",
      "Warning: nan gradient found. The current loss is:  0.6579447984695435\n",
      "Warning: nan gradient found. The current loss is:  0.49824708700180054\n",
      "Warning: nan gradient found. The current loss is:  0.5801834464073181\n",
      "Warning: nan gradient found. The current loss is:  0.3309336006641388\n",
      "Warning: nan gradient found. The current loss is:  0.6192522048950195\n",
      "Warning: nan gradient found. The current loss is:  1.9549643993377686\n",
      "Warning: nan gradient found. The current loss is:  0.548344612121582\n",
      "Warning: nan gradient found. The current loss is:  0.4636889100074768\n",
      "Warning: nan gradient found. The current loss is:  0.20296119153499603\n",
      "Warning: nan gradient found. The current loss is:  0.1510935127735138\n",
      "Warning: nan gradient found. The current loss is:  0.18349581956863403\n",
      "Warning: nan gradient found. The current loss is:  1.0800753831863403\n",
      "Warning: nan gradient found. The current loss is:  0.4965422749519348\n",
      "Warning: nan gradient found. The current loss is:  0.45262640714645386\n",
      "Warning: nan gradient found. The current loss is:  0.42847615480422974\n",
      "Warning: nan gradient found. The current loss is:  0.3999837040901184\n",
      "Warning: nan gradient found. The current loss is:  0.44664502143859863\n",
      "Warning: nan gradient found. The current loss is:  0.24214699864387512\n",
      "Warning: nan gradient found. The current loss is:  0.030245307832956314\n",
      "Warning: nan gradient found. The current loss is:  1.1653435230255127\n",
      "Warning: nan gradient found. The current loss is:  0.7391074895858765\n",
      "Warning: nan gradient found. The current loss is:  1.3846890926361084\n",
      "Warning: nan gradient found. The current loss is:  1.6503260135650635\n",
      "Warning: nan gradient found. The current loss is:  0.48899099230766296\n",
      "Warning: nan gradient found. The current loss is:  0.3146750330924988\n",
      "Warning: nan gradient found. The current loss is:  0.7528406381607056\n",
      "Warning: nan gradient found. The current loss is:  0.7453169226646423\n",
      "Warning: nan gradient found. The current loss is:  0.7920518517494202\n",
      "Warning: nan gradient found. The current loss is:  0.7502281069755554\n",
      "Warning: nan gradient found. The current loss is:  0.3880975842475891\n",
      "Warning: nan gradient found. The current loss is:  0.38871529698371887\n",
      "Warning: nan gradient found. The current loss is:  1.3158015012741089\n",
      "Warning: nan gradient found. The current loss is:  1.5601999759674072\n",
      "Warning: nan gradient found. The current loss is:  0.10308849811553955\n",
      "Warning: nan gradient found. The current loss is:  0.5843626856803894\n",
      "Warning: nan gradient found. The current loss is:  0.5586948394775391\n",
      "Warning: nan gradient found. The current loss is:  0.7191278338432312\n",
      "Warning: nan gradient found. The current loss is:  0.7970555424690247\n",
      "Warning: nan gradient found. The current loss is:  1.2817049026489258\n",
      "Warning: nan gradient found. The current loss is:  0.4900108277797699\n",
      "Warning: nan gradient found. The current loss is:  0.7184423208236694\n",
      "Warning: nan gradient found. The current loss is:  0.488726943731308\n",
      "Warning: nan gradient found. The current loss is:  0.2735508680343628\n",
      "Warning: nan gradient found. The current loss is:  0.1545368880033493\n",
      "Warning: nan gradient found. The current loss is:  0.3183402717113495\n",
      "Warning: nan gradient found. The current loss is:  0.9476906657218933\n",
      "Warning: nan gradient found. The current loss is:  0.7366139888763428\n",
      "Warning: nan gradient found. The current loss is:  0.6203771829605103\n",
      "Warning: nan gradient found. The current loss is:  0.08783014118671417\n",
      "Warning: nan gradient found. The current loss is:  0.9025683999061584\n",
      "Warning: nan gradient found. The current loss is:  0.429941862821579\n",
      "Warning: nan gradient found. The current loss is:  0.4498782753944397\n",
      "Warning: nan gradient found. The current loss is:  0.26672905683517456\n",
      "Warning: nan gradient found. The current loss is:  0.6157585382461548\n",
      "Current batch training loss: 0.615759  [1484800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6402235627174377\n",
      "Warning: nan gradient found. The current loss is:  0.4931183159351349\n",
      "Warning: nan gradient found. The current loss is:  0.5083351135253906\n",
      "Warning: nan gradient found. The current loss is:  0.6032549738883972\n",
      "Warning: nan gradient found. The current loss is:  0.7479720711708069\n",
      "Warning: nan gradient found. The current loss is:  0.3929659128189087\n",
      "Warning: nan gradient found. The current loss is:  0.8537949919700623\n",
      "Warning: nan gradient found. The current loss is:  0.5786394476890564\n",
      "Warning: nan gradient found. The current loss is:  0.3157569169998169\n",
      "Warning: nan gradient found. The current loss is:  0.4278768301010132\n",
      "Warning: nan gradient found. The current loss is:  0.4090065360069275\n",
      "Warning: nan gradient found. The current loss is:  0.5158787369728088\n",
      "Warning: nan gradient found. The current loss is:  0.187223881483078\n",
      "Warning: nan gradient found. The current loss is:  0.4182240962982178\n",
      "Warning: nan gradient found. The current loss is:  0.6430530548095703\n",
      "Warning: nan gradient found. The current loss is:  0.1073095053434372\n",
      "Warning: nan gradient found. The current loss is:  0.3521779775619507\n",
      "Warning: nan gradient found. The current loss is:  0.5963285565376282\n",
      "Warning: nan gradient found. The current loss is:  1.5020508766174316\n",
      "Warning: nan gradient found. The current loss is:  0.5396561622619629\n",
      "Warning: nan gradient found. The current loss is:  0.6969481706619263\n",
      "Warning: nan gradient found. The current loss is:  1.111154317855835\n",
      "Warning: nan gradient found. The current loss is:  0.8142702579498291\n",
      "Warning: nan gradient found. The current loss is:  0.5378076434135437\n",
      "Warning: nan gradient found. The current loss is:  0.7064528465270996\n",
      "Warning: nan gradient found. The current loss is:  0.42947524785995483\n",
      "Warning: nan gradient found. The current loss is:  0.34745901823043823\n",
      "Warning: nan gradient found. The current loss is:  0.4499693214893341\n",
      "Warning: nan gradient found. The current loss is:  0.5342243909835815\n",
      "Warning: nan gradient found. The current loss is:  0.4397117495536804\n",
      "Warning: nan gradient found. The current loss is:  0.49882379174232483\n",
      "Warning: nan gradient found. The current loss is:  0.19752605259418488\n",
      "Warning: nan gradient found. The current loss is:  0.14652332663536072\n",
      "Warning: nan gradient found. The current loss is:  0.6237488985061646\n",
      "Warning: nan gradient found. The current loss is:  0.6126776337623596\n",
      "Warning: nan gradient found. The current loss is:  0.32755133509635925\n",
      "Warning: nan gradient found. The current loss is:  0.7209545373916626\n",
      "Warning: nan gradient found. The current loss is:  0.4538806974887848\n",
      "Warning: nan gradient found. The current loss is:  0.9347032308578491\n",
      "Warning: nan gradient found. The current loss is:  0.3073938190937042\n",
      "Warning: nan gradient found. The current loss is:  0.5090300440788269\n",
      "Warning: nan gradient found. The current loss is:  1.0882264375686646\n",
      "Warning: nan gradient found. The current loss is:  0.661418080329895\n",
      "Warning: nan gradient found. The current loss is:  0.477987676858902\n",
      "Warning: nan gradient found. The current loss is:  0.6919028759002686\n",
      "Warning: nan gradient found. The current loss is:  1.1064529418945312\n",
      "Warning: nan gradient found. The current loss is:  1.0516798496246338\n",
      "Warning: nan gradient found. The current loss is:  0.17019891738891602\n",
      "Warning: nan gradient found. The current loss is:  0.3258516192436218\n",
      "Warning: nan gradient found. The current loss is:  0.5479862093925476\n",
      "Warning: nan gradient found. The current loss is:  0.43008852005004883\n",
      "Warning: nan gradient found. The current loss is:  0.9693687558174133\n",
      "Warning: nan gradient found. The current loss is:  0.051664087921381\n",
      "Warning: nan gradient found. The current loss is:  0.900319516658783\n",
      "Warning: nan gradient found. The current loss is:  0.30214500427246094\n",
      "Warning: nan gradient found. The current loss is:  0.047190941870212555\n",
      "Warning: nan gradient found. The current loss is:  0.2689780592918396\n",
      "Warning: nan gradient found. The current loss is:  0.7395670413970947\n",
      "Warning: nan gradient found. The current loss is:  0.6997638940811157\n",
      "Warning: nan gradient found. The current loss is:  0.796280562877655\n",
      "Warning: nan gradient found. The current loss is:  1.225386142730713\n",
      "Warning: nan gradient found. The current loss is:  0.06450437009334564\n",
      "Warning: nan gradient found. The current loss is:  0.4179038107395172\n",
      "Warning: nan gradient found. The current loss is:  -0.02907034382224083\n",
      "Warning: nan gradient found. The current loss is:  1.1431599855422974\n",
      "Warning: nan gradient found. The current loss is:  0.39389947056770325\n",
      "Warning: nan gradient found. The current loss is:  0.45514488220214844\n",
      "Warning: nan gradient found. The current loss is:  0.6841431856155396\n",
      "Warning: nan gradient found. The current loss is:  0.9878423810005188\n",
      "Warning: nan gradient found. The current loss is:  0.31704872846603394\n",
      "Warning: nan gradient found. The current loss is:  0.7459703087806702\n",
      "Warning: nan gradient found. The current loss is:  0.7430576086044312\n",
      "Warning: nan gradient found. The current loss is:  0.3049994707107544\n",
      "Warning: nan gradient found. The current loss is:  0.6246142983436584\n",
      "Warning: nan gradient found. The current loss is:  0.43326646089553833\n",
      "Warning: nan gradient found. The current loss is:  0.571906328201294\n",
      "Warning: nan gradient found. The current loss is:  0.4124211370944977\n",
      "Warning: nan gradient found. The current loss is:  0.6580274701118469\n",
      "Warning: nan gradient found. The current loss is:  0.3824382722377777\n",
      "Warning: nan gradient found. The current loss is:  0.3869434595108032\n",
      "Warning: nan gradient found. The current loss is:  0.539063036441803\n",
      "Warning: nan gradient found. The current loss is:  1.0099669694900513\n",
      "Warning: nan gradient found. The current loss is:  0.19379177689552307\n",
      "Warning: nan gradient found. The current loss is:  0.411906361579895\n",
      "Warning: nan gradient found. The current loss is:  0.1732534021139145\n",
      "Warning: nan gradient found. The current loss is:  1.4182910919189453\n",
      "Warning: nan gradient found. The current loss is:  0.5123133659362793\n",
      "Warning: nan gradient found. The current loss is:  0.49197813868522644\n",
      "Warning: nan gradient found. The current loss is:  0.6552015542984009\n",
      "Warning: nan gradient found. The current loss is:  0.04743891581892967\n",
      "Warning: nan gradient found. The current loss is:  0.18456049263477325\n",
      "Warning: nan gradient found. The current loss is:  0.6923655867576599\n",
      "Warning: nan gradient found. The current loss is:  0.4334908127784729\n",
      "Warning: nan gradient found. The current loss is:  0.11366529762744904\n",
      "Warning: nan gradient found. The current loss is:  0.6929957866668701\n",
      "Warning: nan gradient found. The current loss is:  1.3830674886703491\n",
      "Warning: nan gradient found. The current loss is:  0.2602624297142029\n",
      "Warning: nan gradient found. The current loss is:  0.18854564428329468\n",
      "Warning: nan gradient found. The current loss is:  0.668178915977478\n",
      "Warning: nan gradient found. The current loss is:  1.031563401222229\n",
      "Current batch training loss: 1.031563  [1510400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7277724742889404\n",
      "Warning: nan gradient found. The current loss is:  1.1424132585525513\n",
      "Warning: nan gradient found. The current loss is:  0.8973273038864136\n",
      "Warning: nan gradient found. The current loss is:  0.5363906025886536\n",
      "Warning: nan gradient found. The current loss is:  0.09021632373332977\n",
      "Warning: nan gradient found. The current loss is:  0.6822690963745117\n",
      "Warning: nan gradient found. The current loss is:  -0.009627889841794968\n",
      "Warning: nan gradient found. The current loss is:  0.5684889554977417\n",
      "Warning: nan gradient found. The current loss is:  0.511397123336792\n",
      "Warning: nan gradient found. The current loss is:  0.4962896406650543\n",
      "Warning: nan gradient found. The current loss is:  0.8985451459884644\n",
      "Warning: nan gradient found. The current loss is:  0.27272069454193115\n",
      "Warning: nan gradient found. The current loss is:  0.5166609883308411\n",
      "Warning: nan gradient found. The current loss is:  0.42156338691711426\n",
      "Warning: nan gradient found. The current loss is:  0.8337759971618652\n",
      "Warning: nan gradient found. The current loss is:  0.4488757848739624\n",
      "Warning: nan gradient found. The current loss is:  0.14336277544498444\n",
      "Warning: nan gradient found. The current loss is:  -0.026122644543647766\n",
      "Warning: nan gradient found. The current loss is:  0.6987915635108948\n",
      "Warning: nan gradient found. The current loss is:  0.7504560351371765\n",
      "Warning: nan gradient found. The current loss is:  0.28249451518058777\n",
      "Warning: nan gradient found. The current loss is:  0.5271352529525757\n",
      "Warning: nan gradient found. The current loss is:  0.8323467969894409\n",
      "Warning: nan gradient found. The current loss is:  0.6207531690597534\n",
      "Warning: nan gradient found. The current loss is:  0.28525328636169434\n",
      "Warning: nan gradient found. The current loss is:  0.8325448632240295\n",
      "Warning: nan gradient found. The current loss is:  -0.11814400553703308\n",
      "Warning: nan gradient found. The current loss is:  0.4030420780181885\n",
      "Warning: nan gradient found. The current loss is:  2.008859157562256\n",
      "Warning: nan gradient found. The current loss is:  0.2735583484172821\n",
      "Warning: nan gradient found. The current loss is:  0.4062010645866394\n",
      "Warning: nan gradient found. The current loss is:  0.6731909513473511\n",
      "Warning: nan gradient found. The current loss is:  0.6316689252853394\n",
      "Warning: nan gradient found. The current loss is:  0.39251673221588135\n",
      "Warning: nan gradient found. The current loss is:  0.4655916690826416\n",
      "Warning: nan gradient found. The current loss is:  1.041330337524414\n",
      "Warning: nan gradient found. The current loss is:  0.12252302467823029\n",
      "Warning: nan gradient found. The current loss is:  0.38382622599601746\n",
      "Warning: nan gradient found. The current loss is:  0.8141288757324219\n",
      "Warning: nan gradient found. The current loss is:  0.2415952980518341\n",
      "Warning: nan gradient found. The current loss is:  1.1777113676071167\n",
      "Warning: nan gradient found. The current loss is:  0.7003298997879028\n",
      "Warning: nan gradient found. The current loss is:  0.8943037390708923\n",
      "Warning: nan gradient found. The current loss is:  0.03622770309448242\n",
      "Warning: nan gradient found. The current loss is:  0.4521353542804718\n",
      "Warning: nan gradient found. The current loss is:  1.0023859739303589\n",
      "Warning: nan gradient found. The current loss is:  0.43641307950019836\n",
      "Warning: nan gradient found. The current loss is:  1.0853757858276367\n",
      "Warning: nan gradient found. The current loss is:  0.43674302101135254\n",
      "Warning: nan gradient found. The current loss is:  0.5372965335845947\n",
      "Warning: nan gradient found. The current loss is:  1.1813541650772095\n",
      "Warning: nan gradient found. The current loss is:  0.8921643495559692\n",
      "Warning: nan gradient found. The current loss is:  0.7015525102615356\n",
      "Warning: nan gradient found. The current loss is:  2.0338757038116455\n",
      "Warning: nan gradient found. The current loss is:  0.4737588167190552\n",
      "Warning: nan gradient found. The current loss is:  0.3455836772918701\n",
      "Warning: nan gradient found. The current loss is:  0.5178971290588379\n",
      "Warning: nan gradient found. The current loss is:  0.5627325177192688\n",
      "Warning: nan gradient found. The current loss is:  0.44254836440086365\n",
      "Warning: nan gradient found. The current loss is:  0.5968629717826843\n",
      "Warning: nan gradient found. The current loss is:  0.25407975912094116\n",
      "Warning: nan gradient found. The current loss is:  0.8973743319511414\n",
      "Warning: nan gradient found. The current loss is:  0.18896102905273438\n",
      "Warning: nan gradient found. The current loss is:  1.1754189729690552\n",
      "Warning: nan gradient found. The current loss is:  0.7239564657211304\n",
      "Warning: nan gradient found. The current loss is:  1.4305633306503296\n",
      "Warning: nan gradient found. The current loss is:  1.004715085029602\n",
      "Warning: nan gradient found. The current loss is:  1.3833948373794556\n",
      "Warning: nan gradient found. The current loss is:  0.0202353335916996\n",
      "Warning: nan gradient found. The current loss is:  0.6934483051300049\n",
      "Warning: nan gradient found. The current loss is:  0.5000134706497192\n",
      "Warning: nan gradient found. The current loss is:  0.13618406653404236\n",
      "Warning: nan gradient found. The current loss is:  0.619117021560669\n",
      "Warning: nan gradient found. The current loss is:  0.45901912450790405\n",
      "Warning: nan gradient found. The current loss is:  0.5772920846939087\n",
      "Warning: nan gradient found. The current loss is:  0.5379643440246582\n",
      "Warning: nan gradient found. The current loss is:  0.3664088845252991\n",
      "Warning: nan gradient found. The current loss is:  0.857645571231842\n",
      "Warning: nan gradient found. The current loss is:  0.37502050399780273\n",
      "Warning: nan gradient found. The current loss is:  0.6309725642204285\n",
      "Warning: nan gradient found. The current loss is:  0.40763145685195923\n",
      "Warning: nan gradient found. The current loss is:  0.22142069041728973\n",
      "Warning: nan gradient found. The current loss is:  0.9167606234550476\n",
      "Warning: nan gradient found. The current loss is:  0.19398854672908783\n",
      "Warning: nan gradient found. The current loss is:  0.6835922002792358\n",
      "Warning: nan gradient found. The current loss is:  0.16829748451709747\n",
      "Warning: nan gradient found. The current loss is:  1.285181999206543\n",
      "Warning: nan gradient found. The current loss is:  0.43499311804771423\n",
      "Warning: nan gradient found. The current loss is:  0.7723221778869629\n",
      "Warning: nan gradient found. The current loss is:  0.25196996331214905\n",
      "Warning: nan gradient found. The current loss is:  0.308383047580719\n",
      "Warning: nan gradient found. The current loss is:  0.020207086578011513\n",
      "Warning: nan gradient found. The current loss is:  0.47411784529685974\n",
      "Warning: nan gradient found. The current loss is:  0.8600976467132568\n",
      "Warning: nan gradient found. The current loss is:  0.6958574056625366\n",
      "Warning: nan gradient found. The current loss is:  1.0038036108016968\n",
      "Warning: nan gradient found. The current loss is:  0.38840240240097046\n",
      "Warning: nan gradient found. The current loss is:  0.4041390120983124\n",
      "Warning: nan gradient found. The current loss is:  0.1513693481683731\n",
      "Warning: nan gradient found. The current loss is:  0.33169132471084595\n",
      "Current batch training loss: 0.331691  [1536000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9558104872703552\n",
      "Warning: nan gradient found. The current loss is:  0.7651658058166504\n",
      "Warning: nan gradient found. The current loss is:  0.694351077079773\n",
      "Warning: nan gradient found. The current loss is:  0.4548470079898834\n",
      "Warning: nan gradient found. The current loss is:  0.6656720042228699\n",
      "Warning: nan gradient found. The current loss is:  1.210574984550476\n",
      "Warning: nan gradient found. The current loss is:  0.48471519351005554\n",
      "Warning: nan gradient found. The current loss is:  0.5586519241333008\n",
      "Warning: nan gradient found. The current loss is:  0.8592747449874878\n",
      "Warning: nan gradient found. The current loss is:  0.37493419647216797\n",
      "Warning: nan gradient found. The current loss is:  0.54360032081604\n",
      "Warning: nan gradient found. The current loss is:  0.45060187578201294\n",
      "Warning: nan gradient found. The current loss is:  0.3165165185928345\n",
      "Warning: nan gradient found. The current loss is:  0.7128354907035828\n",
      "Warning: nan gradient found. The current loss is:  0.3787139058113098\n",
      "Warning: nan gradient found. The current loss is:  0.5897513628005981\n",
      "Warning: nan gradient found. The current loss is:  0.8344906568527222\n",
      "Warning: nan gradient found. The current loss is:  0.517913818359375\n",
      "Warning: nan gradient found. The current loss is:  0.5569654107093811\n",
      "Warning: nan gradient found. The current loss is:  0.30091214179992676\n",
      "Warning: nan gradient found. The current loss is:  0.47233694791793823\n",
      "Warning: nan gradient found. The current loss is:  0.7740881443023682\n",
      "Warning: nan gradient found. The current loss is:  0.6452617645263672\n",
      "Warning: nan gradient found. The current loss is:  0.39260250329971313\n",
      "Warning: nan gradient found. The current loss is:  -0.043571218848228455\n",
      "Warning: nan gradient found. The current loss is:  0.18691280484199524\n",
      "Warning: nan gradient found. The current loss is:  0.743982195854187\n",
      "Warning: nan gradient found. The current loss is:  0.31640592217445374\n",
      "Warning: nan gradient found. The current loss is:  0.4214640259742737\n",
      "Warning: nan gradient found. The current loss is:  -0.05565226078033447\n",
      "Warning: nan gradient found. The current loss is:  0.5701508522033691\n",
      "Warning: nan gradient found. The current loss is:  0.38413697481155396\n",
      "Warning: nan gradient found. The current loss is:  0.4412842392921448\n",
      "Warning: nan gradient found. The current loss is:  0.4867439270019531\n",
      "Warning: nan gradient found. The current loss is:  1.0377912521362305\n",
      "Warning: nan gradient found. The current loss is:  0.3125738501548767\n",
      "Warning: nan gradient found. The current loss is:  0.48193737864494324\n",
      "Warning: nan gradient found. The current loss is:  0.47594010829925537\n",
      "Warning: nan gradient found. The current loss is:  0.9361211061477661\n",
      "Warning: nan gradient found. The current loss is:  0.6690735220909119\n",
      "Warning: nan gradient found. The current loss is:  0.5789192914962769\n",
      "Warning: nan gradient found. The current loss is:  0.45191627740859985\n",
      "Warning: nan gradient found. The current loss is:  0.34605103731155396\n",
      "Warning: nan gradient found. The current loss is:  1.61046302318573\n",
      "Warning: nan gradient found. The current loss is:  0.20691311359405518\n",
      "Warning: nan gradient found. The current loss is:  0.47271206974983215\n",
      "Warning: nan gradient found. The current loss is:  0.3319348096847534\n",
      "Warning: nan gradient found. The current loss is:  0.7361627817153931\n",
      "Warning: nan gradient found. The current loss is:  0.8468260169029236\n",
      "Warning: nan gradient found. The current loss is:  0.3999794125556946\n",
      "Warning: nan gradient found. The current loss is:  0.7938843965530396\n",
      "Warning: nan gradient found. The current loss is:  0.5133656859397888\n",
      "Warning: nan gradient found. The current loss is:  0.694886326789856\n",
      "Warning: nan gradient found. The current loss is:  0.18405690789222717\n",
      "Warning: nan gradient found. The current loss is:  0.42690181732177734\n",
      "Warning: nan gradient found. The current loss is:  0.9972204566001892\n",
      "Warning: nan gradient found. The current loss is:  0.9834370017051697\n",
      "Warning: nan gradient found. The current loss is:  1.2092112302780151\n",
      "Warning: nan gradient found. The current loss is:  0.20361530780792236\n",
      "Warning: nan gradient found. The current loss is:  0.7145706415176392\n",
      "Warning: nan gradient found. The current loss is:  0.1507544070482254\n",
      "Warning: nan gradient found. The current loss is:  0.6251825094223022\n",
      "Warning: nan gradient found. The current loss is:  0.9077284336090088\n",
      "Warning: nan gradient found. The current loss is:  0.8843740224838257\n",
      "Warning: nan gradient found. The current loss is:  0.5387675166130066\n",
      "Warning: nan gradient found. The current loss is:  1.0905210971832275\n",
      "Warning: nan gradient found. The current loss is:  0.8774839639663696\n",
      "Warning: nan gradient found. The current loss is:  0.10951603949069977\n",
      "Warning: nan gradient found. The current loss is:  0.009270906448364258\n",
      "Warning: nan gradient found. The current loss is:  0.6217973828315735\n",
      "Warning: nan gradient found. The current loss is:  0.6439129114151001\n",
      "Warning: nan gradient found. The current loss is:  0.5462256669998169\n",
      "Warning: nan gradient found. The current loss is:  0.37610113620758057\n",
      "Warning: nan gradient found. The current loss is:  0.14873820543289185\n",
      "Warning: nan gradient found. The current loss is:  1.1185572147369385\n",
      "Warning: nan gradient found. The current loss is:  0.4064173698425293\n",
      "Warning: nan gradient found. The current loss is:  0.4595831632614136\n",
      "Warning: nan gradient found. The current loss is:  0.5703526735305786\n",
      "Warning: nan gradient found. The current loss is:  0.20713406801223755\n",
      "Warning: nan gradient found. The current loss is:  0.32033440470695496\n",
      "Warning: nan gradient found. The current loss is:  0.2943756580352783\n",
      "Warning: nan gradient found. The current loss is:  0.3851862847805023\n",
      "Warning: nan gradient found. The current loss is:  0.3279880881309509\n",
      "Warning: nan gradient found. The current loss is:  2.2895495891571045\n",
      "Warning: nan gradient found. The current loss is:  0.5282232761383057\n",
      "Warning: nan gradient found. The current loss is:  1.2268805503845215\n",
      "Warning: nan gradient found. The current loss is:  0.8219344615936279\n",
      "Warning: nan gradient found. The current loss is:  0.321327269077301\n",
      "Warning: nan gradient found. The current loss is:  0.679496705532074\n",
      "Warning: nan gradient found. The current loss is:  0.9625821709632874\n",
      "Warning: nan gradient found. The current loss is:  0.7183893918991089\n",
      "Warning: nan gradient found. The current loss is:  0.21184353530406952\n",
      "Warning: nan gradient found. The current loss is:  0.12760064005851746\n",
      "Warning: nan gradient found. The current loss is:  0.5389028787612915\n",
      "Warning: nan gradient found. The current loss is:  0.6471399068832397\n",
      "Warning: nan gradient found. The current loss is:  1.2276384830474854\n",
      "Warning: nan gradient found. The current loss is:  1.1788642406463623\n",
      "Warning: nan gradient found. The current loss is:  0.12959733605384827\n",
      "Warning: nan gradient found. The current loss is:  0.6050653457641602\n",
      "Warning: nan gradient found. The current loss is:  0.2573341727256775\n",
      "Current batch training loss: 0.257334  [1561600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4925064742565155\n",
      "Warning: nan gradient found. The current loss is:  0.7874006628990173\n",
      "Warning: nan gradient found. The current loss is:  0.33359295129776\n",
      "Warning: nan gradient found. The current loss is:  0.9533098936080933\n",
      "Warning: nan gradient found. The current loss is:  0.4196481704711914\n",
      "Warning: nan gradient found. The current loss is:  0.4431961178779602\n",
      "Warning: nan gradient found. The current loss is:  0.17509427666664124\n",
      "Warning: nan gradient found. The current loss is:  0.4086555540561676\n",
      "Warning: nan gradient found. The current loss is:  0.3401198983192444\n",
      "Warning: nan gradient found. The current loss is:  1.3009690046310425\n",
      "Warning: nan gradient found. The current loss is:  0.31455785036087036\n",
      "Warning: nan gradient found. The current loss is:  0.8078385591506958\n",
      "Warning: nan gradient found. The current loss is:  0.3348511755466461\n",
      "Warning: nan gradient found. The current loss is:  0.8695046901702881\n",
      "Warning: nan gradient found. The current loss is:  0.19397801160812378\n",
      "Warning: nan gradient found. The current loss is:  0.6279244422912598\n",
      "Warning: nan gradient found. The current loss is:  1.3786792755126953\n",
      "Warning: nan gradient found. The current loss is:  1.0401337146759033\n",
      "Warning: nan gradient found. The current loss is:  0.6103019714355469\n",
      "Warning: nan gradient found. The current loss is:  0.6178368926048279\n",
      "Warning: nan gradient found. The current loss is:  0.7138752937316895\n",
      "Warning: nan gradient found. The current loss is:  0.983331561088562\n",
      "Warning: nan gradient found. The current loss is:  0.6180961728096008\n",
      "Warning: nan gradient found. The current loss is:  0.4130408763885498\n",
      "Warning: nan gradient found. The current loss is:  0.5532867312431335\n",
      "Warning: nan gradient found. The current loss is:  0.7065920829772949\n",
      "Warning: nan gradient found. The current loss is:  0.22264373302459717\n",
      "Warning: nan gradient found. The current loss is:  0.5917487144470215\n",
      "Warning: nan gradient found. The current loss is:  0.26530295610427856\n",
      "Warning: nan gradient found. The current loss is:  0.6222869753837585\n",
      "Warning: nan gradient found. The current loss is:  0.8259835243225098\n",
      "Warning: nan gradient found. The current loss is:  0.3763810098171234\n",
      "Warning: nan gradient found. The current loss is:  0.5347088575363159\n",
      "Warning: nan gradient found. The current loss is:  0.7092627882957458\n",
      "Warning: nan gradient found. The current loss is:  0.2428073137998581\n",
      "Warning: nan gradient found. The current loss is:  1.1307624578475952\n",
      "Warning: nan gradient found. The current loss is:  0.25299304723739624\n",
      "Warning: nan gradient found. The current loss is:  0.9774736166000366\n",
      "Warning: nan gradient found. The current loss is:  0.09939408302307129\n",
      "Warning: nan gradient found. The current loss is:  0.2097155600786209\n",
      "Warning: nan gradient found. The current loss is:  1.4227252006530762\n",
      "Warning: nan gradient found. The current loss is:  0.4167611300945282\n",
      "Warning: nan gradient found. The current loss is:  0.5422531962394714\n",
      "Warning: nan gradient found. The current loss is:  0.23476189374923706\n",
      "Warning: nan gradient found. The current loss is:  0.6588367819786072\n",
      "Warning: nan gradient found. The current loss is:  1.2183490991592407\n",
      "Warning: nan gradient found. The current loss is:  0.37905311584472656\n",
      "Warning: nan gradient found. The current loss is:  0.4592406153678894\n",
      "Warning: nan gradient found. The current loss is:  0.41721510887145996\n",
      "Warning: nan gradient found. The current loss is:  1.0137261152267456\n",
      "Warning: nan gradient found. The current loss is:  0.3070005178451538\n",
      "Warning: nan gradient found. The current loss is:  0.3544888198375702\n",
      "Warning: nan gradient found. The current loss is:  0.3542710542678833\n",
      "Warning: nan gradient found. The current loss is:  0.2593696713447571\n",
      "Warning: nan gradient found. The current loss is:  1.270622730255127\n",
      "Warning: nan gradient found. The current loss is:  0.5591883063316345\n",
      "Warning: nan gradient found. The current loss is:  0.42699572443962097\n",
      "Warning: nan gradient found. The current loss is:  0.3134099841117859\n",
      "Warning: nan gradient found. The current loss is:  0.3156701922416687\n",
      "Warning: nan gradient found. The current loss is:  0.44773396849632263\n",
      "Warning: nan gradient found. The current loss is:  1.2650985717773438\n",
      "Warning: nan gradient found. The current loss is:  1.2306897640228271\n",
      "Warning: nan gradient found. The current loss is:  0.041062913835048676\n",
      "Warning: nan gradient found. The current loss is:  0.3862694203853607\n",
      "Warning: nan gradient found. The current loss is:  0.17069928348064423\n",
      "Warning: nan gradient found. The current loss is:  0.21696220338344574\n",
      "Warning: nan gradient found. The current loss is:  0.7916984558105469\n",
      "Warning: nan gradient found. The current loss is:  0.5049228668212891\n",
      "Warning: nan gradient found. The current loss is:  0.4528093934059143\n",
      "Warning: nan gradient found. The current loss is:  0.0937783271074295\n",
      "Warning: nan gradient found. The current loss is:  0.19919095933437347\n",
      "Warning: nan gradient found. The current loss is:  0.4342672526836395\n",
      "Warning: nan gradient found. The current loss is:  0.5917841196060181\n",
      "Warning: nan gradient found. The current loss is:  0.7318438291549683\n",
      "Warning: nan gradient found. The current loss is:  0.2684789001941681\n",
      "Warning: nan gradient found. The current loss is:  0.8015912771224976\n",
      "Warning: nan gradient found. The current loss is:  0.5709653496742249\n",
      "Warning: nan gradient found. The current loss is:  0.32490378618240356\n",
      "Warning: nan gradient found. The current loss is:  0.5143704414367676\n",
      "Warning: nan gradient found. The current loss is:  0.0546206459403038\n",
      "Warning: nan gradient found. The current loss is:  0.9966521859169006\n",
      "Warning: nan gradient found. The current loss is:  0.6344510912895203\n",
      "Warning: nan gradient found. The current loss is:  0.38747286796569824\n",
      "Warning: nan gradient found. The current loss is:  0.6005856990814209\n",
      "Warning: nan gradient found. The current loss is:  1.4357881546020508\n",
      "Warning: nan gradient found. The current loss is:  0.6207901835441589\n",
      "Warning: nan gradient found. The current loss is:  0.4029059410095215\n",
      "Warning: nan gradient found. The current loss is:  0.4434058666229248\n",
      "Warning: nan gradient found. The current loss is:  0.1392628401517868\n",
      "Warning: nan gradient found. The current loss is:  0.3486860990524292\n",
      "Warning: nan gradient found. The current loss is:  0.6676929593086243\n",
      "Warning: nan gradient found. The current loss is:  0.3723048269748688\n",
      "Warning: nan gradient found. The current loss is:  0.23318324983119965\n",
      "Warning: nan gradient found. The current loss is:  1.2140417098999023\n",
      "Warning: nan gradient found. The current loss is:  0.652031421661377\n",
      "Warning: nan gradient found. The current loss is:  0.3541221618652344\n",
      "Warning: nan gradient found. The current loss is:  0.38115209341049194\n",
      "Warning: nan gradient found. The current loss is:  0.8196749091148376\n",
      "Warning: nan gradient found. The current loss is:  0.7416314482688904\n",
      "Warning: nan gradient found. The current loss is:  0.08687634021043777\n",
      "Current batch training loss: 0.086876  [1587200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.08030823618173599\n",
      "Warning: nan gradient found. The current loss is:  0.3781142830848694\n",
      "Warning: nan gradient found. The current loss is:  0.20541879534721375\n",
      "Warning: nan gradient found. The current loss is:  1.3873597383499146\n",
      "Warning: nan gradient found. The current loss is:  0.7364606261253357\n",
      "Warning: nan gradient found. The current loss is:  0.5691214203834534\n",
      "Warning: nan gradient found. The current loss is:  1.1446986198425293\n",
      "Warning: nan gradient found. The current loss is:  0.6566031575202942\n",
      "Warning: nan gradient found. The current loss is:  1.075502872467041\n",
      "Warning: nan gradient found. The current loss is:  0.7804210186004639\n",
      "Warning: nan gradient found. The current loss is:  0.41043639183044434\n",
      "Warning: nan gradient found. The current loss is:  1.3077480792999268\n",
      "Warning: nan gradient found. The current loss is:  1.03641939163208\n",
      "Warning: nan gradient found. The current loss is:  1.3770637512207031\n",
      "Warning: nan gradient found. The current loss is:  0.19373440742492676\n",
      "Warning: nan gradient found. The current loss is:  0.40166816115379333\n",
      "Warning: nan gradient found. The current loss is:  0.20966580510139465\n",
      "Warning: nan gradient found. The current loss is:  3.5993683338165283\n",
      "Warning: nan gradient found. The current loss is:  0.4911300241947174\n",
      "Warning: nan gradient found. The current loss is:  0.3415083587169647\n",
      "Warning: nan gradient found. The current loss is:  0.8381893038749695\n",
      "Warning: nan gradient found. The current loss is:  0.4177435040473938\n",
      "Warning: nan gradient found. The current loss is:  0.8493130207061768\n",
      "Warning: nan gradient found. The current loss is:  1.2806909084320068\n",
      "Warning: nan gradient found. The current loss is:  0.6465980410575867\n",
      "Warning: nan gradient found. The current loss is:  0.5606074929237366\n",
      "Warning: nan gradient found. The current loss is:  0.6016245484352112\n",
      "Warning: nan gradient found. The current loss is:  0.9563532471656799\n",
      "Warning: nan gradient found. The current loss is:  0.3243764340877533\n",
      "Warning: nan gradient found. The current loss is:  1.1307764053344727\n",
      "Warning: nan gradient found. The current loss is:  0.5516276359558105\n",
      "Warning: nan gradient found. The current loss is:  0.2979079484939575\n",
      "Warning: nan gradient found. The current loss is:  0.1935468316078186\n",
      "Warning: nan gradient found. The current loss is:  1.193704605102539\n",
      "Warning: nan gradient found. The current loss is:  0.9735884666442871\n",
      "Warning: nan gradient found. The current loss is:  0.32533419132232666\n",
      "Warning: nan gradient found. The current loss is:  0.32558614015579224\n",
      "Warning: nan gradient found. The current loss is:  0.4765002131462097\n",
      "Warning: nan gradient found. The current loss is:  0.8711554408073425\n",
      "Warning: nan gradient found. The current loss is:  0.4878958463668823\n",
      "Warning: nan gradient found. The current loss is:  0.9881857633590698\n",
      "Warning: nan gradient found. The current loss is:  1.051037073135376\n",
      "Warning: nan gradient found. The current loss is:  1.7864444255828857\n",
      "Warning: nan gradient found. The current loss is:  0.5708014965057373\n",
      "Warning: nan gradient found. The current loss is:  0.380337119102478\n",
      "Warning: nan gradient found. The current loss is:  0.3265063166618347\n",
      "Warning: nan gradient found. The current loss is:  1.685586929321289\n",
      "Warning: nan gradient found. The current loss is:  0.1596401482820511\n",
      "Warning: nan gradient found. The current loss is:  0.8948454856872559\n",
      "Warning: nan gradient found. The current loss is:  0.66347736120224\n",
      "Warning: nan gradient found. The current loss is:  0.2567726969718933\n",
      "Warning: nan gradient found. The current loss is:  0.947080671787262\n",
      "Warning: nan gradient found. The current loss is:  2.0797221660614014\n",
      "Warning: nan gradient found. The current loss is:  0.2194381207227707\n",
      "Warning: nan gradient found. The current loss is:  0.583756685256958\n",
      "Warning: nan gradient found. The current loss is:  0.4987178444862366\n",
      "Warning: nan gradient found. The current loss is:  0.5749719738960266\n",
      "Warning: nan gradient found. The current loss is:  0.44526052474975586\n",
      "Warning: nan gradient found. The current loss is:  0.7183316946029663\n",
      "Warning: nan gradient found. The current loss is:  0.6130024194717407\n",
      "Warning: nan gradient found. The current loss is:  0.9877222180366516\n",
      "Warning: nan gradient found. The current loss is:  0.4794265627861023\n",
      "Warning: nan gradient found. The current loss is:  1.2088303565979004\n",
      "Warning: nan gradient found. The current loss is:  0.3486537039279938\n",
      "Warning: nan gradient found. The current loss is:  0.3711892366409302\n",
      "Warning: nan gradient found. The current loss is:  0.42310747504234314\n",
      "Warning: nan gradient found. The current loss is:  0.46246209740638733\n",
      "Warning: nan gradient found. The current loss is:  0.6610569953918457\n",
      "Warning: nan gradient found. The current loss is:  0.6094827055931091\n",
      "Warning: nan gradient found. The current loss is:  0.07284185290336609\n",
      "Warning: nan gradient found. The current loss is:  0.7961313128471375\n",
      "Warning: nan gradient found. The current loss is:  1.757982850074768\n",
      "Warning: nan gradient found. The current loss is:  0.6493802666664124\n",
      "Warning: nan gradient found. The current loss is:  0.5267768502235413\n",
      "Warning: nan gradient found. The current loss is:  0.06081090867519379\n",
      "Warning: nan gradient found. The current loss is:  0.33639565110206604\n",
      "Warning: nan gradient found. The current loss is:  0.6420741081237793\n",
      "Warning: nan gradient found. The current loss is:  0.6437807083129883\n",
      "Warning: nan gradient found. The current loss is:  0.42223310470581055\n",
      "Warning: nan gradient found. The current loss is:  0.37670615315437317\n",
      "Warning: nan gradient found. The current loss is:  0.3320193290710449\n",
      "Warning: nan gradient found. The current loss is:  0.5844718217849731\n",
      "Warning: nan gradient found. The current loss is:  0.358295738697052\n",
      "Warning: nan gradient found. The current loss is:  0.955116331577301\n",
      "Warning: nan gradient found. The current loss is:  0.24296337366104126\n",
      "Warning: nan gradient found. The current loss is:  0.49010926485061646\n",
      "Warning: nan gradient found. The current loss is:  0.6179793477058411\n",
      "Warning: nan gradient found. The current loss is:  0.7998498678207397\n",
      "Warning: nan gradient found. The current loss is:  1.6516454219818115\n",
      "Warning: nan gradient found. The current loss is:  0.4529210925102234\n",
      "Warning: nan gradient found. The current loss is:  0.9304211139678955\n",
      "Warning: nan gradient found. The current loss is:  0.6026537418365479\n",
      "Warning: nan gradient found. The current loss is:  0.4971632957458496\n",
      "Warning: nan gradient found. The current loss is:  0.41544607281684875\n",
      "Warning: nan gradient found. The current loss is:  0.3054831027984619\n",
      "Warning: nan gradient found. The current loss is:  1.7405331134796143\n",
      "Warning: nan gradient found. The current loss is:  0.5025392770767212\n",
      "Warning: nan gradient found. The current loss is:  0.2895393371582031\n",
      "Warning: nan gradient found. The current loss is:  0.40830978751182556\n",
      "Warning: nan gradient found. The current loss is:  0.4720899760723114\n",
      "Current batch training loss: 0.472090  [1612800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6004533767700195\n",
      "Warning: nan gradient found. The current loss is:  0.7992977499961853\n",
      "Warning: nan gradient found. The current loss is:  0.4410085082054138\n",
      "Warning: nan gradient found. The current loss is:  0.3852008581161499\n",
      "Warning: nan gradient found. The current loss is:  0.5491729378700256\n",
      "Warning: nan gradient found. The current loss is:  0.07913956046104431\n",
      "Warning: nan gradient found. The current loss is:  0.36815834045410156\n",
      "Warning: nan gradient found. The current loss is:  0.30251598358154297\n",
      "Warning: nan gradient found. The current loss is:  0.15648458898067474\n",
      "Warning: nan gradient found. The current loss is:  0.19455170631408691\n",
      "Warning: nan gradient found. The current loss is:  0.817581295967102\n",
      "Warning: nan gradient found. The current loss is:  0.3857162296772003\n",
      "Warning: nan gradient found. The current loss is:  1.0312731266021729\n",
      "Warning: nan gradient found. The current loss is:  0.5819422006607056\n",
      "Warning: nan gradient found. The current loss is:  0.6464787125587463\n",
      "Warning: nan gradient found. The current loss is:  0.13512884080410004\n",
      "Warning: nan gradient found. The current loss is:  0.3640313446521759\n",
      "Warning: nan gradient found. The current loss is:  0.6051152944564819\n",
      "Warning: nan gradient found. The current loss is:  0.43434447050094604\n",
      "Warning: nan gradient found. The current loss is:  0.21974946558475494\n",
      "Warning: nan gradient found. The current loss is:  0.9737006425857544\n",
      "Warning: nan gradient found. The current loss is:  0.4308905303478241\n",
      "Warning: nan gradient found. The current loss is:  0.8048043847084045\n",
      "Warning: nan gradient found. The current loss is:  0.6481443643569946\n",
      "Warning: nan gradient found. The current loss is:  0.5761616230010986\n",
      "Warning: nan gradient found. The current loss is:  1.1544499397277832\n",
      "Warning: nan gradient found. The current loss is:  0.7364985942840576\n",
      "Warning: nan gradient found. The current loss is:  0.9996821880340576\n",
      "Warning: nan gradient found. The current loss is:  0.47889915108680725\n",
      "Warning: nan gradient found. The current loss is:  0.9191616773605347\n",
      "Warning: nan gradient found. The current loss is:  0.5451006293296814\n",
      "Warning: nan gradient found. The current loss is:  0.2512317895889282\n",
      "Warning: nan gradient found. The current loss is:  1.049527645111084\n",
      "Warning: nan gradient found. The current loss is:  0.6376651525497437\n",
      "Warning: nan gradient found. The current loss is:  0.3809334635734558\n",
      "Warning: nan gradient found. The current loss is:  0.39518749713897705\n",
      "Warning: nan gradient found. The current loss is:  1.4586482048034668\n",
      "Warning: nan gradient found. The current loss is:  0.553409218788147\n",
      "Warning: nan gradient found. The current loss is:  0.0956905260682106\n",
      "Warning: nan gradient found. The current loss is:  0.6657004356384277\n",
      "Warning: nan gradient found. The current loss is:  0.551077127456665\n",
      "Warning: nan gradient found. The current loss is:  0.9282245635986328\n",
      "Warning: nan gradient found. The current loss is:  0.8481021523475647\n",
      "Warning: nan gradient found. The current loss is:  0.25685733556747437\n",
      "Warning: nan gradient found. The current loss is:  0.5263452529907227\n",
      "Warning: nan gradient found. The current loss is:  0.4336062967777252\n",
      "Warning: nan gradient found. The current loss is:  0.7623200416564941\n",
      "Warning: nan gradient found. The current loss is:  0.3613787293434143\n",
      "Warning: nan gradient found. The current loss is:  0.6627002954483032\n",
      "Warning: nan gradient found. The current loss is:  0.6325209736824036\n",
      "Warning: nan gradient found. The current loss is:  1.636478066444397\n",
      "Warning: nan gradient found. The current loss is:  0.4817180335521698\n",
      "Warning: nan gradient found. The current loss is:  0.7308270931243896\n",
      "Warning: nan gradient found. The current loss is:  0.5096677541732788\n",
      "Warning: nan gradient found. The current loss is:  2.5847537517547607\n",
      "Warning: nan gradient found. The current loss is:  0.4544907212257385\n",
      "Warning: nan gradient found. The current loss is:  0.6548584699630737\n",
      "Warning: nan gradient found. The current loss is:  0.35307735204696655\n",
      "Warning: nan gradient found. The current loss is:  0.549753725528717\n",
      "Warning: nan gradient found. The current loss is:  0.6866909861564636\n",
      "Warning: nan gradient found. The current loss is:  1.4628114700317383\n",
      "Warning: nan gradient found. The current loss is:  0.4949421286582947\n",
      "Warning: nan gradient found. The current loss is:  0.42075401544570923\n",
      "Warning: nan gradient found. The current loss is:  0.5690097808837891\n",
      "Warning: nan gradient found. The current loss is:  0.683955729007721\n",
      "Warning: nan gradient found. The current loss is:  0.012350380420684814\n",
      "Warning: nan gradient found. The current loss is:  0.06400134414434433\n",
      "Warning: nan gradient found. The current loss is:  0.9116204380989075\n",
      "Warning: nan gradient found. The current loss is:  0.10965192317962646\n",
      "Warning: nan gradient found. The current loss is:  0.6736466884613037\n",
      "Warning: nan gradient found. The current loss is:  0.832895040512085\n",
      "Warning: nan gradient found. The current loss is:  0.9012148976325989\n",
      "Warning: nan gradient found. The current loss is:  0.1679152101278305\n",
      "Warning: nan gradient found. The current loss is:  0.6282510757446289\n",
      "Warning: nan gradient found. The current loss is:  1.214032769203186\n",
      "Warning: nan gradient found. The current loss is:  0.707984447479248\n",
      "Warning: nan gradient found. The current loss is:  0.6350013017654419\n",
      "Warning: nan gradient found. The current loss is:  0.42716437578201294\n",
      "Warning: nan gradient found. The current loss is:  0.04976877570152283\n",
      "Warning: nan gradient found. The current loss is:  0.3499181568622589\n",
      "Warning: nan gradient found. The current loss is:  0.4971874952316284\n",
      "Warning: nan gradient found. The current loss is:  0.3781457543373108\n",
      "Warning: nan gradient found. The current loss is:  0.6099883317947388\n",
      "Warning: nan gradient found. The current loss is:  0.18351368606090546\n",
      "Warning: nan gradient found. The current loss is:  1.1033157110214233\n",
      "Warning: nan gradient found. The current loss is:  0.2913036346435547\n",
      "Warning: nan gradient found. The current loss is:  0.9707598090171814\n",
      "Warning: nan gradient found. The current loss is:  1.146993637084961\n",
      "Warning: nan gradient found. The current loss is:  0.5289409160614014\n",
      "Warning: nan gradient found. The current loss is:  0.38407617807388306\n",
      "Warning: nan gradient found. The current loss is:  0.6408829689025879\n",
      "Warning: nan gradient found. The current loss is:  0.047287717461586\n",
      "Warning: nan gradient found. The current loss is:  1.2608342170715332\n",
      "Warning: nan gradient found. The current loss is:  1.6321406364440918\n",
      "Warning: nan gradient found. The current loss is:  0.5215371251106262\n",
      "Warning: nan gradient found. The current loss is:  0.8718386888504028\n",
      "Warning: nan gradient found. The current loss is:  0.4559699296951294\n",
      "Warning: nan gradient found. The current loss is:  0.0984509214758873\n",
      "Warning: nan gradient found. The current loss is:  0.26599037647247314\n",
      "Warning: nan gradient found. The current loss is:  0.5470272302627563\n",
      "Current batch training loss: 0.547027  [1638400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.328880250453949\n",
      "Warning: nan gradient found. The current loss is:  0.5434985756874084\n",
      "Warning: nan gradient found. The current loss is:  0.7307336330413818\n",
      "Warning: nan gradient found. The current loss is:  0.7823017835617065\n",
      "Warning: nan gradient found. The current loss is:  0.22955119609832764\n",
      "Warning: nan gradient found. The current loss is:  0.05085043981671333\n",
      "Warning: nan gradient found. The current loss is:  0.5311734676361084\n",
      "Warning: nan gradient found. The current loss is:  0.6725575923919678\n",
      "Warning: nan gradient found. The current loss is:  0.796346127986908\n",
      "Warning: nan gradient found. The current loss is:  0.6800171136856079\n",
      "Warning: nan gradient found. The current loss is:  0.8758216500282288\n",
      "Warning: nan gradient found. The current loss is:  0.35396966338157654\n",
      "Warning: nan gradient found. The current loss is:  0.191896453499794\n",
      "Warning: nan gradient found. The current loss is:  0.34542781114578247\n",
      "Warning: nan gradient found. The current loss is:  0.33840739727020264\n",
      "Warning: nan gradient found. The current loss is:  0.7577218413352966\n",
      "Warning: nan gradient found. The current loss is:  1.189546823501587\n",
      "Warning: nan gradient found. The current loss is:  1.3650842905044556\n",
      "Warning: nan gradient found. The current loss is:  0.19804106652736664\n",
      "Warning: nan gradient found. The current loss is:  0.14072316884994507\n",
      "Warning: nan gradient found. The current loss is:  1.1980140209197998\n",
      "Warning: nan gradient found. The current loss is:  0.5906673073768616\n",
      "Warning: nan gradient found. The current loss is:  0.5639106035232544\n",
      "Warning: nan gradient found. The current loss is:  0.08620154857635498\n",
      "Warning: nan gradient found. The current loss is:  1.5925407409667969\n",
      "Warning: nan gradient found. The current loss is:  0.5182878971099854\n",
      "Warning: nan gradient found. The current loss is:  0.4957360327243805\n",
      "Warning: nan gradient found. The current loss is:  0.32181525230407715\n",
      "Warning: nan gradient found. The current loss is:  0.7340750694274902\n",
      "Warning: nan gradient found. The current loss is:  0.5943225622177124\n",
      "Warning: nan gradient found. The current loss is:  0.36662784218788147\n",
      "Warning: nan gradient found. The current loss is:  0.5607085227966309\n",
      "Warning: nan gradient found. The current loss is:  0.828000009059906\n",
      "Warning: nan gradient found. The current loss is:  0.8307421207427979\n",
      "Warning: nan gradient found. The current loss is:  0.5491325855255127\n",
      "Warning: nan gradient found. The current loss is:  0.3052009344100952\n",
      "Warning: nan gradient found. The current loss is:  0.8429132699966431\n",
      "Warning: nan gradient found. The current loss is:  0.9130614399909973\n",
      "Warning: nan gradient found. The current loss is:  0.48263251781463623\n",
      "Warning: nan gradient found. The current loss is:  0.3256725072860718\n",
      "Warning: nan gradient found. The current loss is:  1.090881586074829\n",
      "Warning: nan gradient found. The current loss is:  1.0167739391326904\n",
      "Warning: nan gradient found. The current loss is:  0.5140275955200195\n",
      "Warning: nan gradient found. The current loss is:  0.7101348638534546\n",
      "Warning: nan gradient found. The current loss is:  1.956102728843689\n",
      "Warning: nan gradient found. The current loss is:  0.9011517763137817\n",
      "Warning: nan gradient found. The current loss is:  0.5456610918045044\n",
      "Warning: nan gradient found. The current loss is:  0.8370463252067566\n",
      "Warning: nan gradient found. The current loss is:  0.6856060028076172\n",
      "Warning: nan gradient found. The current loss is:  0.9519733786582947\n",
      "Warning: nan gradient found. The current loss is:  0.7474958896636963\n",
      "Warning: nan gradient found. The current loss is:  0.19023780524730682\n",
      "Warning: nan gradient found. The current loss is:  0.02078370377421379\n",
      "Warning: nan gradient found. The current loss is:  0.9006439447402954\n",
      "Warning: nan gradient found. The current loss is:  0.5192570090293884\n",
      "Warning: nan gradient found. The current loss is:  0.21542906761169434\n",
      "Warning: nan gradient found. The current loss is:  0.6910337209701538\n",
      "Warning: nan gradient found. The current loss is:  0.5446954965591431\n",
      "Warning: nan gradient found. The current loss is:  0.16747096180915833\n",
      "Warning: nan gradient found. The current loss is:  0.7470544576644897\n",
      "Warning: nan gradient found. The current loss is:  0.29438042640686035\n",
      "Warning: nan gradient found. The current loss is:  0.09584606438875198\n",
      "Warning: nan gradient found. The current loss is:  0.3663821816444397\n",
      "Warning: nan gradient found. The current loss is:  0.3081972599029541\n",
      "Warning: nan gradient found. The current loss is:  0.4290950894355774\n",
      "Warning: nan gradient found. The current loss is:  1.433817744255066\n",
      "Warning: nan gradient found. The current loss is:  0.07870900630950928\n",
      "Warning: nan gradient found. The current loss is:  0.8616266846656799\n",
      "Warning: nan gradient found. The current loss is:  0.8621346354484558\n",
      "Warning: nan gradient found. The current loss is:  0.3120258152484894\n",
      "Warning: nan gradient found. The current loss is:  0.27960067987442017\n",
      "Warning: nan gradient found. The current loss is:  0.362947940826416\n",
      "Warning: nan gradient found. The current loss is:  0.6573780179023743\n",
      "Warning: nan gradient found. The current loss is:  0.661934494972229\n",
      "Warning: nan gradient found. The current loss is:  0.7553697824478149\n",
      "Warning: nan gradient found. The current loss is:  2.3453550338745117\n",
      "Warning: nan gradient found. The current loss is:  0.7027502059936523\n",
      "Warning: nan gradient found. The current loss is:  0.6304368376731873\n",
      "Warning: nan gradient found. The current loss is:  0.6959782838821411\n",
      "Warning: nan gradient found. The current loss is:  0.30574387311935425\n",
      "Warning: nan gradient found. The current loss is:  0.16023516654968262\n",
      "Warning: nan gradient found. The current loss is:  0.7076630592346191\n",
      "Warning: nan gradient found. The current loss is:  0.787784218788147\n",
      "Warning: nan gradient found. The current loss is:  0.6703120470046997\n",
      "Warning: nan gradient found. The current loss is:  0.710051417350769\n",
      "Warning: nan gradient found. The current loss is:  0.5576608180999756\n",
      "Warning: nan gradient found. The current loss is:  0.43963822722435\n",
      "Warning: nan gradient found. The current loss is:  1.1841342449188232\n",
      "Warning: nan gradient found. The current loss is:  0.28575748205184937\n",
      "Warning: nan gradient found. The current loss is:  0.9501818418502808\n",
      "Warning: nan gradient found. The current loss is:  0.9155477285385132\n",
      "Warning: nan gradient found. The current loss is:  0.3330610394477844\n",
      "Warning: nan gradient found. The current loss is:  0.25283533334732056\n",
      "Warning: nan gradient found. The current loss is:  0.08410204201936722\n",
      "Warning: nan gradient found. The current loss is:  0.5655669569969177\n",
      "Warning: nan gradient found. The current loss is:  0.5486973524093628\n",
      "Warning: nan gradient found. The current loss is:  1.3134052753448486\n",
      "Warning: nan gradient found. The current loss is:  0.41927260160446167\n",
      "Warning: nan gradient found. The current loss is:  0.5085712671279907\n",
      "Warning: nan gradient found. The current loss is:  0.44220131635665894\n",
      "Current batch training loss: 0.442201  [1664000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7896002531051636\n",
      "Warning: nan gradient found. The current loss is:  0.8045123815536499\n",
      "Warning: nan gradient found. The current loss is:  0.35675638914108276\n",
      "Warning: nan gradient found. The current loss is:  0.8106454610824585\n",
      "Warning: nan gradient found. The current loss is:  0.4063228368759155\n",
      "Warning: nan gradient found. The current loss is:  1.3495615720748901\n",
      "Warning: nan gradient found. The current loss is:  0.1798454225063324\n",
      "Warning: nan gradient found. The current loss is:  0.6127275824546814\n",
      "Warning: nan gradient found. The current loss is:  0.5372428894042969\n",
      "Warning: nan gradient found. The current loss is:  0.07440027594566345\n",
      "Warning: nan gradient found. The current loss is:  0.42667266726493835\n",
      "Warning: nan gradient found. The current loss is:  0.7518696784973145\n",
      "Warning: nan gradient found. The current loss is:  0.24846404790878296\n",
      "Warning: nan gradient found. The current loss is:  0.8410131931304932\n",
      "Warning: nan gradient found. The current loss is:  0.5571714639663696\n",
      "Warning: nan gradient found. The current loss is:  0.7861485481262207\n",
      "Warning: nan gradient found. The current loss is:  0.3664602041244507\n",
      "Warning: nan gradient found. The current loss is:  0.33730483055114746\n",
      "Warning: nan gradient found. The current loss is:  0.5517454147338867\n",
      "Warning: nan gradient found. The current loss is:  0.6871117949485779\n",
      "Warning: nan gradient found. The current loss is:  0.3981400430202484\n",
      "Warning: nan gradient found. The current loss is:  0.5602625608444214\n",
      "Warning: nan gradient found. The current loss is:  1.082273006439209\n",
      "Warning: nan gradient found. The current loss is:  0.39238664507865906\n",
      "Warning: nan gradient found. The current loss is:  0.4759978950023651\n",
      "Warning: nan gradient found. The current loss is:  0.6402562260627747\n",
      "Warning: nan gradient found. The current loss is:  0.46124863624572754\n",
      "Warning: nan gradient found. The current loss is:  0.3783406913280487\n",
      "Warning: nan gradient found. The current loss is:  0.49447089433670044\n",
      "Warning: nan gradient found. The current loss is:  0.2684192359447479\n",
      "Warning: nan gradient found. The current loss is:  0.38903841376304626\n",
      "Warning: nan gradient found. The current loss is:  0.4777218699455261\n",
      "Warning: nan gradient found. The current loss is:  0.8572326302528381\n",
      "Warning: nan gradient found. The current loss is:  0.632928729057312\n",
      "Warning: nan gradient found. The current loss is:  0.17849089205265045\n",
      "Warning: nan gradient found. The current loss is:  0.9834365248680115\n",
      "Warning: nan gradient found. The current loss is:  0.7985241413116455\n",
      "Warning: nan gradient found. The current loss is:  0.5974411368370056\n",
      "Warning: nan gradient found. The current loss is:  0.337563157081604\n",
      "Warning: nan gradient found. The current loss is:  0.3814152479171753\n",
      "Warning: nan gradient found. The current loss is:  2.7119264602661133\n",
      "Warning: nan gradient found. The current loss is:  0.5225468873977661\n",
      "Warning: nan gradient found. The current loss is:  0.23409759998321533\n",
      "Warning: nan gradient found. The current loss is:  0.28568029403686523\n",
      "Warning: nan gradient found. The current loss is:  0.17083770036697388\n",
      "Warning: nan gradient found. The current loss is:  0.2759568989276886\n",
      "Warning: nan gradient found. The current loss is:  0.25985071063041687\n",
      "Warning: nan gradient found. The current loss is:  0.6496516466140747\n",
      "Warning: nan gradient found. The current loss is:  0.5938640832901001\n",
      "Warning: nan gradient found. The current loss is:  0.18459784984588623\n",
      "Warning: nan gradient found. The current loss is:  0.7289749979972839\n",
      "Warning: nan gradient found. The current loss is:  0.4420042037963867\n",
      "Warning: nan gradient found. The current loss is:  1.1727796792984009\n",
      "Warning: nan gradient found. The current loss is:  0.5283342599868774\n",
      "Warning: nan gradient found. The current loss is:  0.40748825669288635\n",
      "Warning: nan gradient found. The current loss is:  0.8393906354904175\n",
      "Warning: nan gradient found. The current loss is:  0.5990919470787048\n",
      "Warning: nan gradient found. The current loss is:  0.19857892394065857\n",
      "Warning: nan gradient found. The current loss is:  1.768359661102295\n",
      "Warning: nan gradient found. The current loss is:  0.42589280009269714\n",
      "Warning: nan gradient found. The current loss is:  0.424481064081192\n",
      "Warning: nan gradient found. The current loss is:  0.6750348806381226\n",
      "Warning: nan gradient found. The current loss is:  0.4699997007846832\n",
      "Warning: nan gradient found. The current loss is:  0.28441959619522095\n",
      "Warning: nan gradient found. The current loss is:  0.4730280041694641\n",
      "Warning: nan gradient found. The current loss is:  0.4913240373134613\n",
      "Warning: nan gradient found. The current loss is:  0.5514271855354309\n",
      "Warning: nan gradient found. The current loss is:  0.7326737642288208\n",
      "Warning: nan gradient found. The current loss is:  0.5706557631492615\n",
      "Warning: nan gradient found. The current loss is:  0.9405488967895508\n",
      "Warning: nan gradient found. The current loss is:  0.6075762510299683\n",
      "Warning: nan gradient found. The current loss is:  0.20321232080459595\n",
      "Warning: nan gradient found. The current loss is:  0.16142091155052185\n",
      "Warning: nan gradient found. The current loss is:  0.5052785873413086\n",
      "Warning: nan gradient found. The current loss is:  0.2782236635684967\n",
      "Warning: nan gradient found. The current loss is:  0.362498939037323\n",
      "Warning: nan gradient found. The current loss is:  0.39991652965545654\n",
      "Warning: nan gradient found. The current loss is:  0.37986162304878235\n",
      "Warning: nan gradient found. The current loss is:  0.53218674659729\n",
      "Warning: nan gradient found. The current loss is:  0.07348648458719254\n",
      "Warning: nan gradient found. The current loss is:  0.30396726727485657\n",
      "Warning: nan gradient found. The current loss is:  0.314493864774704\n",
      "Warning: nan gradient found. The current loss is:  0.06242635101079941\n",
      "Warning: nan gradient found. The current loss is:  0.6576089859008789\n",
      "Warning: nan gradient found. The current loss is:  1.6328283548355103\n",
      "Warning: nan gradient found. The current loss is:  0.5949874520301819\n",
      "Warning: nan gradient found. The current loss is:  0.11940397322177887\n",
      "Warning: nan gradient found. The current loss is:  0.3676486611366272\n",
      "Warning: nan gradient found. The current loss is:  0.0705835297703743\n",
      "Warning: nan gradient found. The current loss is:  0.6588013172149658\n",
      "Warning: nan gradient found. The current loss is:  0.10743571072816849\n",
      "Warning: nan gradient found. The current loss is:  1.2022249698638916\n",
      "Warning: nan gradient found. The current loss is:  1.237825632095337\n",
      "Warning: nan gradient found. The current loss is:  0.5143072009086609\n",
      "Warning: nan gradient found. The current loss is:  1.415067195892334\n",
      "Warning: nan gradient found. The current loss is:  1.2281270027160645\n",
      "Warning: nan gradient found. The current loss is:  0.34757199883461\n",
      "Warning: nan gradient found. The current loss is:  0.6889117360115051\n",
      "Warning: nan gradient found. The current loss is:  0.7654693126678467\n",
      "Warning: nan gradient found. The current loss is:  0.22960463166236877\n",
      "Current batch training loss: 0.229605  [1689600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.24576464295387268\n",
      "Warning: nan gradient found. The current loss is:  0.7258714437484741\n",
      "Warning: nan gradient found. The current loss is:  0.403461754322052\n",
      "Warning: nan gradient found. The current loss is:  0.5598849654197693\n",
      "Warning: nan gradient found. The current loss is:  0.48689502477645874\n",
      "Warning: nan gradient found. The current loss is:  0.5392547249794006\n",
      "Warning: nan gradient found. The current loss is:  0.28210586309432983\n",
      "Warning: nan gradient found. The current loss is:  0.35759812593460083\n",
      "Warning: nan gradient found. The current loss is:  0.45927929878234863\n",
      "Warning: nan gradient found. The current loss is:  0.341910719871521\n",
      "Warning: nan gradient found. The current loss is:  0.3664342761039734\n",
      "Warning: nan gradient found. The current loss is:  0.6428226232528687\n",
      "Warning: nan gradient found. The current loss is:  0.605195164680481\n",
      "Warning: nan gradient found. The current loss is:  0.293876051902771\n",
      "Warning: nan gradient found. The current loss is:  0.4818381667137146\n",
      "Warning: nan gradient found. The current loss is:  0.09566039592027664\n",
      "Warning: nan gradient found. The current loss is:  0.3587792217731476\n",
      "Warning: nan gradient found. The current loss is:  1.3033137321472168\n",
      "Warning: nan gradient found. The current loss is:  0.2214500606060028\n",
      "Warning: nan gradient found. The current loss is:  0.7119237184524536\n",
      "Warning: nan gradient found. The current loss is:  0.2157852202653885\n",
      "Warning: nan gradient found. The current loss is:  0.12770143151283264\n",
      "Warning: nan gradient found. The current loss is:  0.29992562532424927\n",
      "Warning: nan gradient found. The current loss is:  0.3641151189804077\n",
      "Warning: nan gradient found. The current loss is:  0.2527811825275421\n",
      "Warning: nan gradient found. The current loss is:  0.5783225297927856\n",
      "Warning: nan gradient found. The current loss is:  0.27407339215278625\n",
      "Warning: nan gradient found. The current loss is:  0.35858404636383057\n",
      "Warning: nan gradient found. The current loss is:  0.6889188885688782\n",
      "Warning: nan gradient found. The current loss is:  0.15196624398231506\n",
      "Warning: nan gradient found. The current loss is:  0.6543511152267456\n",
      "Warning: nan gradient found. The current loss is:  0.32333171367645264\n",
      "Warning: nan gradient found. The current loss is:  1.0998446941375732\n",
      "Warning: nan gradient found. The current loss is:  0.1361195147037506\n",
      "Warning: nan gradient found. The current loss is:  0.5080156326293945\n",
      "Warning: nan gradient found. The current loss is:  0.22667264938354492\n",
      "Warning: nan gradient found. The current loss is:  0.5733799338340759\n",
      "Warning: nan gradient found. The current loss is:  0.8616502285003662\n",
      "Warning: nan gradient found. The current loss is:  0.8252618312835693\n",
      "Warning: nan gradient found. The current loss is:  0.9099425673484802\n",
      "Warning: nan gradient found. The current loss is:  0.7431999444961548\n",
      "Warning: nan gradient found. The current loss is:  0.08337809145450592\n",
      "Warning: nan gradient found. The current loss is:  0.400459349155426\n",
      "Warning: nan gradient found. The current loss is:  0.30206233263015747\n",
      "Warning: nan gradient found. The current loss is:  0.9032115936279297\n",
      "Warning: nan gradient found. The current loss is:  0.14929349720478058\n",
      "Warning: nan gradient found. The current loss is:  0.3690648674964905\n",
      "Warning: nan gradient found. The current loss is:  0.5416910648345947\n",
      "Warning: nan gradient found. The current loss is:  0.11785369366407394\n",
      "Warning: nan gradient found. The current loss is:  0.20972475409507751\n",
      "Warning: nan gradient found. The current loss is:  0.7359890937805176\n",
      "Warning: nan gradient found. The current loss is:  0.3914385437965393\n",
      "Warning: nan gradient found. The current loss is:  0.6722257137298584\n",
      "Warning: nan gradient found. The current loss is:  0.7855542898178101\n",
      "Warning: nan gradient found. The current loss is:  0.9182167053222656\n",
      "Warning: nan gradient found. The current loss is:  0.7143231630325317\n",
      "Warning: nan gradient found. The current loss is:  0.37161898612976074\n",
      "Warning: nan gradient found. The current loss is:  0.8204875588417053\n",
      "Warning: nan gradient found. The current loss is:  0.522179901599884\n",
      "Warning: nan gradient found. The current loss is:  0.1383095681667328\n",
      "Warning: nan gradient found. The current loss is:  1.0822135210037231\n",
      "Warning: nan gradient found. The current loss is:  0.7945591807365417\n",
      "Warning: nan gradient found. The current loss is:  0.6864694356918335\n",
      "Warning: nan gradient found. The current loss is:  0.33587852120399475\n",
      "Warning: nan gradient found. The current loss is:  0.5603027939796448\n",
      "Warning: nan gradient found. The current loss is:  0.6946249604225159\n",
      "Warning: nan gradient found. The current loss is:  0.4654124975204468\n",
      "Warning: nan gradient found. The current loss is:  0.4432441294193268\n",
      "Warning: nan gradient found. The current loss is:  0.320860356092453\n",
      "Warning: nan gradient found. The current loss is:  0.15200696885585785\n",
      "Warning: nan gradient found. The current loss is:  0.5861507654190063\n",
      "Warning: nan gradient found. The current loss is:  0.4579758644104004\n",
      "Warning: nan gradient found. The current loss is:  0.30933254957199097\n",
      "Warning: nan gradient found. The current loss is:  0.5655670166015625\n",
      "Warning: nan gradient found. The current loss is:  0.6648449301719666\n",
      "Warning: nan gradient found. The current loss is:  0.3592899441719055\n",
      "Warning: nan gradient found. The current loss is:  0.5323951840400696\n",
      "Warning: nan gradient found. The current loss is:  0.727960467338562\n",
      "Warning: nan gradient found. The current loss is:  0.3727473020553589\n",
      "Warning: nan gradient found. The current loss is:  2.0167574882507324\n",
      "Warning: nan gradient found. The current loss is:  0.27411139011383057\n",
      "Warning: nan gradient found. The current loss is:  0.5557574033737183\n",
      "Warning: nan gradient found. The current loss is:  0.552797257900238\n",
      "Warning: nan gradient found. The current loss is:  0.8328049182891846\n",
      "Warning: nan gradient found. The current loss is:  0.4030439555644989\n",
      "Warning: nan gradient found. The current loss is:  0.9108107089996338\n",
      "Warning: nan gradient found. The current loss is:  0.3592672646045685\n",
      "Warning: nan gradient found. The current loss is:  0.33585211634635925\n",
      "Warning: nan gradient found. The current loss is:  0.6318917274475098\n",
      "Warning: nan gradient found. The current loss is:  0.45209890604019165\n",
      "Warning: nan gradient found. The current loss is:  0.2620495557785034\n",
      "Warning: nan gradient found. The current loss is:  0.24350614845752716\n",
      "Warning: nan gradient found. The current loss is:  0.2868765890598297\n",
      "Warning: nan gradient found. The current loss is:  0.41244590282440186\n",
      "Warning: nan gradient found. The current loss is:  0.49119511246681213\n",
      "Warning: nan gradient found. The current loss is:  0.07560857385396957\n",
      "Warning: nan gradient found. The current loss is:  0.7185601592063904\n",
      "Warning: nan gradient found. The current loss is:  0.5617821216583252\n",
      "Warning: nan gradient found. The current loss is:  0.4414920508861542\n",
      "Warning: nan gradient found. The current loss is:  0.3913325071334839\n",
      "Current batch training loss: 0.391333  [1715200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.1427295207977295\n",
      "Warning: nan gradient found. The current loss is:  0.293145090341568\n",
      "Warning: nan gradient found. The current loss is:  0.3640453815460205\n",
      "Warning: nan gradient found. The current loss is:  0.7676663994789124\n",
      "Warning: nan gradient found. The current loss is:  0.25861093401908875\n",
      "Warning: nan gradient found. The current loss is:  0.5296009182929993\n",
      "Warning: nan gradient found. The current loss is:  0.512244701385498\n",
      "Warning: nan gradient found. The current loss is:  0.552678108215332\n",
      "Warning: nan gradient found. The current loss is:  0.30866608023643494\n",
      "Warning: nan gradient found. The current loss is:  -0.01092207059264183\n",
      "Warning: nan gradient found. The current loss is:  0.988976001739502\n",
      "Warning: nan gradient found. The current loss is:  0.3305438756942749\n",
      "Warning: nan gradient found. The current loss is:  1.1567639112472534\n",
      "Warning: nan gradient found. The current loss is:  0.8235353231430054\n",
      "Warning: nan gradient found. The current loss is:  0.7573038339614868\n",
      "Warning: nan gradient found. The current loss is:  0.6079872846603394\n",
      "Warning: nan gradient found. The current loss is:  0.4691980481147766\n",
      "Warning: nan gradient found. The current loss is:  0.1396978348493576\n",
      "Warning: nan gradient found. The current loss is:  0.6345939040184021\n",
      "Warning: nan gradient found. The current loss is:  0.6912143230438232\n",
      "Warning: nan gradient found. The current loss is:  0.42374104261398315\n",
      "Warning: nan gradient found. The current loss is:  0.7743922472000122\n",
      "Warning: nan gradient found. The current loss is:  0.3461461663246155\n",
      "Warning: nan gradient found. The current loss is:  0.307625412940979\n",
      "Warning: nan gradient found. The current loss is:  0.2198646366596222\n",
      "Warning: nan gradient found. The current loss is:  0.6292532086372375\n",
      "Warning: nan gradient found. The current loss is:  0.6065581440925598\n",
      "Warning: nan gradient found. The current loss is:  0.1693548560142517\n",
      "Warning: nan gradient found. The current loss is:  0.881473183631897\n",
      "Warning: nan gradient found. The current loss is:  0.5986875891685486\n",
      "Warning: nan gradient found. The current loss is:  0.5249671936035156\n",
      "Warning: nan gradient found. The current loss is:  0.9011226892471313\n",
      "Warning: nan gradient found. The current loss is:  0.4035717248916626\n",
      "Warning: nan gradient found. The current loss is:  0.4328489899635315\n",
      "Warning: nan gradient found. The current loss is:  1.1048798561096191\n",
      "Warning: nan gradient found. The current loss is:  0.23303477466106415\n",
      "Warning: nan gradient found. The current loss is:  0.6797314882278442\n",
      "Warning: nan gradient found. The current loss is:  0.2977336645126343\n",
      "Warning: nan gradient found. The current loss is:  0.911160945892334\n",
      "Warning: nan gradient found. The current loss is:  0.41425201296806335\n",
      "Warning: nan gradient found. The current loss is:  0.06356822699308395\n",
      "Warning: nan gradient found. The current loss is:  0.8247721791267395\n",
      "Warning: nan gradient found. The current loss is:  0.45897117257118225\n",
      "Warning: nan gradient found. The current loss is:  0.39993196725845337\n",
      "Warning: nan gradient found. The current loss is:  0.33573827147483826\n",
      "Warning: nan gradient found. The current loss is:  0.39598608016967773\n",
      "Warning: nan gradient found. The current loss is:  0.7857878804206848\n",
      "Warning: nan gradient found. The current loss is:  1.1838139295578003\n",
      "Warning: nan gradient found. The current loss is:  1.138338327407837\n",
      "Warning: nan gradient found. The current loss is:  0.8387770652770996\n",
      "Warning: nan gradient found. The current loss is:  0.38981392979621887\n",
      "Warning: nan gradient found. The current loss is:  0.19601087272167206\n",
      "Warning: nan gradient found. The current loss is:  0.2813207805156708\n",
      "Warning: nan gradient found. The current loss is:  0.8559857606887817\n",
      "Warning: nan gradient found. The current loss is:  0.3844100534915924\n",
      "Warning: nan gradient found. The current loss is:  0.29793477058410645\n",
      "Warning: nan gradient found. The current loss is:  0.4455603063106537\n",
      "Warning: nan gradient found. The current loss is:  1.317787528038025\n",
      "Warning: nan gradient found. The current loss is:  0.533288300037384\n",
      "Warning: nan gradient found. The current loss is:  0.9601199626922607\n",
      "Warning: nan gradient found. The current loss is:  0.9911485910415649\n",
      "Warning: nan gradient found. The current loss is:  0.5985703468322754\n",
      "Warning: nan gradient found. The current loss is:  0.3687979578971863\n",
      "Warning: nan gradient found. The current loss is:  0.11179995536804199\n",
      "Warning: nan gradient found. The current loss is:  0.6109364032745361\n",
      "Warning: nan gradient found. The current loss is:  1.3935203552246094\n",
      "Warning: nan gradient found. The current loss is:  0.22035440802574158\n",
      "Warning: nan gradient found. The current loss is:  0.08785854279994965\n",
      "Warning: nan gradient found. The current loss is:  0.6441525220870972\n",
      "Warning: nan gradient found. The current loss is:  0.95756995677948\n",
      "Warning: nan gradient found. The current loss is:  0.29572996497154236\n",
      "Warning: nan gradient found. The current loss is:  0.44859132170677185\n",
      "Warning: nan gradient found. The current loss is:  0.6191521286964417\n",
      "Warning: nan gradient found. The current loss is:  0.521436333656311\n",
      "Warning: nan gradient found. The current loss is:  1.9966626167297363\n",
      "Warning: nan gradient found. The current loss is:  0.4807565212249756\n",
      "Warning: nan gradient found. The current loss is:  0.5774290561676025\n",
      "Warning: nan gradient found. The current loss is:  0.20692680776119232\n",
      "Warning: nan gradient found. The current loss is:  1.431694746017456\n",
      "Warning: nan gradient found. The current loss is:  0.8267449140548706\n",
      "Warning: nan gradient found. The current loss is:  0.11734350025653839\n",
      "Warning: nan gradient found. The current loss is:  0.34917163848876953\n",
      "Warning: nan gradient found. The current loss is:  0.4161270260810852\n",
      "Warning: nan gradient found. The current loss is:  0.7592795491218567\n",
      "Warning: nan gradient found. The current loss is:  0.22443300485610962\n",
      "Warning: nan gradient found. The current loss is:  0.23469017446041107\n",
      "Warning: nan gradient found. The current loss is:  0.7868056893348694\n",
      "Warning: nan gradient found. The current loss is:  0.3483434319496155\n",
      "Warning: nan gradient found. The current loss is:  0.15284064412117004\n",
      "Warning: nan gradient found. The current loss is:  0.8010586500167847\n",
      "Warning: nan gradient found. The current loss is:  0.5693424344062805\n",
      "Warning: nan gradient found. The current loss is:  0.2667461931705475\n",
      "Warning: nan gradient found. The current loss is:  0.5457412600517273\n",
      "Warning: nan gradient found. The current loss is:  0.3925643265247345\n",
      "Warning: nan gradient found. The current loss is:  0.7981680035591125\n",
      "Warning: nan gradient found. The current loss is:  0.2898966073989868\n",
      "Warning: nan gradient found. The current loss is:  1.4149103164672852\n",
      "Warning: nan gradient found. The current loss is:  0.3183925449848175\n",
      "Warning: nan gradient found. The current loss is:  0.4469732642173767\n",
      "Warning: nan gradient found. The current loss is:  0.1554137021303177\n",
      "Current batch training loss: 0.155414  [1740800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5491529703140259\n",
      "Warning: nan gradient found. The current loss is:  0.35871464014053345\n",
      "Warning: nan gradient found. The current loss is:  0.34128257632255554\n",
      "Warning: nan gradient found. The current loss is:  0.786520779132843\n",
      "Warning: nan gradient found. The current loss is:  0.8626211285591125\n",
      "Warning: nan gradient found. The current loss is:  0.5164836645126343\n",
      "Warning: nan gradient found. The current loss is:  1.6498667001724243\n",
      "Warning: nan gradient found. The current loss is:  1.2087088823318481\n",
      "Warning: nan gradient found. The current loss is:  0.7500449419021606\n",
      "Warning: nan gradient found. The current loss is:  0.9899554252624512\n",
      "Warning: nan gradient found. The current loss is:  0.3451218008995056\n",
      "Warning: nan gradient found. The current loss is:  0.5769205093383789\n",
      "Warning: nan gradient found. The current loss is:  0.6757724285125732\n",
      "Warning: nan gradient found. The current loss is:  0.42263275384902954\n",
      "Warning: nan gradient found. The current loss is:  0.26096516847610474\n",
      "Warning: nan gradient found. The current loss is:  0.6023063659667969\n",
      "Warning: nan gradient found. The current loss is:  0.7854852676391602\n",
      "Warning: nan gradient found. The current loss is:  0.5255619287490845\n",
      "Warning: nan gradient found. The current loss is:  0.3665722906589508\n",
      "Warning: nan gradient found. The current loss is:  0.43310272693634033\n",
      "Warning: nan gradient found. The current loss is:  0.24719510972499847\n",
      "Warning: nan gradient found. The current loss is:  0.8231640458106995\n",
      "Warning: nan gradient found. The current loss is:  0.8430815935134888\n",
      "Warning: nan gradient found. The current loss is:  0.4635251462459564\n",
      "Warning: nan gradient found. The current loss is:  0.4124848246574402\n",
      "Warning: nan gradient found. The current loss is:  0.9596346616744995\n",
      "Warning: nan gradient found. The current loss is:  0.6755425930023193\n",
      "Warning: nan gradient found. The current loss is:  0.48599281907081604\n",
      "Warning: nan gradient found. The current loss is:  0.4980906844139099\n",
      "Warning: nan gradient found. The current loss is:  0.7174860239028931\n",
      "Warning: nan gradient found. The current loss is:  0.29149436950683594\n",
      "Warning: nan gradient found. The current loss is:  0.3145638108253479\n",
      "Warning: nan gradient found. The current loss is:  0.31345492601394653\n",
      "Warning: nan gradient found. The current loss is:  0.2649402618408203\n",
      "Warning: nan gradient found. The current loss is:  -0.027303308248519897\n",
      "Warning: nan gradient found. The current loss is:  0.8378167152404785\n",
      "Warning: nan gradient found. The current loss is:  1.233991265296936\n",
      "Warning: nan gradient found. The current loss is:  0.7725740671157837\n",
      "Warning: nan gradient found. The current loss is:  0.19715015590190887\n",
      "Warning: nan gradient found. The current loss is:  0.6633990406990051\n",
      "Warning: nan gradient found. The current loss is:  0.35182827711105347\n",
      "Warning: nan gradient found. The current loss is:  0.5102627873420715\n",
      "Warning: nan gradient found. The current loss is:  0.26930493116378784\n",
      "Warning: nan gradient found. The current loss is:  0.32164108753204346\n",
      "Warning: nan gradient found. The current loss is:  0.5911332368850708\n",
      "Warning: nan gradient found. The current loss is:  0.8710271120071411\n",
      "Warning: nan gradient found. The current loss is:  0.5881582498550415\n",
      "Warning: nan gradient found. The current loss is:  0.5523278713226318\n",
      "Warning: nan gradient found. The current loss is:  1.2074315547943115\n",
      "Warning: nan gradient found. The current loss is:  0.62152498960495\n",
      "Warning: nan gradient found. The current loss is:  0.2699166536331177\n",
      "Warning: nan gradient found. The current loss is:  0.031038878485560417\n",
      "Warning: nan gradient found. The current loss is:  0.6763361096382141\n",
      "Warning: nan gradient found. The current loss is:  0.6045898199081421\n",
      "Warning: nan gradient found. The current loss is:  1.5840076208114624\n",
      "Warning: nan gradient found. The current loss is:  0.4336349368095398\n",
      "Warning: nan gradient found. The current loss is:  0.7895433902740479\n",
      "Warning: nan gradient found. The current loss is:  1.1168525218963623\n",
      "Warning: nan gradient found. The current loss is:  0.8998656272888184\n",
      "Warning: nan gradient found. The current loss is:  0.9001721143722534\n",
      "Warning: nan gradient found. The current loss is:  0.2191535383462906\n",
      "Warning: nan gradient found. The current loss is:  0.4959719777107239\n",
      "Warning: nan gradient found. The current loss is:  0.5000235438346863\n",
      "Warning: nan gradient found. The current loss is:  0.7986716628074646\n",
      "Warning: nan gradient found. The current loss is:  0.32522979378700256\n",
      "Warning: nan gradient found. The current loss is:  1.086472511291504\n",
      "Warning: nan gradient found. The current loss is:  0.8646852970123291\n",
      "Warning: nan gradient found. The current loss is:  0.31355613470077515\n",
      "Warning: nan gradient found. The current loss is:  0.6598803997039795\n",
      "Warning: nan gradient found. The current loss is:  0.6008938550949097\n",
      "Warning: nan gradient found. The current loss is:  0.7621601819992065\n",
      "Warning: nan gradient found. The current loss is:  0.7660740613937378\n",
      "Warning: nan gradient found. The current loss is:  0.8021913766860962\n",
      "Warning: nan gradient found. The current loss is:  0.9443683624267578\n",
      "Warning: nan gradient found. The current loss is:  0.1444069743156433\n",
      "Warning: nan gradient found. The current loss is:  0.3488842844963074\n",
      "Warning: nan gradient found. The current loss is:  -0.07007379829883575\n",
      "Warning: nan gradient found. The current loss is:  0.4479503035545349\n",
      "Warning: nan gradient found. The current loss is:  0.7588624954223633\n",
      "Warning: nan gradient found. The current loss is:  0.27328670024871826\n",
      "Warning: nan gradient found. The current loss is:  0.7201675772666931\n",
      "Warning: nan gradient found. The current loss is:  0.5662186741828918\n",
      "Warning: nan gradient found. The current loss is:  0.33619424700737\n",
      "Warning: nan gradient found. The current loss is:  0.2949453294277191\n",
      "Warning: nan gradient found. The current loss is:  0.19863948225975037\n",
      "Warning: nan gradient found. The current loss is:  0.2249574363231659\n",
      "Warning: nan gradient found. The current loss is:  0.7817246913909912\n",
      "Warning: nan gradient found. The current loss is:  0.6701599359512329\n",
      "Warning: nan gradient found. The current loss is:  0.41277849674224854\n",
      "Warning: nan gradient found. The current loss is:  0.11503538489341736\n",
      "Warning: nan gradient found. The current loss is:  0.9392236471176147\n",
      "Warning: nan gradient found. The current loss is:  0.34765487909317017\n",
      "Warning: nan gradient found. The current loss is:  0.29542943835258484\n",
      "Warning: nan gradient found. The current loss is:  0.29619064927101135\n",
      "Warning: nan gradient found. The current loss is:  0.44638991355895996\n",
      "Warning: nan gradient found. The current loss is:  0.8662508130073547\n",
      "Warning: nan gradient found. The current loss is:  0.03930680453777313\n",
      "Warning: nan gradient found. The current loss is:  1.149181604385376\n",
      "Warning: nan gradient found. The current loss is:  -0.06574009358882904\n",
      "Warning: nan gradient found. The current loss is:  0.43058544397354126\n",
      "Current batch training loss: 0.430585  [1766400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9819346070289612\n",
      "Warning: nan gradient found. The current loss is:  0.6790511608123779\n",
      "Warning: nan gradient found. The current loss is:  0.5618644952774048\n",
      "Warning: nan gradient found. The current loss is:  0.2790093719959259\n",
      "Warning: nan gradient found. The current loss is:  0.5134461522102356\n",
      "Warning: nan gradient found. The current loss is:  0.3895150125026703\n",
      "Warning: nan gradient found. The current loss is:  0.13568899035453796\n",
      "Warning: nan gradient found. The current loss is:  0.3395364284515381\n",
      "Warning: nan gradient found. The current loss is:  0.40593206882476807\n",
      "Warning: nan gradient found. The current loss is:  0.30903422832489014\n",
      "Warning: nan gradient found. The current loss is:  0.39003145694732666\n",
      "Warning: nan gradient found. The current loss is:  0.5657809376716614\n",
      "Warning: nan gradient found. The current loss is:  0.7062283754348755\n",
      "Warning: nan gradient found. The current loss is:  0.47942182421684265\n",
      "Warning: nan gradient found. The current loss is:  0.3015875816345215\n",
      "Warning: nan gradient found. The current loss is:  1.1159716844558716\n",
      "Warning: nan gradient found. The current loss is:  1.35699462890625\n",
      "Warning: nan gradient found. The current loss is:  0.37295371294021606\n",
      "Warning: nan gradient found. The current loss is:  0.6775407791137695\n",
      "Warning: nan gradient found. The current loss is:  0.5190140604972839\n",
      "Warning: nan gradient found. The current loss is:  0.49023011326789856\n",
      "Warning: nan gradient found. The current loss is:  0.5047010183334351\n",
      "Warning: nan gradient found. The current loss is:  0.8975460529327393\n",
      "Warning: nan gradient found. The current loss is:  0.6281973123550415\n",
      "Warning: nan gradient found. The current loss is:  0.8767117261886597\n",
      "Warning: nan gradient found. The current loss is:  0.1383865475654602\n",
      "Warning: nan gradient found. The current loss is:  0.2606056332588196\n",
      "Warning: nan gradient found. The current loss is:  0.6359542012214661\n",
      "Warning: nan gradient found. The current loss is:  1.2658871412277222\n",
      "Warning: nan gradient found. The current loss is:  0.43971753120422363\n",
      "Warning: nan gradient found. The current loss is:  0.314799040555954\n",
      "Warning: nan gradient found. The current loss is:  0.5186218619346619\n",
      "Warning: nan gradient found. The current loss is:  0.8728451132774353\n",
      "Warning: nan gradient found. The current loss is:  0.45606136322021484\n",
      "Warning: nan gradient found. The current loss is:  0.6356531381607056\n",
      "Warning: nan gradient found. The current loss is:  0.6414861083030701\n",
      "Warning: nan gradient found. The current loss is:  0.13848349452018738\n",
      "Warning: nan gradient found. The current loss is:  0.5884788036346436\n",
      "Warning: nan gradient found. The current loss is:  0.7190090417861938\n",
      "Warning: nan gradient found. The current loss is:  0.825126051902771\n",
      "Warning: nan gradient found. The current loss is:  0.36628568172454834\n",
      "Warning: nan gradient found. The current loss is:  0.6159135699272156\n",
      "Warning: nan gradient found. The current loss is:  0.5388749241828918\n",
      "Warning: nan gradient found. The current loss is:  0.35383307933807373\n",
      "Warning: nan gradient found. The current loss is:  0.3702145516872406\n",
      "Warning: nan gradient found. The current loss is:  0.5080724954605103\n",
      "Warning: nan gradient found. The current loss is:  0.6265528798103333\n",
      "Warning: nan gradient found. The current loss is:  1.0107414722442627\n",
      "Warning: nan gradient found. The current loss is:  1.243851900100708\n",
      "Warning: nan gradient found. The current loss is:  0.32684600353240967\n",
      "Warning: nan gradient found. The current loss is:  0.3925820291042328\n",
      "Warning: nan gradient found. The current loss is:  0.4882095456123352\n",
      "Warning: nan gradient found. The current loss is:  0.9765270948410034\n",
      "Warning: nan gradient found. The current loss is:  0.28702011704444885\n",
      "Warning: nan gradient found. The current loss is:  0.44105374813079834\n",
      "Warning: nan gradient found. The current loss is:  0.5190659165382385\n",
      "Warning: nan gradient found. The current loss is:  0.554674506187439\n",
      "Warning: nan gradient found. The current loss is:  0.27028295397758484\n",
      "Warning: nan gradient found. The current loss is:  0.22456206381320953\n",
      "Warning: nan gradient found. The current loss is:  0.6209347248077393\n",
      "Warning: nan gradient found. The current loss is:  0.9420568943023682\n",
      "Warning: nan gradient found. The current loss is:  0.4552738070487976\n",
      "Warning: nan gradient found. The current loss is:  0.42130300402641296\n",
      "Warning: nan gradient found. The current loss is:  0.24859154224395752\n",
      "Warning: nan gradient found. The current loss is:  0.3995417058467865\n",
      "Warning: nan gradient found. The current loss is:  0.05587516725063324\n",
      "Warning: nan gradient found. The current loss is:  0.2913208603858948\n",
      "Warning: nan gradient found. The current loss is:  0.6958240866661072\n",
      "Warning: nan gradient found. The current loss is:  0.6409271955490112\n",
      "Warning: nan gradient found. The current loss is:  0.42828860878944397\n",
      "Warning: nan gradient found. The current loss is:  0.20281058549880981\n",
      "Warning: nan gradient found. The current loss is:  0.34126704931259155\n",
      "Warning: nan gradient found. The current loss is:  1.6339614391326904\n",
      "Warning: nan gradient found. The current loss is:  0.16067823767662048\n",
      "Warning: nan gradient found. The current loss is:  0.24987949430942535\n",
      "Warning: nan gradient found. The current loss is:  0.4113404154777527\n",
      "Warning: nan gradient found. The current loss is:  0.5656326413154602\n",
      "Warning: nan gradient found. The current loss is:  0.400727242231369\n",
      "Warning: nan gradient found. The current loss is:  0.6796477437019348\n",
      "Warning: nan gradient found. The current loss is:  1.502885103225708\n",
      "Warning: nan gradient found. The current loss is:  0.5195650458335876\n",
      "Warning: nan gradient found. The current loss is:  1.5858091115951538\n",
      "Warning: nan gradient found. The current loss is:  0.08503517508506775\n",
      "Warning: nan gradient found. The current loss is:  0.7815061807632446\n",
      "Warning: nan gradient found. The current loss is:  0.5265669822692871\n",
      "Warning: nan gradient found. The current loss is:  0.3173971474170685\n",
      "Warning: nan gradient found. The current loss is:  0.8163803815841675\n",
      "Warning: nan gradient found. The current loss is:  0.22395384311676025\n",
      "Warning: nan gradient found. The current loss is:  0.19442643225193024\n",
      "Warning: nan gradient found. The current loss is:  0.5304230451583862\n",
      "Warning: nan gradient found. The current loss is:  0.4676053524017334\n",
      "Warning: nan gradient found. The current loss is:  0.47068601846694946\n",
      "Warning: nan gradient found. The current loss is:  0.11770497262477875\n",
      "Warning: nan gradient found. The current loss is:  0.03830677270889282\n",
      "Warning: nan gradient found. The current loss is:  0.7275188565254211\n",
      "Warning: nan gradient found. The current loss is:  0.9785013794898987\n",
      "Warning: nan gradient found. The current loss is:  0.42834967374801636\n",
      "Warning: nan gradient found. The current loss is:  0.2880650758743286\n",
      "Warning: nan gradient found. The current loss is:  0.4590213894844055\n",
      "Warning: nan gradient found. The current loss is:  0.4952896535396576\n",
      "Current batch training loss: 0.495290  [1792000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.1582406759262085\n",
      "Warning: nan gradient found. The current loss is:  0.089093416929245\n",
      "Warning: nan gradient found. The current loss is:  0.7896917462348938\n",
      "Warning: nan gradient found. The current loss is:  0.6446229219436646\n",
      "Warning: nan gradient found. The current loss is:  0.4901893436908722\n",
      "Warning: nan gradient found. The current loss is:  0.4204419255256653\n",
      "Warning: nan gradient found. The current loss is:  0.6461927890777588\n",
      "Warning: nan gradient found. The current loss is:  0.7921727299690247\n",
      "Warning: nan gradient found. The current loss is:  0.5209106802940369\n",
      "Warning: nan gradient found. The current loss is:  0.2804928123950958\n",
      "Warning: nan gradient found. The current loss is:  0.3796460032463074\n",
      "Warning: nan gradient found. The current loss is:  0.25227224826812744\n",
      "Warning: nan gradient found. The current loss is:  0.508710503578186\n",
      "Warning: nan gradient found. The current loss is:  0.4937732219696045\n",
      "Warning: nan gradient found. The current loss is:  0.48784905672073364\n",
      "Warning: nan gradient found. The current loss is:  0.14245113730430603\n",
      "Warning: nan gradient found. The current loss is:  0.43272513151168823\n",
      "Warning: nan gradient found. The current loss is:  0.7714074850082397\n",
      "Warning: nan gradient found. The current loss is:  0.5039562582969666\n",
      "Warning: nan gradient found. The current loss is:  0.3675615191459656\n",
      "Warning: nan gradient found. The current loss is:  0.17891395092010498\n",
      "Warning: nan gradient found. The current loss is:  0.10422247648239136\n",
      "Warning: nan gradient found. The current loss is:  0.47597867250442505\n",
      "Warning: nan gradient found. The current loss is:  1.2750104665756226\n",
      "Warning: nan gradient found. The current loss is:  0.19593766331672668\n",
      "Warning: nan gradient found. The current loss is:  0.5104123950004578\n",
      "Warning: nan gradient found. The current loss is:  0.540958046913147\n",
      "Warning: nan gradient found. The current loss is:  0.48164859414100647\n",
      "Warning: nan gradient found. The current loss is:  0.32185477018356323\n",
      "Warning: nan gradient found. The current loss is:  0.3826335370540619\n",
      "Warning: nan gradient found. The current loss is:  0.08713239431381226\n",
      "Warning: nan gradient found. The current loss is:  0.9384893178939819\n",
      "Warning: nan gradient found. The current loss is:  0.2825692892074585\n",
      "Warning: nan gradient found. The current loss is:  0.786498486995697\n",
      "Warning: nan gradient found. The current loss is:  0.6579651236534119\n",
      "Warning: nan gradient found. The current loss is:  0.4679144024848938\n",
      "Warning: nan gradient found. The current loss is:  1.6135036945343018\n",
      "Warning: nan gradient found. The current loss is:  0.042366430163383484\n",
      "Warning: nan gradient found. The current loss is:  0.6285163164138794\n",
      "Warning: nan gradient found. The current loss is:  0.4204004406929016\n",
      "Warning: nan gradient found. The current loss is:  0.34862783551216125\n",
      "Warning: nan gradient found. The current loss is:  0.5694848299026489\n",
      "Warning: nan gradient found. The current loss is:  1.4546737670898438\n",
      "Warning: nan gradient found. The current loss is:  0.63116055727005\n",
      "Warning: nan gradient found. The current loss is:  0.785915732383728\n",
      "Warning: nan gradient found. The current loss is:  0.6836336851119995\n",
      "Warning: nan gradient found. The current loss is:  1.4400107860565186\n",
      "Warning: nan gradient found. The current loss is:  0.7001608610153198\n",
      "Warning: nan gradient found. The current loss is:  0.442505419254303\n",
      "Warning: nan gradient found. The current loss is:  1.2397888898849487\n",
      "Warning: nan gradient found. The current loss is:  0.6053357720375061\n",
      "Warning: nan gradient found. The current loss is:  0.5621843338012695\n",
      "Warning: nan gradient found. The current loss is:  0.33310115337371826\n",
      "Warning: nan gradient found. The current loss is:  0.5102742910385132\n",
      "Warning: nan gradient found. The current loss is:  0.5297258496284485\n",
      "Warning: nan gradient found. The current loss is:  0.4219931662082672\n",
      "Warning: nan gradient found. The current loss is:  0.5652246475219727\n",
      "Warning: nan gradient found. The current loss is:  0.49929261207580566\n",
      "Warning: nan gradient found. The current loss is:  0.055829983204603195\n",
      "Warning: nan gradient found. The current loss is:  0.19074758887290955\n",
      "Warning: nan gradient found. The current loss is:  1.1326348781585693\n",
      "Warning: nan gradient found. The current loss is:  0.4057598114013672\n",
      "Warning: nan gradient found. The current loss is:  0.8478269577026367\n",
      "Warning: nan gradient found. The current loss is:  0.5696971416473389\n",
      "Warning: nan gradient found. The current loss is:  0.47848281264305115\n",
      "Warning: nan gradient found. The current loss is:  0.8030701875686646\n",
      "Warning: nan gradient found. The current loss is:  0.5874361991882324\n",
      "Warning: nan gradient found. The current loss is:  0.4517000913619995\n",
      "Warning: nan gradient found. The current loss is:  0.5807589292526245\n",
      "Warning: nan gradient found. The current loss is:  1.3395575284957886\n",
      "Warning: nan gradient found. The current loss is:  0.7179515957832336\n",
      "Warning: nan gradient found. The current loss is:  1.2293792963027954\n",
      "Warning: nan gradient found. The current loss is:  0.38671165704727173\n",
      "Warning: nan gradient found. The current loss is:  1.5554232597351074\n",
      "Warning: nan gradient found. The current loss is:  0.6544779539108276\n",
      "Warning: nan gradient found. The current loss is:  0.4295993447303772\n",
      "Warning: nan gradient found. The current loss is:  0.25837644934654236\n",
      "Warning: nan gradient found. The current loss is:  0.7041692137718201\n",
      "Warning: nan gradient found. The current loss is:  0.6553279161453247\n",
      "Warning: nan gradient found. The current loss is:  0.2351226955652237\n",
      "Warning: nan gradient found. The current loss is:  0.2672131359577179\n",
      "Warning: nan gradient found. The current loss is:  0.5317344665527344\n",
      "Warning: nan gradient found. The current loss is:  0.6187953352928162\n",
      "Warning: nan gradient found. The current loss is:  0.40292495489120483\n",
      "Warning: nan gradient found. The current loss is:  1.1731462478637695\n",
      "Warning: nan gradient found. The current loss is:  0.5523209571838379\n",
      "Warning: nan gradient found. The current loss is:  0.9090465903282166\n",
      "Warning: nan gradient found. The current loss is:  0.6521875262260437\n",
      "Warning: nan gradient found. The current loss is:  0.5145485997200012\n",
      "Warning: nan gradient found. The current loss is:  0.8119133710861206\n",
      "Warning: nan gradient found. The current loss is:  0.8261559009552002\n",
      "Warning: nan gradient found. The current loss is:  0.9925901889801025\n",
      "Warning: nan gradient found. The current loss is:  1.832660436630249\n",
      "Warning: nan gradient found. The current loss is:  0.18280330300331116\n",
      "Warning: nan gradient found. The current loss is:  0.40554285049438477\n",
      "Warning: nan gradient found. The current loss is:  0.09267967939376831\n",
      "Warning: nan gradient found. The current loss is:  0.37044626474380493\n",
      "Warning: nan gradient found. The current loss is:  1.5760581493377686\n",
      "Warning: nan gradient found. The current loss is:  0.5864595174789429\n",
      "Warning: nan gradient found. The current loss is:  0.4769349694252014\n",
      "Current batch training loss: 0.476935  [1817600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8463621139526367\n",
      "Warning: nan gradient found. The current loss is:  0.4722870886325836\n",
      "Warning: nan gradient found. The current loss is:  1.1433000564575195\n",
      "Warning: nan gradient found. The current loss is:  0.35461917519569397\n",
      "Warning: nan gradient found. The current loss is:  0.0658070296049118\n",
      "Warning: nan gradient found. The current loss is:  0.25483736395835876\n",
      "Warning: nan gradient found. The current loss is:  1.2291462421417236\n",
      "Warning: nan gradient found. The current loss is:  1.4084365367889404\n",
      "Warning: nan gradient found. The current loss is:  0.6854638457298279\n",
      "Warning: nan gradient found. The current loss is:  0.4992133378982544\n",
      "Warning: nan gradient found. The current loss is:  0.41176703572273254\n",
      "Warning: nan gradient found. The current loss is:  0.2220468521118164\n",
      "Warning: nan gradient found. The current loss is:  0.08554807305335999\n",
      "Warning: nan gradient found. The current loss is:  0.30210739374160767\n",
      "Warning: nan gradient found. The current loss is:  1.1579909324645996\n",
      "Warning: nan gradient found. The current loss is:  1.169081449508667\n",
      "Warning: nan gradient found. The current loss is:  0.5285625457763672\n",
      "Warning: nan gradient found. The current loss is:  0.5147576332092285\n",
      "Warning: nan gradient found. The current loss is:  0.7917447090148926\n",
      "Warning: nan gradient found. The current loss is:  1.2533375024795532\n",
      "Warning: nan gradient found. The current loss is:  0.5569414496421814\n",
      "Warning: nan gradient found. The current loss is:  0.5000367760658264\n",
      "Warning: nan gradient found. The current loss is:  0.36567753553390503\n",
      "Warning: nan gradient found. The current loss is:  0.4738653600215912\n",
      "Warning: nan gradient found. The current loss is:  1.6020011901855469\n",
      "Warning: nan gradient found. The current loss is:  0.22194764018058777\n",
      "Warning: nan gradient found. The current loss is:  0.5821894407272339\n",
      "Warning: nan gradient found. The current loss is:  0.26812002062797546\n",
      "Warning: nan gradient found. The current loss is:  0.5585170984268188\n",
      "Warning: nan gradient found. The current loss is:  0.620743989944458\n",
      "Warning: nan gradient found. The current loss is:  0.4713180661201477\n",
      "Warning: nan gradient found. The current loss is:  0.2854403257369995\n",
      "Warning: nan gradient found. The current loss is:  0.3551069498062134\n",
      "Warning: nan gradient found. The current loss is:  0.5493119955062866\n",
      "Warning: nan gradient found. The current loss is:  1.5355342626571655\n",
      "Warning: nan gradient found. The current loss is:  0.7078835368156433\n",
      "Warning: nan gradient found. The current loss is:  0.5132806301116943\n",
      "Warning: nan gradient found. The current loss is:  0.22089123725891113\n",
      "Warning: nan gradient found. The current loss is:  0.32670077681541443\n",
      "Warning: nan gradient found. The current loss is:  0.30198222398757935\n",
      "Warning: nan gradient found. The current loss is:  0.729991614818573\n",
      "Warning: nan gradient found. The current loss is:  0.6230258941650391\n",
      "Warning: nan gradient found. The current loss is:  0.40630999207496643\n",
      "Warning: nan gradient found. The current loss is:  1.1453849077224731\n",
      "Warning: nan gradient found. The current loss is:  0.64382404088974\n",
      "Warning: nan gradient found. The current loss is:  0.5791628956794739\n",
      "Warning: nan gradient found. The current loss is:  0.31058377027511597\n",
      "Warning: nan gradient found. The current loss is:  0.42439377307891846\n",
      "Warning: nan gradient found. The current loss is:  0.44775429368019104\n",
      "Warning: nan gradient found. The current loss is:  0.06479337811470032\n",
      "Warning: nan gradient found. The current loss is:  0.5207808017730713\n",
      "Warning: nan gradient found. The current loss is:  0.5429621934890747\n",
      "Warning: nan gradient found. The current loss is:  0.7614623308181763\n",
      "Warning: nan gradient found. The current loss is:  0.6810901165008545\n",
      "Warning: nan gradient found. The current loss is:  0.8344904184341431\n",
      "Warning: nan gradient found. The current loss is:  0.3333205580711365\n",
      "Warning: nan gradient found. The current loss is:  0.14886564016342163\n",
      "Warning: nan gradient found. The current loss is:  0.6059008836746216\n",
      "Warning: nan gradient found. The current loss is:  0.5503509044647217\n",
      "Warning: nan gradient found. The current loss is:  0.4519449770450592\n",
      "Warning: nan gradient found. The current loss is:  2.1282577514648438\n",
      "Warning: nan gradient found. The current loss is:  0.23147083818912506\n",
      "Warning: nan gradient found. The current loss is:  0.30339112877845764\n",
      "Warning: nan gradient found. The current loss is:  0.5436297059059143\n",
      "Warning: nan gradient found. The current loss is:  0.29495692253112793\n",
      "Warning: nan gradient found. The current loss is:  0.5669289231300354\n",
      "Warning: nan gradient found. The current loss is:  0.6448755264282227\n",
      "Warning: nan gradient found. The current loss is:  0.6798938512802124\n",
      "Warning: nan gradient found. The current loss is:  0.3532869219779968\n",
      "Warning: nan gradient found. The current loss is:  0.5562340021133423\n",
      "Warning: nan gradient found. The current loss is:  0.5555888414382935\n",
      "Warning: nan gradient found. The current loss is:  0.13306115567684174\n",
      "Warning: nan gradient found. The current loss is:  0.45016902685165405\n",
      "Warning: nan gradient found. The current loss is:  0.3276204466819763\n",
      "Warning: nan gradient found. The current loss is:  0.40744102001190186\n",
      "Warning: nan gradient found. The current loss is:  1.7712700366973877\n",
      "Warning: nan gradient found. The current loss is:  0.6184068322181702\n",
      "Warning: nan gradient found. The current loss is:  0.9103763103485107\n",
      "Warning: nan gradient found. The current loss is:  0.6479602456092834\n",
      "Warning: nan gradient found. The current loss is:  1.9761247634887695\n",
      "Warning: nan gradient found. The current loss is:  0.4513207674026489\n",
      "Warning: nan gradient found. The current loss is:  0.17597806453704834\n",
      "Warning: nan gradient found. The current loss is:  0.2124897688627243\n",
      "Warning: nan gradient found. The current loss is:  0.33421972393989563\n",
      "Warning: nan gradient found. The current loss is:  0.9185047745704651\n",
      "Warning: nan gradient found. The current loss is:  0.2907490134239197\n",
      "Warning: nan gradient found. The current loss is:  0.42171454429626465\n",
      "Warning: nan gradient found. The current loss is:  0.89034104347229\n",
      "Warning: nan gradient found. The current loss is:  0.583930253982544\n",
      "Warning: nan gradient found. The current loss is:  0.842100977897644\n",
      "Warning: nan gradient found. The current loss is:  0.2646394670009613\n",
      "Warning: nan gradient found. The current loss is:  0.12763455510139465\n",
      "Warning: nan gradient found. The current loss is:  0.276239275932312\n",
      "Warning: nan gradient found. The current loss is:  0.44541117548942566\n",
      "Warning: nan gradient found. The current loss is:  0.3778145909309387\n",
      "Warning: nan gradient found. The current loss is:  2.2099578380584717\n",
      "Warning: nan gradient found. The current loss is:  0.4196155369281769\n",
      "Warning: nan gradient found. The current loss is:  0.4836525022983551\n",
      "Warning: nan gradient found. The current loss is:  0.6797675490379333\n",
      "Warning: nan gradient found. The current loss is:  0.7452991008758545\n",
      "Current batch training loss: 0.745299  [1843200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6923205256462097\n",
      "Warning: nan gradient found. The current loss is:  0.7101234197616577\n",
      "Warning: nan gradient found. The current loss is:  0.42614471912384033\n",
      "Warning: nan gradient found. The current loss is:  0.54830002784729\n",
      "Warning: nan gradient found. The current loss is:  0.5960711240768433\n",
      "Warning: nan gradient found. The current loss is:  -0.029822148382663727\n",
      "Warning: nan gradient found. The current loss is:  0.14496788382530212\n",
      "Warning: nan gradient found. The current loss is:  0.275018572807312\n",
      "Warning: nan gradient found. The current loss is:  1.0157346725463867\n",
      "Warning: nan gradient found. The current loss is:  0.35551363229751587\n",
      "Warning: nan gradient found. The current loss is:  0.3608735501766205\n",
      "Warning: nan gradient found. The current loss is:  0.8594609498977661\n",
      "Warning: nan gradient found. The current loss is:  0.4744449853897095\n",
      "Warning: nan gradient found. The current loss is:  0.15794092416763306\n",
      "Warning: nan gradient found. The current loss is:  0.5372865796089172\n",
      "Warning: nan gradient found. The current loss is:  0.8010836839675903\n",
      "Warning: nan gradient found. The current loss is:  0.850868821144104\n",
      "Warning: nan gradient found. The current loss is:  1.0106840133666992\n",
      "Warning: nan gradient found. The current loss is:  0.46779030561447144\n",
      "Warning: nan gradient found. The current loss is:  0.5593819618225098\n",
      "Warning: nan gradient found. The current loss is:  0.6655652523040771\n",
      "Warning: nan gradient found. The current loss is:  1.0025324821472168\n",
      "Warning: nan gradient found. The current loss is:  0.4069894552230835\n",
      "Warning: nan gradient found. The current loss is:  -0.10857059061527252\n",
      "Warning: nan gradient found. The current loss is:  0.6119555234909058\n",
      "Warning: nan gradient found. The current loss is:  0.21800655126571655\n",
      "Warning: nan gradient found. The current loss is:  0.2789062261581421\n",
      "Warning: nan gradient found. The current loss is:  0.8275995850563049\n",
      "Warning: nan gradient found. The current loss is:  0.1694432944059372\n",
      "Warning: nan gradient found. The current loss is:  1.286803126335144\n",
      "Warning: nan gradient found. The current loss is:  0.39727699756622314\n",
      "Warning: nan gradient found. The current loss is:  0.6896915435791016\n",
      "Warning: nan gradient found. The current loss is:  0.8082966804504395\n",
      "Warning: nan gradient found. The current loss is:  0.1737949401140213\n",
      "Warning: nan gradient found. The current loss is:  0.8721984624862671\n",
      "Warning: nan gradient found. The current loss is:  1.113739013671875\n",
      "Warning: nan gradient found. The current loss is:  0.35373497009277344\n",
      "Warning: nan gradient found. The current loss is:  0.7532835602760315\n",
      "Warning: nan gradient found. The current loss is:  0.4735199213027954\n",
      "Warning: nan gradient found. The current loss is:  0.6421557068824768\n",
      "Warning: nan gradient found. The current loss is:  0.5535342693328857\n",
      "Warning: nan gradient found. The current loss is:  0.31082695722579956\n",
      "Warning: nan gradient found. The current loss is:  0.3510691523551941\n",
      "Warning: nan gradient found. The current loss is:  0.8108910322189331\n",
      "Warning: nan gradient found. The current loss is:  0.5682914853096008\n",
      "Warning: nan gradient found. The current loss is:  0.5699803829193115\n",
      "Warning: nan gradient found. The current loss is:  0.28132379055023193\n",
      "Warning: nan gradient found. The current loss is:  0.7124074101448059\n",
      "Warning: nan gradient found. The current loss is:  0.75620436668396\n",
      "Warning: nan gradient found. The current loss is:  0.14321830868721008\n",
      "Warning: nan gradient found. The current loss is:  0.26116716861724854\n",
      "Warning: nan gradient found. The current loss is:  0.4345431327819824\n",
      "Warning: nan gradient found. The current loss is:  0.539432168006897\n",
      "Warning: nan gradient found. The current loss is:  0.29885047674179077\n",
      "Warning: nan gradient found. The current loss is:  0.2781503200531006\n",
      "Warning: nan gradient found. The current loss is:  0.4219469428062439\n",
      "Warning: nan gradient found. The current loss is:  0.32492345571517944\n",
      "Warning: nan gradient found. The current loss is:  0.3275436460971832\n",
      "Warning: nan gradient found. The current loss is:  0.6830019950866699\n",
      "Warning: nan gradient found. The current loss is:  0.7443422675132751\n",
      "Warning: nan gradient found. The current loss is:  0.24211947619915009\n",
      "Warning: nan gradient found. The current loss is:  0.9056243300437927\n",
      "Warning: nan gradient found. The current loss is:  0.30210474133491516\n",
      "Warning: nan gradient found. The current loss is:  0.04897502064704895\n",
      "Warning: nan gradient found. The current loss is:  0.38422563672065735\n",
      "Warning: nan gradient found. The current loss is:  0.4067844748497009\n",
      "Warning: nan gradient found. The current loss is:  0.7137327194213867\n",
      "Warning: nan gradient found. The current loss is:  1.4314969778060913\n",
      "Warning: nan gradient found. The current loss is:  0.12339520454406738\n",
      "Warning: nan gradient found. The current loss is:  0.2967493236064911\n",
      "Warning: nan gradient found. The current loss is:  0.44194644689559937\n",
      "Warning: nan gradient found. The current loss is:  0.7900110483169556\n",
      "Warning: nan gradient found. The current loss is:  0.10151912271976471\n",
      "Warning: nan gradient found. The current loss is:  0.4524635672569275\n",
      "Warning: nan gradient found. The current loss is:  0.6911138296127319\n",
      "Warning: nan gradient found. The current loss is:  0.6333902478218079\n",
      "Warning: nan gradient found. The current loss is:  1.001335620880127\n",
      "Warning: nan gradient found. The current loss is:  0.08019573986530304\n",
      "Warning: nan gradient found. The current loss is:  0.7437187433242798\n",
      "Warning: nan gradient found. The current loss is:  0.7461378574371338\n",
      "Warning: nan gradient found. The current loss is:  0.2051151692867279\n",
      "Warning: nan gradient found. The current loss is:  0.2707333266735077\n",
      "Warning: nan gradient found. The current loss is:  0.03681670501828194\n",
      "Warning: nan gradient found. The current loss is:  0.13970106840133667\n",
      "Warning: nan gradient found. The current loss is:  0.39482393860816956\n",
      "Warning: nan gradient found. The current loss is:  1.0326929092407227\n",
      "Warning: nan gradient found. The current loss is:  0.513727605342865\n",
      "Warning: nan gradient found. The current loss is:  0.3045012950897217\n",
      "Warning: nan gradient found. The current loss is:  0.7213143110275269\n",
      "Warning: nan gradient found. The current loss is:  0.9310904741287231\n",
      "Warning: nan gradient found. The current loss is:  0.7336173057556152\n",
      "Warning: nan gradient found. The current loss is:  0.08359234035015106\n",
      "Warning: nan gradient found. The current loss is:  0.41092002391815186\n",
      "Warning: nan gradient found. The current loss is:  0.49363523721694946\n",
      "Warning: nan gradient found. The current loss is:  0.4827646315097809\n",
      "Warning: nan gradient found. The current loss is:  0.6824966669082642\n",
      "Warning: nan gradient found. The current loss is:  0.7909339666366577\n",
      "Warning: nan gradient found. The current loss is:  1.1342103481292725\n",
      "Warning: nan gradient found. The current loss is:  0.25721275806427\n",
      "Warning: nan gradient found. The current loss is:  0.2429608702659607\n",
      "Current batch training loss: 0.242961  [1868800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.1898168921470642\n",
      "Warning: nan gradient found. The current loss is:  1.009031057357788\n",
      "Warning: nan gradient found. The current loss is:  0.21420419216156006\n",
      "Warning: nan gradient found. The current loss is:  0.17854739725589752\n",
      "Warning: nan gradient found. The current loss is:  1.0367364883422852\n",
      "Warning: nan gradient found. The current loss is:  1.0706591606140137\n",
      "Warning: nan gradient found. The current loss is:  0.2422725111246109\n",
      "Warning: nan gradient found. The current loss is:  0.441877543926239\n",
      "Warning: nan gradient found. The current loss is:  0.2091180831193924\n",
      "Warning: nan gradient found. The current loss is:  0.2430022656917572\n",
      "Warning: nan gradient found. The current loss is:  0.5432447195053101\n",
      "Warning: nan gradient found. The current loss is:  0.5747971534729004\n",
      "Warning: nan gradient found. The current loss is:  0.3090548515319824\n",
      "Warning: nan gradient found. The current loss is:  0.47859662771224976\n",
      "Warning: nan gradient found. The current loss is:  0.5492697954177856\n",
      "Warning: nan gradient found. The current loss is:  0.6685052514076233\n",
      "Warning: nan gradient found. The current loss is:  0.5684201717376709\n",
      "Warning: nan gradient found. The current loss is:  0.22203513979911804\n",
      "Warning: nan gradient found. The current loss is:  0.5514239072799683\n",
      "Warning: nan gradient found. The current loss is:  0.7075016498565674\n",
      "Warning: nan gradient found. The current loss is:  0.8126266002655029\n",
      "Warning: nan gradient found. The current loss is:  0.28701528906822205\n",
      "Warning: nan gradient found. The current loss is:  1.173572063446045\n",
      "Warning: nan gradient found. The current loss is:  0.9586389064788818\n",
      "Warning: nan gradient found. The current loss is:  0.44677332043647766\n",
      "Warning: nan gradient found. The current loss is:  0.4319479465484619\n",
      "Warning: nan gradient found. The current loss is:  0.3589031398296356\n",
      "Warning: nan gradient found. The current loss is:  0.6337931156158447\n",
      "Warning: nan gradient found. The current loss is:  0.7778291702270508\n",
      "Warning: nan gradient found. The current loss is:  0.870187520980835\n",
      "Warning: nan gradient found. The current loss is:  0.20627997815608978\n",
      "Warning: nan gradient found. The current loss is:  0.6742656230926514\n",
      "Warning: nan gradient found. The current loss is:  0.24312791228294373\n",
      "Warning: nan gradient found. The current loss is:  0.49625134468078613\n",
      "Warning: nan gradient found. The current loss is:  0.4974909722805023\n",
      "Warning: nan gradient found. The current loss is:  1.2685623168945312\n",
      "Warning: nan gradient found. The current loss is:  1.8732233047485352\n",
      "Warning: nan gradient found. The current loss is:  0.5066331624984741\n",
      "Warning: nan gradient found. The current loss is:  0.25539278984069824\n",
      "Warning: nan gradient found. The current loss is:  0.26659125089645386\n",
      "Warning: nan gradient found. The current loss is:  0.12037393450737\n",
      "Warning: nan gradient found. The current loss is:  0.14654302597045898\n",
      "Warning: nan gradient found. The current loss is:  -0.022914625704288483\n",
      "Warning: nan gradient found. The current loss is:  1.2322323322296143\n",
      "Warning: nan gradient found. The current loss is:  0.5963937640190125\n",
      "Warning: nan gradient found. The current loss is:  0.31259310245513916\n",
      "Warning: nan gradient found. The current loss is:  0.7281191349029541\n",
      "Warning: nan gradient found. The current loss is:  1.2142536640167236\n",
      "Warning: nan gradient found. The current loss is:  0.255119651556015\n",
      "Warning: nan gradient found. The current loss is:  1.3958266973495483\n",
      "Warning: nan gradient found. The current loss is:  1.0711710453033447\n",
      "Warning: nan gradient found. The current loss is:  0.3321865200996399\n",
      "Warning: nan gradient found. The current loss is:  0.6603702902793884\n",
      "Warning: nan gradient found. The current loss is:  0.35114753246307373\n",
      "Warning: nan gradient found. The current loss is:  0.5873644948005676\n",
      "Warning: nan gradient found. The current loss is:  0.3705173134803772\n",
      "Warning: nan gradient found. The current loss is:  0.5221574902534485\n",
      "Warning: nan gradient found. The current loss is:  0.18941068649291992\n",
      "Warning: nan gradient found. The current loss is:  0.43633580207824707\n",
      "Warning: nan gradient found. The current loss is:  0.9174606800079346\n",
      "Warning: nan gradient found. The current loss is:  0.7136027216911316\n",
      "Warning: nan gradient found. The current loss is:  0.3121529817581177\n",
      "Warning: nan gradient found. The current loss is:  0.5891579389572144\n",
      "Warning: nan gradient found. The current loss is:  0.7928430438041687\n",
      "Warning: nan gradient found. The current loss is:  1.8180742263793945\n",
      "Warning: nan gradient found. The current loss is:  0.40843331813812256\n",
      "Warning: nan gradient found. The current loss is:  0.788992166519165\n",
      "Warning: nan gradient found. The current loss is:  0.1755063831806183\n",
      "Warning: nan gradient found. The current loss is:  0.39125487208366394\n",
      "Warning: nan gradient found. The current loss is:  0.11260787397623062\n",
      "Warning: nan gradient found. The current loss is:  0.47862204909324646\n",
      "Warning: nan gradient found. The current loss is:  0.3070026636123657\n",
      "Warning: nan gradient found. The current loss is:  0.8570945858955383\n",
      "Warning: nan gradient found. The current loss is:  0.7954930663108826\n",
      "Warning: nan gradient found. The current loss is:  0.3703969717025757\n",
      "Warning: nan gradient found. The current loss is:  0.6672773361206055\n",
      "Warning: nan gradient found. The current loss is:  0.2121921181678772\n",
      "Warning: nan gradient found. The current loss is:  0.5569288730621338\n",
      "Warning: nan gradient found. The current loss is:  0.3556697368621826\n",
      "Warning: nan gradient found. The current loss is:  0.6827177405357361\n",
      "Warning: nan gradient found. The current loss is:  0.23748929798603058\n",
      "Warning: nan gradient found. The current loss is:  0.18567082285881042\n",
      "Warning: nan gradient found. The current loss is:  1.2694309949874878\n",
      "Warning: nan gradient found. The current loss is:  2.189138650894165\n",
      "Warning: nan gradient found. The current loss is:  1.0012304782867432\n",
      "Warning: nan gradient found. The current loss is:  0.6082627773284912\n",
      "Warning: nan gradient found. The current loss is:  1.0491052865982056\n",
      "Warning: nan gradient found. The current loss is:  0.7007172703742981\n",
      "Warning: nan gradient found. The current loss is:  0.37446433305740356\n",
      "Warning: nan gradient found. The current loss is:  0.1100931316614151\n",
      "Warning: nan gradient found. The current loss is:  1.103771686553955\n",
      "Warning: nan gradient found. The current loss is:  0.49877187609672546\n",
      "Warning: nan gradient found. The current loss is:  0.8481602072715759\n",
      "Warning: nan gradient found. The current loss is:  0.6967657804489136\n",
      "Warning: nan gradient found. The current loss is:  1.1266788244247437\n",
      "Warning: nan gradient found. The current loss is:  0.47303134202957153\n",
      "Warning: nan gradient found. The current loss is:  0.6135991811752319\n",
      "Warning: nan gradient found. The current loss is:  0.34024596214294434\n",
      "Warning: nan gradient found. The current loss is:  0.6549589037895203\n",
      "Warning: nan gradient found. The current loss is:  0.6017818450927734\n",
      "Current batch training loss: 0.601782  [1894400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.20529744029045105\n",
      "Warning: nan gradient found. The current loss is:  0.524276852607727\n",
      "Warning: nan gradient found. The current loss is:  0.8126410245895386\n",
      "Warning: nan gradient found. The current loss is:  0.5568968057632446\n",
      "Warning: nan gradient found. The current loss is:  0.9608346819877625\n",
      "Warning: nan gradient found. The current loss is:  0.9436323046684265\n",
      "Warning: nan gradient found. The current loss is:  0.4495515823364258\n",
      "Warning: nan gradient found. The current loss is:  0.09067292511463165\n",
      "Warning: nan gradient found. The current loss is:  0.8357359766960144\n",
      "Warning: nan gradient found. The current loss is:  0.9878013134002686\n",
      "Warning: nan gradient found. The current loss is:  1.0120539665222168\n",
      "Warning: nan gradient found. The current loss is:  1.307650089263916\n",
      "Warning: nan gradient found. The current loss is:  0.4678885042667389\n",
      "Warning: nan gradient found. The current loss is:  0.364128053188324\n",
      "Warning: nan gradient found. The current loss is:  0.5638318061828613\n",
      "Warning: nan gradient found. The current loss is:  0.3220723271369934\n",
      "Warning: nan gradient found. The current loss is:  0.3387974202632904\n",
      "Warning: nan gradient found. The current loss is:  0.24227185547351837\n",
      "Warning: nan gradient found. The current loss is:  0.08569299429655075\n",
      "Warning: nan gradient found. The current loss is:  1.299440622329712\n",
      "Warning: nan gradient found. The current loss is:  0.24824842810630798\n",
      "Warning: nan gradient found. The current loss is:  0.48663294315338135\n",
      "Warning: nan gradient found. The current loss is:  0.18525466322898865\n",
      "Warning: nan gradient found. The current loss is:  0.727462649345398\n",
      "Warning: nan gradient found. The current loss is:  0.16399788856506348\n",
      "Warning: nan gradient found. The current loss is:  1.7789511680603027\n",
      "Warning: nan gradient found. The current loss is:  0.2721984088420868\n",
      "Warning: nan gradient found. The current loss is:  0.2237970232963562\n",
      "Warning: nan gradient found. The current loss is:  0.6261332035064697\n",
      "Warning: nan gradient found. The current loss is:  0.3204277753829956\n",
      "Warning: nan gradient found. The current loss is:  0.5648362636566162\n",
      "Warning: nan gradient found. The current loss is:  0.13945189118385315\n",
      "Warning: nan gradient found. The current loss is:  0.7613359689712524\n",
      "Warning: nan gradient found. The current loss is:  1.2059834003448486\n",
      "Warning: nan gradient found. The current loss is:  0.5494030714035034\n",
      "Warning: nan gradient found. The current loss is:  0.42852556705474854\n",
      "Warning: nan gradient found. The current loss is:  0.18654179573059082\n",
      "Warning: nan gradient found. The current loss is:  0.36657577753067017\n",
      "Warning: nan gradient found. The current loss is:  0.5734894871711731\n",
      "Warning: nan gradient found. The current loss is:  0.544237494468689\n",
      "Warning: nan gradient found. The current loss is:  0.7705931067466736\n",
      "Warning: nan gradient found. The current loss is:  0.4868227243423462\n",
      "Warning: nan gradient found. The current loss is:  0.33208030462265015\n",
      "Warning: nan gradient found. The current loss is:  0.5611851215362549\n",
      "Warning: nan gradient found. The current loss is:  0.33295804262161255\n",
      "Warning: nan gradient found. The current loss is:  0.07856466621160507\n",
      "Warning: nan gradient found. The current loss is:  0.3583756685256958\n",
      "Warning: nan gradient found. The current loss is:  0.6554068326950073\n",
      "Warning: nan gradient found. The current loss is:  0.2736150920391083\n",
      "Warning: nan gradient found. The current loss is:  0.5629070997238159\n",
      "Warning: nan gradient found. The current loss is:  0.7486283779144287\n",
      "Warning: nan gradient found. The current loss is:  0.28140974044799805\n",
      "Warning: nan gradient found. The current loss is:  -0.08304432034492493\n",
      "Warning: nan gradient found. The current loss is:  0.4638628363609314\n",
      "Warning: nan gradient found. The current loss is:  0.05716080963611603\n",
      "Warning: nan gradient found. The current loss is:  0.9947913885116577\n",
      "Warning: nan gradient found. The current loss is:  0.19538429379463196\n",
      "Warning: nan gradient found. The current loss is:  0.40815168619155884\n",
      "Warning: nan gradient found. The current loss is:  0.21027794480323792\n",
      "Warning: nan gradient found. The current loss is:  0.23334765434265137\n",
      "Warning: nan gradient found. The current loss is:  0.7977734208106995\n",
      "Warning: nan gradient found. The current loss is:  1.3988569974899292\n",
      "Warning: nan gradient found. The current loss is:  0.4059920012950897\n",
      "Warning: nan gradient found. The current loss is:  0.8658733367919922\n",
      "Warning: nan gradient found. The current loss is:  -0.02020610310137272\n",
      "Warning: nan gradient found. The current loss is:  0.31184765696525574\n",
      "Warning: nan gradient found. The current loss is:  0.1759520173072815\n",
      "Warning: nan gradient found. The current loss is:  0.9791123867034912\n",
      "Warning: nan gradient found. The current loss is:  0.503822922706604\n",
      "Warning: nan gradient found. The current loss is:  0.7012942433357239\n",
      "Warning: nan gradient found. The current loss is:  0.5470705628395081\n",
      "Warning: nan gradient found. The current loss is:  0.4248145818710327\n",
      "Warning: nan gradient found. The current loss is:  1.2472383975982666\n",
      "Warning: nan gradient found. The current loss is:  -0.029372714459896088\n",
      "Warning: nan gradient found. The current loss is:  0.24646452069282532\n",
      "Warning: nan gradient found. The current loss is:  0.6514883041381836\n",
      "Warning: nan gradient found. The current loss is:  0.560120701789856\n",
      "Warning: nan gradient found. The current loss is:  0.7324687242507935\n",
      "Warning: nan gradient found. The current loss is:  1.3320229053497314\n",
      "Warning: nan gradient found. The current loss is:  0.4042883515357971\n",
      "Warning: nan gradient found. The current loss is:  0.3109086751937866\n",
      "Warning: nan gradient found. The current loss is:  0.8444244265556335\n",
      "Warning: nan gradient found. The current loss is:  1.27491295337677\n",
      "Warning: nan gradient found. The current loss is:  0.3782022297382355\n",
      "Warning: nan gradient found. The current loss is:  0.2913077175617218\n",
      "Warning: nan gradient found. The current loss is:  1.0163118839263916\n",
      "Warning: nan gradient found. The current loss is:  0.09332095086574554\n",
      "Warning: nan gradient found. The current loss is:  0.3701147735118866\n",
      "Warning: nan gradient found. The current loss is:  0.30839040875434875\n",
      "Warning: nan gradient found. The current loss is:  0.42824864387512207\n",
      "Warning: nan gradient found. The current loss is:  0.4703417122364044\n",
      "Warning: nan gradient found. The current loss is:  0.8469775319099426\n",
      "Warning: nan gradient found. The current loss is:  0.67534339427948\n",
      "Warning: nan gradient found. The current loss is:  0.6273497343063354\n",
      "Warning: nan gradient found. The current loss is:  0.13099125027656555\n",
      "Warning: nan gradient found. The current loss is:  0.8421801328659058\n",
      "Warning: nan gradient found. The current loss is:  0.798261284828186\n",
      "Warning: nan gradient found. The current loss is:  0.7760125398635864\n",
      "Warning: nan gradient found. The current loss is:  0.5238211154937744\n",
      "Warning: nan gradient found. The current loss is:  2.0643062591552734\n",
      "Current batch training loss: 2.064306  [1920000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4034320116043091\n",
      "Warning: nan gradient found. The current loss is:  2.0154144763946533\n",
      "Warning: nan gradient found. The current loss is:  0.39814847707748413\n",
      "Warning: nan gradient found. The current loss is:  0.31428176164627075\n",
      "Warning: nan gradient found. The current loss is:  2.5501365661621094\n",
      "Warning: nan gradient found. The current loss is:  0.8031877279281616\n",
      "Warning: nan gradient found. The current loss is:  0.45347028970718384\n",
      "Warning: nan gradient found. The current loss is:  0.528476893901825\n",
      "Warning: nan gradient found. The current loss is:  0.6612352132797241\n",
      "Warning: nan gradient found. The current loss is:  0.43468302488327026\n",
      "Warning: nan gradient found. The current loss is:  0.9431984424591064\n",
      "Warning: nan gradient found. The current loss is:  1.54304838180542\n",
      "Warning: nan gradient found. The current loss is:  0.44206103682518005\n",
      "Warning: nan gradient found. The current loss is:  0.6477567553520203\n",
      "Warning: nan gradient found. The current loss is:  0.615046501159668\n",
      "Warning: nan gradient found. The current loss is:  0.536115288734436\n",
      "Warning: nan gradient found. The current loss is:  0.49642160534858704\n",
      "Warning: nan gradient found. The current loss is:  0.5201134085655212\n",
      "Warning: nan gradient found. The current loss is:  0.34262415766716003\n",
      "Warning: nan gradient found. The current loss is:  0.6845558285713196\n",
      "Warning: nan gradient found. The current loss is:  0.7398989200592041\n",
      "Warning: nan gradient found. The current loss is:  0.7281174659729004\n",
      "Warning: nan gradient found. The current loss is:  0.5109307765960693\n",
      "Warning: nan gradient found. The current loss is:  0.27739909291267395\n",
      "Warning: nan gradient found. The current loss is:  0.21924985945224762\n",
      "Warning: nan gradient found. The current loss is:  0.29930341243743896\n",
      "Warning: nan gradient found. The current loss is:  0.44500112533569336\n",
      "Warning: nan gradient found. The current loss is:  0.8799285292625427\n",
      "Warning: nan gradient found. The current loss is:  0.8279153108596802\n",
      "Warning: nan gradient found. The current loss is:  0.7171205282211304\n",
      "Warning: nan gradient found. The current loss is:  0.7423171997070312\n",
      "Warning: nan gradient found. The current loss is:  0.5263086557388306\n",
      "Warning: nan gradient found. The current loss is:  0.542959988117218\n",
      "Warning: nan gradient found. The current loss is:  0.22365327179431915\n",
      "Warning: nan gradient found. The current loss is:  0.09908226132392883\n",
      "Warning: nan gradient found. The current loss is:  0.608177900314331\n",
      "Warning: nan gradient found. The current loss is:  0.7363114356994629\n",
      "Warning: nan gradient found. The current loss is:  0.3280462920665741\n",
      "Warning: nan gradient found. The current loss is:  0.5362083911895752\n",
      "Warning: nan gradient found. The current loss is:  0.4328031837940216\n",
      "Warning: nan gradient found. The current loss is:  0.3483137786388397\n",
      "Warning: nan gradient found. The current loss is:  0.9382210969924927\n",
      "Warning: nan gradient found. The current loss is:  0.7968201637268066\n",
      "Warning: nan gradient found. The current loss is:  0.4050223231315613\n",
      "Warning: nan gradient found. The current loss is:  0.48479941487312317\n",
      "Warning: nan gradient found. The current loss is:  0.10008823126554489\n",
      "Warning: nan gradient found. The current loss is:  0.707963228225708\n",
      "Warning: nan gradient found. The current loss is:  0.555988073348999\n",
      "Warning: nan gradient found. The current loss is:  0.5036687254905701\n",
      "Warning: nan gradient found. The current loss is:  0.30656546354293823\n",
      "Warning: nan gradient found. The current loss is:  0.3877411484718323\n",
      "Warning: nan gradient found. The current loss is:  0.5161764621734619\n",
      "Warning: nan gradient found. The current loss is:  0.31673914194107056\n",
      "Warning: nan gradient found. The current loss is:  0.8919633030891418\n",
      "Warning: nan gradient found. The current loss is:  0.45075803995132446\n",
      "Warning: nan gradient found. The current loss is:  0.5940933227539062\n",
      "Warning: nan gradient found. The current loss is:  0.30598244071006775\n",
      "Warning: nan gradient found. The current loss is:  0.7035316824913025\n",
      "Warning: nan gradient found. The current loss is:  0.24792422354221344\n",
      "Warning: nan gradient found. The current loss is:  0.2876778841018677\n",
      "Warning: nan gradient found. The current loss is:  0.36785733699798584\n",
      "Warning: nan gradient found. The current loss is:  0.5641826391220093\n",
      "Warning: nan gradient found. The current loss is:  0.5623056888580322\n",
      "Warning: nan gradient found. The current loss is:  0.8059264421463013\n",
      "Warning: nan gradient found. The current loss is:  0.3128643333911896\n",
      "Warning: nan gradient found. The current loss is:  0.33178773522377014\n",
      "Warning: nan gradient found. The current loss is:  1.198974609375\n",
      "Warning: nan gradient found. The current loss is:  0.53558349609375\n",
      "Warning: nan gradient found. The current loss is:  0.6785235404968262\n",
      "Warning: nan gradient found. The current loss is:  1.0225920677185059\n",
      "Warning: nan gradient found. The current loss is:  0.8301147222518921\n",
      "Warning: nan gradient found. The current loss is:  0.5052272081375122\n",
      "Warning: nan gradient found. The current loss is:  1.40194571018219\n",
      "Warning: nan gradient found. The current loss is:  0.20819184184074402\n",
      "Warning: nan gradient found. The current loss is:  0.7197834253311157\n",
      "Warning: nan gradient found. The current loss is:  0.44954100251197815\n",
      "Warning: nan gradient found. The current loss is:  0.6665804386138916\n",
      "Warning: nan gradient found. The current loss is:  0.5565919876098633\n",
      "Warning: nan gradient found. The current loss is:  0.4620116949081421\n",
      "Warning: nan gradient found. The current loss is:  0.356207013130188\n",
      "Warning: nan gradient found. The current loss is:  0.9579076170921326\n",
      "Warning: nan gradient found. The current loss is:  0.4007236361503601\n",
      "Warning: nan gradient found. The current loss is:  0.3662295937538147\n",
      "Warning: nan gradient found. The current loss is:  0.19083760678768158\n",
      "Warning: nan gradient found. The current loss is:  0.7043198347091675\n",
      "Warning: nan gradient found. The current loss is:  0.5193513631820679\n",
      "Warning: nan gradient found. The current loss is:  0.7697012424468994\n",
      "Warning: nan gradient found. The current loss is:  0.6438051462173462\n",
      "Warning: nan gradient found. The current loss is:  0.5994493961334229\n",
      "Warning: nan gradient found. The current loss is:  0.08536158502101898\n",
      "Warning: nan gradient found. The current loss is:  0.3461377024650574\n",
      "Warning: nan gradient found. The current loss is:  0.8804086446762085\n",
      "Warning: nan gradient found. The current loss is:  0.9060987234115601\n",
      "Warning: nan gradient found. The current loss is:  0.6416058540344238\n",
      "Warning: nan gradient found. The current loss is:  0.7282093167304993\n",
      "Warning: nan gradient found. The current loss is:  0.714895486831665\n",
      "Warning: nan gradient found. The current loss is:  0.9209747314453125\n",
      "Warning: nan gradient found. The current loss is:  0.8995272517204285\n",
      "Warning: nan gradient found. The current loss is:  1.1452134847640991\n",
      "Warning: nan gradient found. The current loss is:  2.0587220191955566\n",
      "Current batch training loss: 2.058722  [1945600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9267564415931702\n",
      "Warning: nan gradient found. The current loss is:  0.6273506879806519\n",
      "Warning: nan gradient found. The current loss is:  0.8738307952880859\n",
      "Warning: nan gradient found. The current loss is:  0.9867433309555054\n",
      "Warning: nan gradient found. The current loss is:  0.21144476532936096\n",
      "Warning: nan gradient found. The current loss is:  0.5991638898849487\n",
      "Warning: nan gradient found. The current loss is:  0.16634859144687653\n",
      "Warning: nan gradient found. The current loss is:  1.176581859588623\n",
      "Warning: nan gradient found. The current loss is:  1.4298701286315918\n",
      "Warning: nan gradient found. The current loss is:  1.3248631954193115\n",
      "Warning: nan gradient found. The current loss is:  0.7755515575408936\n",
      "Warning: nan gradient found. The current loss is:  0.10984618961811066\n",
      "Warning: nan gradient found. The current loss is:  1.144432544708252\n",
      "Warning: nan gradient found. The current loss is:  0.3290162980556488\n",
      "Warning: nan gradient found. The current loss is:  0.3019373118877411\n",
      "Warning: nan gradient found. The current loss is:  0.408500075340271\n",
      "Warning: nan gradient found. The current loss is:  0.6513619422912598\n",
      "Warning: nan gradient found. The current loss is:  0.2286549210548401\n",
      "Warning: nan gradient found. The current loss is:  0.4882199168205261\n",
      "Warning: nan gradient found. The current loss is:  0.3308258354663849\n",
      "Warning: nan gradient found. The current loss is:  0.2589402496814728\n",
      "Warning: nan gradient found. The current loss is:  0.3864862024784088\n",
      "Warning: nan gradient found. The current loss is:  0.6439093351364136\n",
      "Warning: nan gradient found. The current loss is:  0.5200806856155396\n",
      "Warning: nan gradient found. The current loss is:  0.7144203186035156\n",
      "Warning: nan gradient found. The current loss is:  2.4694697856903076\n",
      "Warning: nan gradient found. The current loss is:  0.8238703012466431\n",
      "Warning: nan gradient found. The current loss is:  0.6128752827644348\n",
      "Warning: nan gradient found. The current loss is:  0.3473910689353943\n",
      "Warning: nan gradient found. The current loss is:  0.9386023283004761\n",
      "Warning: nan gradient found. The current loss is:  1.0104174613952637\n",
      "Warning: nan gradient found. The current loss is:  0.31603753566741943\n",
      "Warning: nan gradient found. The current loss is:  0.7599854469299316\n",
      "Warning: nan gradient found. The current loss is:  0.6151444315910339\n",
      "Warning: nan gradient found. The current loss is:  0.8951718211174011\n",
      "Warning: nan gradient found. The current loss is:  0.39900845289230347\n",
      "Warning: nan gradient found. The current loss is:  0.28542768955230713\n",
      "Warning: nan gradient found. The current loss is:  0.31391221284866333\n",
      "Warning: nan gradient found. The current loss is:  0.04311539977788925\n",
      "Warning: nan gradient found. The current loss is:  0.9234267473220825\n",
      "Warning: nan gradient found. The current loss is:  0.5436705350875854\n",
      "Warning: nan gradient found. The current loss is:  0.9329056143760681\n",
      "Warning: nan gradient found. The current loss is:  0.7943432331085205\n",
      "Warning: nan gradient found. The current loss is:  0.7927876114845276\n",
      "Warning: nan gradient found. The current loss is:  0.28978660702705383\n",
      "Warning: nan gradient found. The current loss is:  0.5960594415664673\n",
      "Warning: nan gradient found. The current loss is:  0.0002514328807592392\n",
      "Warning: nan gradient found. The current loss is:  0.5161815881729126\n",
      "Warning: nan gradient found. The current loss is:  0.611085832118988\n",
      "Warning: nan gradient found. The current loss is:  1.154602289199829\n",
      "Warning: nan gradient found. The current loss is:  0.5841697454452515\n",
      "Warning: nan gradient found. The current loss is:  0.6877611875534058\n",
      "Warning: nan gradient found. The current loss is:  0.1348581463098526\n",
      "Warning: nan gradient found. The current loss is:  0.5823816061019897\n",
      "Warning: nan gradient found. The current loss is:  0.1671757996082306\n",
      "Warning: nan gradient found. The current loss is:  1.0058341026306152\n",
      "Warning: nan gradient found. The current loss is:  0.6677957773208618\n",
      "Warning: nan gradient found. The current loss is:  0.6090417504310608\n",
      "Warning: nan gradient found. The current loss is:  0.7503917217254639\n",
      "Warning: nan gradient found. The current loss is:  0.7077574133872986\n",
      "Warning: nan gradient found. The current loss is:  0.5040500164031982\n",
      "Warning: nan gradient found. The current loss is:  -0.18342791497707367\n",
      "Warning: nan gradient found. The current loss is:  0.2654242217540741\n",
      "Warning: nan gradient found. The current loss is:  0.19347164034843445\n",
      "Warning: nan gradient found. The current loss is:  0.21479354798793793\n",
      "Warning: nan gradient found. The current loss is:  0.24286094307899475\n",
      "Warning: nan gradient found. The current loss is:  0.49147623777389526\n",
      "Warning: nan gradient found. The current loss is:  0.544893205165863\n",
      "Warning: nan gradient found. The current loss is:  0.31966862082481384\n",
      "Warning: nan gradient found. The current loss is:  0.8242584466934204\n",
      "Warning: nan gradient found. The current loss is:  0.4357050061225891\n",
      "Warning: nan gradient found. The current loss is:  1.216355562210083\n",
      "Warning: nan gradient found. The current loss is:  0.41407328844070435\n",
      "Warning: nan gradient found. The current loss is:  0.6631837487220764\n",
      "Warning: nan gradient found. The current loss is:  1.1485068798065186\n",
      "Warning: nan gradient found. The current loss is:  0.20528090000152588\n",
      "Warning: nan gradient found. The current loss is:  0.6380074620246887\n",
      "Warning: nan gradient found. The current loss is:  0.3921653628349304\n",
      "Warning: nan gradient found. The current loss is:  0.32168859243392944\n",
      "Warning: nan gradient found. The current loss is:  0.614617109298706\n",
      "Warning: nan gradient found. The current loss is:  0.8342553973197937\n",
      "Warning: nan gradient found. The current loss is:  0.6500855684280396\n",
      "Warning: nan gradient found. The current loss is:  0.7025637626647949\n",
      "Warning: nan gradient found. The current loss is:  0.26084423065185547\n",
      "Warning: nan gradient found. The current loss is:  0.3315815329551697\n",
      "Warning: nan gradient found. The current loss is:  0.4522714614868164\n",
      "Warning: nan gradient found. The current loss is:  0.49198710918426514\n",
      "Warning: nan gradient found. The current loss is:  0.1435752511024475\n",
      "Warning: nan gradient found. The current loss is:  1.0561573505401611\n",
      "Warning: nan gradient found. The current loss is:  1.5383844375610352\n",
      "Warning: nan gradient found. The current loss is:  0.041345976293087006\n",
      "Warning: nan gradient found. The current loss is:  0.4501553475856781\n",
      "Warning: nan gradient found. The current loss is:  0.5718446969985962\n",
      "Warning: nan gradient found. The current loss is:  0.5110079646110535\n",
      "Warning: nan gradient found. The current loss is:  0.4468733072280884\n",
      "Warning: nan gradient found. The current loss is:  0.670330286026001\n",
      "Warning: nan gradient found. The current loss is:  0.5427728891372681\n",
      "Warning: nan gradient found. The current loss is:  0.6123561859130859\n",
      "Warning: nan gradient found. The current loss is:  0.39942610263824463\n",
      "Warning: nan gradient found. The current loss is:  1.1422052383422852\n",
      "Current batch training loss: 1.142205  [1971200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.058420389890670776\n",
      "Warning: nan gradient found. The current loss is:  0.4701045751571655\n",
      "Warning: nan gradient found. The current loss is:  0.8388606309890747\n",
      "Warning: nan gradient found. The current loss is:  0.6089904308319092\n",
      "Warning: nan gradient found. The current loss is:  0.31369855999946594\n",
      "Warning: nan gradient found. The current loss is:  0.7667535543441772\n",
      "Warning: nan gradient found. The current loss is:  1.5643773078918457\n",
      "Warning: nan gradient found. The current loss is:  0.3101392388343811\n",
      "Warning: nan gradient found. The current loss is:  0.8966143131256104\n",
      "Warning: nan gradient found. The current loss is:  0.8701877593994141\n",
      "Warning: nan gradient found. The current loss is:  0.7712693214416504\n",
      "Warning: nan gradient found. The current loss is:  0.45622676610946655\n",
      "Warning: nan gradient found. The current loss is:  0.5465188026428223\n",
      "Warning: nan gradient found. The current loss is:  0.4561401903629303\n",
      "Warning: nan gradient found. The current loss is:  0.3951027989387512\n",
      "Warning: nan gradient found. The current loss is:  0.4866602122783661\n",
      "Warning: nan gradient found. The current loss is:  0.7042160034179688\n",
      "Warning: nan gradient found. The current loss is:  0.38205569982528687\n",
      "Warning: nan gradient found. The current loss is:  1.4487831592559814\n",
      "Warning: nan gradient found. The current loss is:  0.6804516315460205\n",
      "Warning: nan gradient found. The current loss is:  1.032598614692688\n",
      "Warning: nan gradient found. The current loss is:  0.5006851553916931\n",
      "Warning: nan gradient found. The current loss is:  0.552872896194458\n",
      "Warning: nan gradient found. The current loss is:  0.5560688972473145\n",
      "Warning: nan gradient found. The current loss is:  0.6814963221549988\n",
      "Warning: nan gradient found. The current loss is:  0.7406566143035889\n",
      "Warning: nan gradient found. The current loss is:  0.6750204563140869\n",
      "Warning: nan gradient found. The current loss is:  1.2263641357421875\n",
      "Warning: nan gradient found. The current loss is:  1.5240204334259033\n",
      "Warning: nan gradient found. The current loss is:  3.038994073867798\n",
      "Warning: nan gradient found. The current loss is:  0.6188195943832397\n",
      "Warning: nan gradient found. The current loss is:  0.5876095294952393\n",
      "Warning: nan gradient found. The current loss is:  0.8216919898986816\n",
      "Warning: nan gradient found. The current loss is:  0.4006442427635193\n",
      "Warning: nan gradient found. The current loss is:  0.2574402987957001\n",
      "Warning: nan gradient found. The current loss is:  0.3506249785423279\n",
      "Warning: nan gradient found. The current loss is:  1.2732397317886353\n",
      "Warning: nan gradient found. The current loss is:  0.23558901250362396\n",
      "Warning: nan gradient found. The current loss is:  1.3159419298171997\n",
      "Warning: nan gradient found. The current loss is:  0.47941380739212036\n",
      "Warning: nan gradient found. The current loss is:  0.4857662320137024\n",
      "Warning: nan gradient found. The current loss is:  0.7883695363998413\n",
      "Warning: nan gradient found. The current loss is:  0.3254697322845459\n",
      "Warning: nan gradient found. The current loss is:  1.618822455406189\n",
      "Warning: nan gradient found. The current loss is:  0.4676150381565094\n",
      "Warning: nan gradient found. The current loss is:  0.5128039121627808\n",
      "Warning: nan gradient found. The current loss is:  0.25479596853256226\n",
      "Warning: nan gradient found. The current loss is:  1.9940192699432373\n",
      "Warning: nan gradient found. The current loss is:  0.2361074537038803\n",
      "Warning: nan gradient found. The current loss is:  0.6944147348403931\n",
      "Warning: nan gradient found. The current loss is:  0.6332823038101196\n",
      "Warning: nan gradient found. The current loss is:  1.1746822595596313\n",
      "Warning: nan gradient found. The current loss is:  0.5193287134170532\n",
      "Warning: nan gradient found. The current loss is:  0.7196084260940552\n",
      "Warning: nan gradient found. The current loss is:  0.27976179122924805\n",
      "Warning: nan gradient found. The current loss is:  0.3907018303871155\n",
      "Warning: nan gradient found. The current loss is:  0.04615258052945137\n",
      "Warning: nan gradient found. The current loss is:  0.1256738156080246\n",
      "Warning: nan gradient found. The current loss is:  0.7043898105621338\n",
      "Warning: nan gradient found. The current loss is:  0.4221736788749695\n",
      "Warning: nan gradient found. The current loss is:  0.9632733464241028\n",
      "Warning: nan gradient found. The current loss is:  -0.04764857888221741\n",
      "Warning: nan gradient found. The current loss is:  0.575871467590332\n",
      "Warning: nan gradient found. The current loss is:  0.32139259576797485\n",
      "Warning: nan gradient found. The current loss is:  0.6934996843338013\n",
      "Warning: nan gradient found. The current loss is:  0.7533544898033142\n",
      "Warning: nan gradient found. The current loss is:  0.16319844126701355\n",
      "Warning: nan gradient found. The current loss is:  0.45203593373298645\n",
      "Warning: nan gradient found. The current loss is:  0.9328168630599976\n",
      "Warning: nan gradient found. The current loss is:  1.1326786279678345\n",
      "Warning: nan gradient found. The current loss is:  0.4357636272907257\n",
      "Warning: nan gradient found. The current loss is:  1.0535423755645752\n",
      "Warning: nan gradient found. The current loss is:  0.2755403518676758\n",
      "Warning: nan gradient found. The current loss is:  0.469660222530365\n",
      "Warning: nan gradient found. The current loss is:  0.7117606401443481\n",
      "Warning: nan gradient found. The current loss is:  0.4778311550617218\n",
      "Warning: nan gradient found. The current loss is:  0.5225107669830322\n",
      "Warning: nan gradient found. The current loss is:  0.6839371919631958\n",
      "Warning: nan gradient found. The current loss is:  0.2996828258037567\n",
      "Warning: nan gradient found. The current loss is:  0.3581498861312866\n",
      "Warning: nan gradient found. The current loss is:  0.7334634065628052\n",
      "Warning: nan gradient found. The current loss is:  0.46585553884506226\n",
      "Warning: nan gradient found. The current loss is:  1.1235685348510742\n",
      "Warning: nan gradient found. The current loss is:  0.3682439923286438\n",
      "Warning: nan gradient found. The current loss is:  0.20595090091228485\n",
      "Warning: nan gradient found. The current loss is:  0.5236554145812988\n",
      "Warning: nan gradient found. The current loss is:  0.8436211347579956\n",
      "Warning: nan gradient found. The current loss is:  0.7174931764602661\n",
      "Warning: nan gradient found. The current loss is:  1.0556957721710205\n",
      "Warning: nan gradient found. The current loss is:  0.4693882465362549\n",
      "Warning: nan gradient found. The current loss is:  0.3604636788368225\n",
      "Warning: nan gradient found. The current loss is:  0.6616540551185608\n",
      "Warning: nan gradient found. The current loss is:  0.6482188701629639\n",
      "Warning: nan gradient found. The current loss is:  1.0658918619155884\n",
      "Warning: nan gradient found. The current loss is:  0.34076106548309326\n",
      "Warning: nan gradient found. The current loss is:  0.7671640515327454\n",
      "Warning: nan gradient found. The current loss is:  0.26898497343063354\n",
      "Warning: nan gradient found. The current loss is:  0.6735906600952148\n",
      "Warning: nan gradient found. The current loss is:  0.6650914549827576\n",
      "Warning: nan gradient found. The current loss is:  1.219979166984558\n",
      "Current batch training loss: 1.219979  [1996800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6278873682022095\n",
      "Warning: nan gradient found. The current loss is:  0.3744131028652191\n",
      "Warning: nan gradient found. The current loss is:  0.28084796667099\n",
      "Warning: nan gradient found. The current loss is:  1.1870393753051758\n",
      "Warning: nan gradient found. The current loss is:  0.25584667921066284\n",
      "Warning: nan gradient found. The current loss is:  0.5721604824066162\n",
      "Warning: nan gradient found. The current loss is:  0.5273981094360352\n",
      "Warning: nan gradient found. The current loss is:  0.5107882022857666\n",
      "Warning: nan gradient found. The current loss is:  0.12114962190389633\n",
      "Warning: nan gradient found. The current loss is:  1.1925384998321533\n",
      "Warning: nan gradient found. The current loss is:  0.3766995072364807\n",
      "Warning: nan gradient found. The current loss is:  0.634516179561615\n",
      "Warning: nan gradient found. The current loss is:  0.5625007748603821\n",
      "Warning: nan gradient found. The current loss is:  0.2814943790435791\n",
      "Warning: nan gradient found. The current loss is:  1.028846025466919\n",
      "Warning: nan gradient found. The current loss is:  0.5056034326553345\n",
      "Warning: nan gradient found. The current loss is:  1.094761610031128\n",
      "Warning: nan gradient found. The current loss is:  0.5562784671783447\n",
      "Warning: nan gradient found. The current loss is:  0.9109084606170654\n",
      "Warning: nan gradient found. The current loss is:  0.7607035040855408\n",
      "Warning: nan gradient found. The current loss is:  0.9064915180206299\n",
      "Warning: nan gradient found. The current loss is:  0.31065118312835693\n",
      "Warning: nan gradient found. The current loss is:  0.5703674554824829\n",
      "Warning: nan gradient found. The current loss is:  0.2485947608947754\n",
      "Warning: nan gradient found. The current loss is:  0.22423842549324036\n",
      "Warning: nan gradient found. The current loss is:  0.6158236861228943\n",
      "Warning: nan gradient found. The current loss is:  0.4530344605445862\n",
      "Warning: nan gradient found. The current loss is:  0.41956865787506104\n",
      "Warning: nan gradient found. The current loss is:  0.5132910013198853\n",
      "Warning: nan gradient found. The current loss is:  0.6814706325531006\n",
      "Warning: nan gradient found. The current loss is:  0.5180028676986694\n",
      "Warning: nan gradient found. The current loss is:  0.5631046891212463\n",
      "Warning: nan gradient found. The current loss is:  0.5563833713531494\n",
      "Warning: nan gradient found. The current loss is:  0.556769847869873\n",
      "Warning: nan gradient found. The current loss is:  0.2884922921657562\n",
      "Warning: nan gradient found. The current loss is:  0.39700374007225037\n",
      "Warning: nan gradient found. The current loss is:  0.361961305141449\n",
      "Warning: nan gradient found. The current loss is:  0.7041526436805725\n",
      "Warning: nan gradient found. The current loss is:  0.44795292615890503\n",
      "Warning: nan gradient found. The current loss is:  0.5686148405075073\n",
      "Warning: nan gradient found. The current loss is:  0.28191983699798584\n",
      "Warning: nan gradient found. The current loss is:  0.4809762239456177\n",
      "Warning: nan gradient found. The current loss is:  0.7517842054367065\n",
      "Warning: nan gradient found. The current loss is:  0.4356636106967926\n",
      "Warning: nan gradient found. The current loss is:  1.416539192199707\n",
      "Warning: nan gradient found. The current loss is:  0.9536081552505493\n",
      "Warning: nan gradient found. The current loss is:  1.1868398189544678\n",
      "Warning: nan gradient found. The current loss is:  0.3440608084201813\n",
      "Warning: nan gradient found. The current loss is:  0.6681484580039978\n",
      "Warning: nan gradient found. The current loss is:  0.7737111449241638\n",
      "Warning: nan gradient found. The current loss is:  0.33363422751426697\n",
      "Warning: nan gradient found. The current loss is:  0.33616533875465393\n",
      "Warning: nan gradient found. The current loss is:  0.6178469061851501\n",
      "Warning: nan gradient found. The current loss is:  0.7129615545272827\n",
      "Warning: nan gradient found. The current loss is:  0.727682888507843\n",
      "Warning: nan gradient found. The current loss is:  0.28871989250183105\n",
      "Warning: nan gradient found. The current loss is:  0.39287400245666504\n",
      "Warning: nan gradient found. The current loss is:  0.16725794970989227\n",
      "Warning: nan gradient found. The current loss is:  0.3206550180912018\n",
      "Warning: nan gradient found. The current loss is:  0.5929558873176575\n",
      "Warning: nan gradient found. The current loss is:  0.5700410604476929\n",
      "Warning: nan gradient found. The current loss is:  1.0253633260726929\n",
      "Warning: nan gradient found. The current loss is:  0.4923434853553772\n",
      "Warning: nan gradient found. The current loss is:  0.45879504084587097\n",
      "Warning: nan gradient found. The current loss is:  2.294692039489746\n",
      "Warning: nan gradient found. The current loss is:  1.027035117149353\n",
      "Warning: nan gradient found. The current loss is:  0.4306032657623291\n",
      "Warning: nan gradient found. The current loss is:  1.587869644165039\n",
      "Warning: nan gradient found. The current loss is:  0.4836438000202179\n",
      "Warning: nan gradient found. The current loss is:  0.6560567021369934\n",
      "Warning: nan gradient found. The current loss is:  1.142772912979126\n",
      "Warning: nan gradient found. The current loss is:  0.6380078792572021\n",
      "Warning: nan gradient found. The current loss is:  0.3078615665435791\n",
      "Warning: nan gradient found. The current loss is:  0.33034390211105347\n",
      "Warning: nan gradient found. The current loss is:  0.49615293741226196\n",
      "Warning: nan gradient found. The current loss is:  0.3157966136932373\n",
      "Warning: nan gradient found. The current loss is:  0.30694735050201416\n",
      "Warning: nan gradient found. The current loss is:  0.8365978598594666\n",
      "Warning: nan gradient found. The current loss is:  0.3731086254119873\n",
      "Warning: nan gradient found. The current loss is:  0.10029858350753784\n",
      "Warning: nan gradient found. The current loss is:  0.1533358097076416\n",
      "Warning: nan gradient found. The current loss is:  1.1934829950332642\n",
      "Warning: nan gradient found. The current loss is:  1.396106481552124\n",
      "Warning: nan gradient found. The current loss is:  0.2438158243894577\n",
      "Warning: nan gradient found. The current loss is:  0.5740537643432617\n",
      "Warning: nan gradient found. The current loss is:  0.22798126935958862\n",
      "Warning: nan gradient found. The current loss is:  0.5140687823295593\n",
      "Warning: nan gradient found. The current loss is:  1.2308738231658936\n",
      "Warning: nan gradient found. The current loss is:  0.2204076498746872\n",
      "Warning: nan gradient found. The current loss is:  0.7971866130828857\n",
      "Warning: nan gradient found. The current loss is:  0.5036057233810425\n",
      "Warning: nan gradient found. The current loss is:  0.7734171748161316\n",
      "Warning: nan gradient found. The current loss is:  0.32541483640670776\n",
      "Warning: nan gradient found. The current loss is:  -0.030056491494178772\n",
      "Warning: nan gradient found. The current loss is:  0.8485924005508423\n",
      "Warning: nan gradient found. The current loss is:  0.48762497305870056\n",
      "Warning: nan gradient found. The current loss is:  1.5675206184387207\n",
      "Warning: nan gradient found. The current loss is:  1.0430896282196045\n",
      "Warning: nan gradient found. The current loss is:  0.2394561618566513\n",
      "Warning: nan gradient found. The current loss is:  0.5767634510993958\n",
      "Current batch training loss: 0.576763  [2022400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5027550458908081\n",
      "Warning: nan gradient found. The current loss is:  0.3885435163974762\n",
      "Warning: nan gradient found. The current loss is:  0.46468937397003174\n",
      "Warning: nan gradient found. The current loss is:  0.8646858334541321\n",
      "Warning: nan gradient found. The current loss is:  0.5808843374252319\n",
      "Warning: nan gradient found. The current loss is:  0.4848780930042267\n",
      "Warning: nan gradient found. The current loss is:  0.69968581199646\n",
      "Warning: nan gradient found. The current loss is:  0.4197661876678467\n",
      "Warning: nan gradient found. The current loss is:  0.9182900786399841\n",
      "Warning: nan gradient found. The current loss is:  0.8288396596908569\n",
      "Warning: nan gradient found. The current loss is:  0.7318220138549805\n",
      "Warning: nan gradient found. The current loss is:  0.7477397918701172\n",
      "Warning: nan gradient found. The current loss is:  0.4112253785133362\n",
      "Warning: nan gradient found. The current loss is:  0.5759236812591553\n",
      "Warning: nan gradient found. The current loss is:  1.0047821998596191\n",
      "Warning: nan gradient found. The current loss is:  0.6087871193885803\n",
      "Warning: nan gradient found. The current loss is:  0.2675095200538635\n",
      "Warning: nan gradient found. The current loss is:  0.8850769996643066\n",
      "Warning: nan gradient found. The current loss is:  0.29073548316955566\n",
      "Warning: nan gradient found. The current loss is:  0.9143815636634827\n",
      "Warning: nan gradient found. The current loss is:  1.7091455459594727\n",
      "Warning: nan gradient found. The current loss is:  0.7911734580993652\n",
      "Warning: nan gradient found. The current loss is:  0.42230933904647827\n",
      "Warning: nan gradient found. The current loss is:  0.371734619140625\n",
      "Warning: nan gradient found. The current loss is:  0.45037969946861267\n",
      "Warning: nan gradient found. The current loss is:  0.5649332404136658\n",
      "Warning: nan gradient found. The current loss is:  0.570946455001831\n",
      "Warning: nan gradient found. The current loss is:  1.7401933670043945\n",
      "Warning: nan gradient found. The current loss is:  0.784123420715332\n",
      "Warning: nan gradient found. The current loss is:  0.4178779423236847\n",
      "Warning: nan gradient found. The current loss is:  0.5214698314666748\n",
      "Warning: nan gradient found. The current loss is:  0.5192668437957764\n",
      "Warning: nan gradient found. The current loss is:  0.6289873123168945\n",
      "Warning: nan gradient found. The current loss is:  0.5064394474029541\n",
      "Warning: nan gradient found. The current loss is:  0.39673173427581787\n",
      "Warning: nan gradient found. The current loss is:  0.3153659999370575\n",
      "Warning: nan gradient found. The current loss is:  0.8097718954086304\n",
      "Warning: nan gradient found. The current loss is:  0.2528507709503174\n",
      "Warning: nan gradient found. The current loss is:  0.38830119371414185\n",
      "Warning: nan gradient found. The current loss is:  0.40572404861450195\n",
      "Warning: nan gradient found. The current loss is:  0.3236994743347168\n",
      "Warning: nan gradient found. The current loss is:  0.9273127913475037\n",
      "Warning: nan gradient found. The current loss is:  0.36503711342811584\n",
      "Warning: nan gradient found. The current loss is:  0.18868646025657654\n",
      "Warning: nan gradient found. The current loss is:  0.6835488080978394\n",
      "Warning: nan gradient found. The current loss is:  0.1490616798400879\n",
      "Warning: nan gradient found. The current loss is:  0.19632560014724731\n",
      "Warning: nan gradient found. The current loss is:  1.512592077255249\n",
      "Warning: nan gradient found. The current loss is:  0.6484376788139343\n",
      "Warning: nan gradient found. The current loss is:  1.0311387777328491\n",
      "Warning: nan gradient found. The current loss is:  0.518592119216919\n",
      "Warning: nan gradient found. The current loss is:  0.0702538788318634\n",
      "Warning: nan gradient found. The current loss is:  1.0539963245391846\n",
      "Warning: nan gradient found. The current loss is:  0.9540250301361084\n",
      "Warning: nan gradient found. The current loss is:  0.3235933184623718\n",
      "Warning: nan gradient found. The current loss is:  0.27587684988975525\n",
      "Warning: nan gradient found. The current loss is:  0.060775455087423325\n",
      "Warning: nan gradient found. The current loss is:  0.6872221231460571\n",
      "Warning: nan gradient found. The current loss is:  0.3061847388744354\n",
      "Warning: nan gradient found. The current loss is:  0.6138153076171875\n",
      "Warning: nan gradient found. The current loss is:  0.8304060697555542\n",
      "Warning: nan gradient found. The current loss is:  0.5234169960021973\n",
      "Warning: nan gradient found. The current loss is:  0.9073513746261597\n",
      "Warning: nan gradient found. The current loss is:  0.31194430589675903\n",
      "Warning: nan gradient found. The current loss is:  0.7626087069511414\n",
      "Warning: nan gradient found. The current loss is:  -0.1402469426393509\n",
      "Warning: nan gradient found. The current loss is:  0.5864341259002686\n",
      "Warning: nan gradient found. The current loss is:  0.2345651388168335\n",
      "Warning: nan gradient found. The current loss is:  0.5435134172439575\n",
      "Warning: nan gradient found. The current loss is:  0.7252992987632751\n",
      "Warning: nan gradient found. The current loss is:  0.9359046220779419\n",
      "Warning: nan gradient found. The current loss is:  1.0368173122406006\n",
      "Warning: nan gradient found. The current loss is:  0.3497658371925354\n",
      "Warning: nan gradient found. The current loss is:  0.3361644744873047\n",
      "Warning: nan gradient found. The current loss is:  0.4856696128845215\n",
      "Warning: nan gradient found. The current loss is:  0.11765159666538239\n",
      "Warning: nan gradient found. The current loss is:  0.33594000339508057\n",
      "Warning: nan gradient found. The current loss is:  0.4683755934238434\n",
      "Warning: nan gradient found. The current loss is:  0.4289765954017639\n",
      "Warning: nan gradient found. The current loss is:  2.066796064376831\n",
      "Warning: nan gradient found. The current loss is:  0.8743976354598999\n",
      "Warning: nan gradient found. The current loss is:  0.24726706743240356\n",
      "Warning: nan gradient found. The current loss is:  0.4694945812225342\n",
      "Warning: nan gradient found. The current loss is:  0.5763723254203796\n",
      "Warning: nan gradient found. The current loss is:  0.35888203978538513\n",
      "Warning: nan gradient found. The current loss is:  0.6149225234985352\n",
      "Warning: nan gradient found. The current loss is:  0.6230076551437378\n",
      "Warning: nan gradient found. The current loss is:  0.21296687424182892\n",
      "Warning: nan gradient found. The current loss is:  0.5709625482559204\n",
      "Warning: nan gradient found. The current loss is:  0.3456292450428009\n",
      "Warning: nan gradient found. The current loss is:  1.702915072441101\n",
      "Warning: nan gradient found. The current loss is:  0.4327685236930847\n",
      "Warning: nan gradient found. The current loss is:  0.18174125254154205\n",
      "Warning: nan gradient found. The current loss is:  0.4785338342189789\n",
      "Warning: nan gradient found. The current loss is:  0.4588327407836914\n",
      "Warning: nan gradient found. The current loss is:  0.5926660299301147\n",
      "Warning: nan gradient found. The current loss is:  0.9096353650093079\n",
      "Warning: nan gradient found. The current loss is:  0.5056090354919434\n",
      "Warning: nan gradient found. The current loss is:  -0.04105350002646446\n",
      "Warning: nan gradient found. The current loss is:  0.6436513662338257\n",
      "Current batch training loss: 0.643651  [2048000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.49641841650009155\n",
      "Warning: nan gradient found. The current loss is:  0.5921226739883423\n",
      "Warning: nan gradient found. The current loss is:  0.6288276314735413\n",
      "Warning: nan gradient found. The current loss is:  1.7991225719451904\n",
      "Warning: nan gradient found. The current loss is:  0.06963291019201279\n",
      "Warning: nan gradient found. The current loss is:  0.7078964114189148\n",
      "Warning: nan gradient found. The current loss is:  0.6832260489463806\n",
      "Warning: nan gradient found. The current loss is:  0.06703229248523712\n",
      "Warning: nan gradient found. The current loss is:  0.3740295171737671\n",
      "Warning: nan gradient found. The current loss is:  0.5107179880142212\n",
      "Warning: nan gradient found. The current loss is:  0.43369343876838684\n",
      "Warning: nan gradient found. The current loss is:  0.28940942883491516\n",
      "Warning: nan gradient found. The current loss is:  0.4402637481689453\n",
      "Warning: nan gradient found. The current loss is:  0.31964945793151855\n",
      "Warning: nan gradient found. The current loss is:  0.4701993465423584\n",
      "Warning: nan gradient found. The current loss is:  0.1407564878463745\n",
      "Warning: nan gradient found. The current loss is:  0.08627384901046753\n",
      "Warning: nan gradient found. The current loss is:  0.36543914675712585\n",
      "Warning: nan gradient found. The current loss is:  0.4351899027824402\n",
      "Warning: nan gradient found. The current loss is:  0.2889690399169922\n",
      "Warning: nan gradient found. The current loss is:  0.5300960540771484\n",
      "Warning: nan gradient found. The current loss is:  0.4968540668487549\n",
      "Warning: nan gradient found. The current loss is:  0.6150334477424622\n",
      "Warning: nan gradient found. The current loss is:  0.3232896327972412\n",
      "Warning: nan gradient found. The current loss is:  0.5240492820739746\n",
      "Warning: nan gradient found. The current loss is:  0.21654373407363892\n",
      "Warning: nan gradient found. The current loss is:  0.4725627601146698\n",
      "Warning: nan gradient found. The current loss is:  0.7678498029708862\n",
      "Warning: nan gradient found. The current loss is:  0.12963293492794037\n",
      "Warning: nan gradient found. The current loss is:  0.4144194424152374\n",
      "Warning: nan gradient found. The current loss is:  0.051001809537410736\n",
      "Warning: nan gradient found. The current loss is:  0.1463339775800705\n",
      "Warning: nan gradient found. The current loss is:  0.8183680176734924\n",
      "Warning: nan gradient found. The current loss is:  0.7400962710380554\n",
      "Warning: nan gradient found. The current loss is:  0.6652189493179321\n",
      "Warning: nan gradient found. The current loss is:  0.20447605848312378\n",
      "Warning: nan gradient found. The current loss is:  2.473010778427124\n",
      "Warning: nan gradient found. The current loss is:  0.7580752372741699\n",
      "Warning: nan gradient found. The current loss is:  0.3445183336734772\n",
      "Warning: nan gradient found. The current loss is:  0.3809508681297302\n",
      "Warning: nan gradient found. The current loss is:  1.8002855777740479\n",
      "Warning: nan gradient found. The current loss is:  0.25305604934692383\n",
      "Warning: nan gradient found. The current loss is:  0.7041152119636536\n",
      "Warning: nan gradient found. The current loss is:  0.9971583485603333\n",
      "Warning: nan gradient found. The current loss is:  0.1318027675151825\n",
      "Warning: nan gradient found. The current loss is:  0.2738678455352783\n",
      "Warning: nan gradient found. The current loss is:  0.2982326149940491\n",
      "Warning: nan gradient found. The current loss is:  0.9707528352737427\n",
      "Warning: nan gradient found. The current loss is:  0.14931127429008484\n",
      "Warning: nan gradient found. The current loss is:  0.9364317655563354\n",
      "Warning: nan gradient found. The current loss is:  0.7633318901062012\n",
      "Warning: nan gradient found. The current loss is:  0.27430281043052673\n",
      "Warning: nan gradient found. The current loss is:  1.0653300285339355\n",
      "Warning: nan gradient found. The current loss is:  1.1234028339385986\n",
      "Warning: nan gradient found. The current loss is:  0.4896351397037506\n",
      "Warning: nan gradient found. The current loss is:  0.38892897963523865\n",
      "Warning: nan gradient found. The current loss is:  1.0899643898010254\n",
      "Warning: nan gradient found. The current loss is:  0.2541362941265106\n",
      "Warning: nan gradient found. The current loss is:  0.33432072401046753\n",
      "Warning: nan gradient found. The current loss is:  0.5431698560714722\n",
      "Warning: nan gradient found. The current loss is:  0.4529231786727905\n",
      "Warning: nan gradient found. The current loss is:  1.7529809474945068\n",
      "Warning: nan gradient found. The current loss is:  0.8202031850814819\n",
      "Warning: nan gradient found. The current loss is:  0.36587804555892944\n",
      "Warning: nan gradient found. The current loss is:  0.7612276673316956\n",
      "Warning: nan gradient found. The current loss is:  0.40748780965805054\n",
      "Warning: nan gradient found. The current loss is:  0.29337650537490845\n",
      "Warning: nan gradient found. The current loss is:  1.446747064590454\n",
      "Warning: nan gradient found. The current loss is:  0.9699389934539795\n",
      "Warning: nan gradient found. The current loss is:  0.055794764310121536\n",
      "Warning: nan gradient found. The current loss is:  0.8281645774841309\n",
      "Warning: nan gradient found. The current loss is:  0.7438674569129944\n",
      "Warning: nan gradient found. The current loss is:  0.3948853611946106\n",
      "Warning: nan gradient found. The current loss is:  0.5681936740875244\n",
      "Warning: nan gradient found. The current loss is:  1.0226330757141113\n",
      "Warning: nan gradient found. The current loss is:  0.7365391850471497\n",
      "Warning: nan gradient found. The current loss is:  0.5467219352722168\n",
      "Warning: nan gradient found. The current loss is:  0.6343566179275513\n",
      "Warning: nan gradient found. The current loss is:  0.34812021255493164\n",
      "Warning: nan gradient found. The current loss is:  0.4976447820663452\n",
      "Warning: nan gradient found. The current loss is:  1.1961103677749634\n",
      "Warning: nan gradient found. The current loss is:  0.6525712013244629\n",
      "Warning: nan gradient found. The current loss is:  0.6813806295394897\n",
      "Warning: nan gradient found. The current loss is:  0.27227655053138733\n",
      "Warning: nan gradient found. The current loss is:  0.34704768657684326\n",
      "Warning: nan gradient found. The current loss is:  0.24681001901626587\n",
      "Warning: nan gradient found. The current loss is:  0.3148435950279236\n",
      "Warning: nan gradient found. The current loss is:  0.7261644601821899\n",
      "Warning: nan gradient found. The current loss is:  0.32146787643432617\n",
      "Warning: nan gradient found. The current loss is:  0.4079517722129822\n",
      "Warning: nan gradient found. The current loss is:  1.1523149013519287\n",
      "Warning: nan gradient found. The current loss is:  0.16946807503700256\n",
      "Warning: nan gradient found. The current loss is:  0.8370728492736816\n",
      "Warning: nan gradient found. The current loss is:  0.5062716603279114\n",
      "Warning: nan gradient found. The current loss is:  0.22727183997631073\n",
      "Warning: nan gradient found. The current loss is:  1.083019495010376\n",
      "Warning: nan gradient found. The current loss is:  0.8107906579971313\n",
      "Warning: nan gradient found. The current loss is:  0.6232120394706726\n",
      "Warning: nan gradient found. The current loss is:  0.7190920114517212\n",
      "Warning: nan gradient found. The current loss is:  1.5974563360214233\n",
      "Current batch training loss: 1.597456  [2073600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.25821805000305176\n",
      "Warning: nan gradient found. The current loss is:  0.38187897205352783\n",
      "Warning: nan gradient found. The current loss is:  0.4903080463409424\n",
      "Warning: nan gradient found. The current loss is:  1.0286046266555786\n",
      "Warning: nan gradient found. The current loss is:  0.9586375951766968\n",
      "Warning: nan gradient found. The current loss is:  0.7184673547744751\n",
      "Warning: nan gradient found. The current loss is:  0.4549017548561096\n",
      "Warning: nan gradient found. The current loss is:  0.7466528415679932\n",
      "Warning: nan gradient found. The current loss is:  0.18710216879844666\n",
      "Warning: nan gradient found. The current loss is:  0.22658973932266235\n",
      "Warning: nan gradient found. The current loss is:  0.43478915095329285\n",
      "Warning: nan gradient found. The current loss is:  1.1129481792449951\n",
      "Warning: nan gradient found. The current loss is:  1.6549842357635498\n",
      "Warning: nan gradient found. The current loss is:  1.0192383527755737\n",
      "Warning: nan gradient found. The current loss is:  0.37867429852485657\n",
      "Warning: nan gradient found. The current loss is:  1.8881456851959229\n",
      "Warning: nan gradient found. The current loss is:  0.775592565536499\n",
      "Warning: nan gradient found. The current loss is:  0.6768150329589844\n",
      "Warning: nan gradient found. The current loss is:  0.9975244998931885\n",
      "Warning: nan gradient found. The current loss is:  0.9974204897880554\n",
      "Warning: nan gradient found. The current loss is:  0.3632007837295532\n",
      "Warning: nan gradient found. The current loss is:  0.6981812119483948\n",
      "Warning: nan gradient found. The current loss is:  0.8608916997909546\n",
      "Warning: nan gradient found. The current loss is:  0.5246930122375488\n",
      "Warning: nan gradient found. The current loss is:  0.21897739171981812\n",
      "Warning: nan gradient found. The current loss is:  0.9962390661239624\n",
      "Warning: nan gradient found. The current loss is:  0.9063518047332764\n",
      "Warning: nan gradient found. The current loss is:  0.8598807454109192\n",
      "Warning: nan gradient found. The current loss is:  -0.012178812175989151\n",
      "Warning: nan gradient found. The current loss is:  -0.07663988322019577\n",
      "Warning: nan gradient found. The current loss is:  0.5435940623283386\n",
      "Warning: nan gradient found. The current loss is:  0.0916982889175415\n",
      "Warning: nan gradient found. The current loss is:  1.4614121913909912\n",
      "Warning: nan gradient found. The current loss is:  1.3249695301055908\n",
      "Warning: nan gradient found. The current loss is:  0.2870510518550873\n",
      "Warning: nan gradient found. The current loss is:  0.8447988629341125\n",
      "Warning: nan gradient found. The current loss is:  0.09499083459377289\n",
      "Warning: nan gradient found. The current loss is:  0.4194427728652954\n",
      "Warning: nan gradient found. The current loss is:  0.8241060972213745\n",
      "Warning: nan gradient found. The current loss is:  0.4152182340621948\n",
      "Warning: nan gradient found. The current loss is:  0.8197163343429565\n",
      "Warning: nan gradient found. The current loss is:  1.118349313735962\n",
      "Warning: nan gradient found. The current loss is:  0.1961992084980011\n",
      "Warning: nan gradient found. The current loss is:  0.9292081594467163\n",
      "Warning: nan gradient found. The current loss is:  0.5400983691215515\n",
      "Warning: nan gradient found. The current loss is:  0.2905116081237793\n",
      "Warning: nan gradient found. The current loss is:  0.2651946544647217\n",
      "Warning: nan gradient found. The current loss is:  0.24973571300506592\n",
      "Warning: nan gradient found. The current loss is:  0.9381365776062012\n",
      "Warning: nan gradient found. The current loss is:  0.5943417549133301\n",
      "Warning: nan gradient found. The current loss is:  0.7419233918190002\n",
      "Warning: nan gradient found. The current loss is:  0.3872681260108948\n",
      "Warning: nan gradient found. The current loss is:  0.47114238142967224\n",
      "Warning: nan gradient found. The current loss is:  0.2419963926076889\n",
      "Warning: nan gradient found. The current loss is:  0.6646772027015686\n",
      "Warning: nan gradient found. The current loss is:  1.0249236822128296\n",
      "Warning: nan gradient found. The current loss is:  0.9031623005867004\n",
      "Warning: nan gradient found. The current loss is:  0.5965050458908081\n",
      "Warning: nan gradient found. The current loss is:  1.2097344398498535\n",
      "Warning: nan gradient found. The current loss is:  0.8613342642784119\n",
      "Warning: nan gradient found. The current loss is:  1.4736963510513306\n",
      "Warning: nan gradient found. The current loss is:  1.4039047956466675\n",
      "Warning: nan gradient found. The current loss is:  0.6826029419898987\n",
      "Warning: nan gradient found. The current loss is:  0.3176509141921997\n",
      "Warning: nan gradient found. The current loss is:  0.4898701310157776\n",
      "Warning: nan gradient found. The current loss is:  0.36608827114105225\n",
      "Warning: nan gradient found. The current loss is:  0.29618507623672485\n",
      "Warning: nan gradient found. The current loss is:  0.16770654916763306\n",
      "Warning: nan gradient found. The current loss is:  0.27382582426071167\n",
      "Warning: nan gradient found. The current loss is:  1.2616305351257324\n",
      "Warning: nan gradient found. The current loss is:  0.1738920509815216\n",
      "Warning: nan gradient found. The current loss is:  0.26177704334259033\n",
      "Warning: nan gradient found. The current loss is:  0.40828073024749756\n",
      "Warning: nan gradient found. The current loss is:  0.34434837102890015\n",
      "Warning: nan gradient found. The current loss is:  1.1375696659088135\n",
      "Warning: nan gradient found. The current loss is:  0.1430513560771942\n",
      "Warning: nan gradient found. The current loss is:  0.2970413565635681\n",
      "Warning: nan gradient found. The current loss is:  0.2197539359331131\n",
      "Warning: nan gradient found. The current loss is:  0.6069025993347168\n",
      "Warning: nan gradient found. The current loss is:  0.4402225911617279\n",
      "Warning: nan gradient found. The current loss is:  0.37913504242897034\n",
      "Warning: nan gradient found. The current loss is:  0.8051798343658447\n",
      "Warning: nan gradient found. The current loss is:  0.39797791838645935\n",
      "Warning: nan gradient found. The current loss is:  0.2133994698524475\n",
      "Warning: nan gradient found. The current loss is:  0.5979416966438293\n",
      "Warning: nan gradient found. The current loss is:  0.7613693475723267\n",
      "Warning: nan gradient found. The current loss is:  0.794124186038971\n",
      "Warning: nan gradient found. The current loss is:  0.5429676175117493\n",
      "Warning: nan gradient found. The current loss is:  0.5865819454193115\n",
      "Warning: nan gradient found. The current loss is:  0.3827309012413025\n",
      "Warning: nan gradient found. The current loss is:  0.548428475856781\n",
      "Warning: nan gradient found. The current loss is:  0.23716625571250916\n",
      "Warning: nan gradient found. The current loss is:  0.5558521151542664\n",
      "Warning: nan gradient found. The current loss is:  0.7787054777145386\n",
      "Warning: nan gradient found. The current loss is:  0.740350067615509\n",
      "Warning: nan gradient found. The current loss is:  -0.0042364709079265594\n",
      "Warning: nan gradient found. The current loss is:  0.6810876131057739\n",
      "Warning: nan gradient found. The current loss is:  0.37294501066207886\n",
      "Warning: nan gradient found. The current loss is:  0.406390517950058\n",
      "Warning: nan gradient found. The current loss is:  0.5751630067825317\n",
      "Current batch training loss: 0.575163  [2099200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7884162068367004\n",
      "Warning: nan gradient found. The current loss is:  0.9005234241485596\n",
      "Warning: nan gradient found. The current loss is:  0.4286458492279053\n",
      "Warning: nan gradient found. The current loss is:  0.48438143730163574\n",
      "Warning: nan gradient found. The current loss is:  0.596611738204956\n",
      "Warning: nan gradient found. The current loss is:  0.4671769142150879\n",
      "Warning: nan gradient found. The current loss is:  0.33770424127578735\n",
      "Warning: nan gradient found. The current loss is:  0.8337372541427612\n",
      "Warning: nan gradient found. The current loss is:  0.2774249315261841\n",
      "Warning: nan gradient found. The current loss is:  0.8669738173484802\n",
      "Warning: nan gradient found. The current loss is:  0.5344717502593994\n",
      "Warning: nan gradient found. The current loss is:  0.2201506495475769\n",
      "Warning: nan gradient found. The current loss is:  0.5878534317016602\n",
      "Warning: nan gradient found. The current loss is:  0.4026027321815491\n",
      "Warning: nan gradient found. The current loss is:  0.7239367961883545\n",
      "Warning: nan gradient found. The current loss is:  0.5834029912948608\n",
      "Warning: nan gradient found. The current loss is:  0.9520752429962158\n",
      "Warning: nan gradient found. The current loss is:  0.8449699878692627\n",
      "Warning: nan gradient found. The current loss is:  0.6839281320571899\n",
      "Warning: nan gradient found. The current loss is:  0.7444828748703003\n",
      "Warning: nan gradient found. The current loss is:  0.809282124042511\n",
      "Warning: nan gradient found. The current loss is:  0.22480684518814087\n",
      "Warning: nan gradient found. The current loss is:  0.13045558333396912\n",
      "Warning: nan gradient found. The current loss is:  0.4915652871131897\n",
      "Warning: nan gradient found. The current loss is:  0.8490098714828491\n",
      "Warning: nan gradient found. The current loss is:  0.1725720316171646\n",
      "Warning: nan gradient found. The current loss is:  0.5572284460067749\n",
      "Warning: nan gradient found. The current loss is:  0.0947163850069046\n",
      "Warning: nan gradient found. The current loss is:  0.9420980215072632\n",
      "Warning: nan gradient found. The current loss is:  0.12874549627304077\n",
      "Warning: nan gradient found. The current loss is:  0.5616048574447632\n",
      "Warning: nan gradient found. The current loss is:  0.5357481837272644\n",
      "Warning: nan gradient found. The current loss is:  0.2951115369796753\n",
      "Warning: nan gradient found. The current loss is:  0.2749631106853485\n",
      "Warning: nan gradient found. The current loss is:  0.7590792179107666\n",
      "Warning: nan gradient found. The current loss is:  0.37305396795272827\n",
      "Warning: nan gradient found. The current loss is:  0.9163345098495483\n",
      "Warning: nan gradient found. The current loss is:  0.30591800808906555\n",
      "Warning: nan gradient found. The current loss is:  0.2386452555656433\n",
      "Warning: nan gradient found. The current loss is:  0.265935480594635\n",
      "Warning: nan gradient found. The current loss is:  0.7041606903076172\n",
      "Warning: nan gradient found. The current loss is:  0.5601973533630371\n",
      "Warning: nan gradient found. The current loss is:  0.05867234617471695\n",
      "Warning: nan gradient found. The current loss is:  0.15562030673027039\n",
      "Warning: nan gradient found. The current loss is:  0.6748538613319397\n",
      "Warning: nan gradient found. The current loss is:  0.405780553817749\n",
      "Warning: nan gradient found. The current loss is:  0.856293797492981\n",
      "Warning: nan gradient found. The current loss is:  0.20597553253173828\n",
      "Warning: nan gradient found. The current loss is:  0.3954203128814697\n",
      "Warning: nan gradient found. The current loss is:  0.16500918567180634\n",
      "Warning: nan gradient found. The current loss is:  0.8344730734825134\n",
      "Warning: nan gradient found. The current loss is:  0.6814661622047424\n",
      "Warning: nan gradient found. The current loss is:  0.953779399394989\n",
      "Warning: nan gradient found. The current loss is:  0.5668619871139526\n",
      "Warning: nan gradient found. The current loss is:  0.5095579624176025\n",
      "Warning: nan gradient found. The current loss is:  0.39790448546409607\n",
      "Warning: nan gradient found. The current loss is:  0.9857827425003052\n",
      "Warning: nan gradient found. The current loss is:  0.475469172000885\n",
      "Warning: nan gradient found. The current loss is:  0.3412395715713501\n",
      "Warning: nan gradient found. The current loss is:  0.4443453252315521\n",
      "Warning: nan gradient found. The current loss is:  0.8213605284690857\n",
      "Warning: nan gradient found. The current loss is:  0.9089261889457703\n",
      "Warning: nan gradient found. The current loss is:  0.5065532326698303\n",
      "Warning: nan gradient found. The current loss is:  1.0806025266647339\n",
      "Warning: nan gradient found. The current loss is:  0.7255496382713318\n",
      "Warning: nan gradient found. The current loss is:  0.4519786536693573\n",
      "Warning: nan gradient found. The current loss is:  0.3883572816848755\n",
      "Warning: nan gradient found. The current loss is:  0.795082151889801\n",
      "Warning: nan gradient found. The current loss is:  0.41160255670547485\n",
      "Warning: nan gradient found. The current loss is:  0.4545394778251648\n",
      "Warning: nan gradient found. The current loss is:  0.5158416628837585\n",
      "Warning: nan gradient found. The current loss is:  0.6708282232284546\n",
      "Warning: nan gradient found. The current loss is:  0.0279524102807045\n",
      "Warning: nan gradient found. The current loss is:  1.0028427839279175\n",
      "Warning: nan gradient found. The current loss is:  0.20064087212085724\n",
      "Warning: nan gradient found. The current loss is:  1.2026714086532593\n",
      "Warning: nan gradient found. The current loss is:  0.6580231189727783\n",
      "Warning: nan gradient found. The current loss is:  1.9349441528320312\n",
      "Warning: nan gradient found. The current loss is:  0.7259637713432312\n",
      "Warning: nan gradient found. The current loss is:  0.7333403825759888\n",
      "Warning: nan gradient found. The current loss is:  0.9676662683486938\n",
      "Warning: nan gradient found. The current loss is:  0.4270840287208557\n",
      "Warning: nan gradient found. The current loss is:  0.5124745965003967\n",
      "Warning: nan gradient found. The current loss is:  0.505759060382843\n",
      "Warning: nan gradient found. The current loss is:  0.8320110440254211\n",
      "Warning: nan gradient found. The current loss is:  0.6442156434059143\n",
      "Warning: nan gradient found. The current loss is:  0.6784578561782837\n",
      "Warning: nan gradient found. The current loss is:  0.2969692349433899\n",
      "Warning: nan gradient found. The current loss is:  0.6164659261703491\n",
      "Warning: nan gradient found. The current loss is:  0.6467691659927368\n",
      "Warning: nan gradient found. The current loss is:  0.855226993560791\n",
      "Warning: nan gradient found. The current loss is:  0.7530372738838196\n",
      "Warning: nan gradient found. The current loss is:  1.2502816915512085\n",
      "Warning: nan gradient found. The current loss is:  0.9908682107925415\n",
      "Warning: nan gradient found. The current loss is:  0.41249024868011475\n",
      "Warning: nan gradient found. The current loss is:  0.21456515789031982\n",
      "Warning: nan gradient found. The current loss is:  0.7131174206733704\n",
      "Warning: nan gradient found. The current loss is:  0.44174307584762573\n",
      "Warning: nan gradient found. The current loss is:  1.263610601425171\n",
      "Warning: nan gradient found. The current loss is:  0.5843843221664429\n",
      "Current batch training loss: 0.584384  [2124800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9764804244041443\n",
      "Warning: nan gradient found. The current loss is:  0.3083643913269043\n",
      "Warning: nan gradient found. The current loss is:  0.6142392158508301\n",
      "Warning: nan gradient found. The current loss is:  0.6722318530082703\n",
      "Warning: nan gradient found. The current loss is:  0.884484589099884\n",
      "Warning: nan gradient found. The current loss is:  0.8196812868118286\n",
      "Warning: nan gradient found. The current loss is:  0.22448739409446716\n",
      "Warning: nan gradient found. The current loss is:  0.5823452472686768\n",
      "Warning: nan gradient found. The current loss is:  1.5751044750213623\n",
      "Warning: nan gradient found. The current loss is:  0.4770253002643585\n",
      "Warning: nan gradient found. The current loss is:  0.11928664147853851\n",
      "Warning: nan gradient found. The current loss is:  0.7900229692459106\n",
      "Warning: nan gradient found. The current loss is:  0.23519715666770935\n",
      "Warning: nan gradient found. The current loss is:  0.11841502785682678\n",
      "Warning: nan gradient found. The current loss is:  0.31631654500961304\n",
      "Warning: nan gradient found. The current loss is:  0.37192636728286743\n",
      "Warning: nan gradient found. The current loss is:  0.3429392874240875\n",
      "Warning: nan gradient found. The current loss is:  0.8116288185119629\n",
      "Warning: nan gradient found. The current loss is:  0.4832853674888611\n",
      "Warning: nan gradient found. The current loss is:  1.359675645828247\n",
      "Warning: nan gradient found. The current loss is:  0.9948054552078247\n",
      "Warning: nan gradient found. The current loss is:  0.6508573293685913\n",
      "Warning: nan gradient found. The current loss is:  0.5103583931922913\n",
      "Warning: nan gradient found. The current loss is:  0.20333227515220642\n",
      "Warning: nan gradient found. The current loss is:  0.9381704926490784\n",
      "Warning: nan gradient found. The current loss is:  0.6771405935287476\n",
      "Warning: nan gradient found. The current loss is:  0.7262120246887207\n",
      "Warning: nan gradient found. The current loss is:  2.079028367996216\n",
      "Warning: nan gradient found. The current loss is:  0.22028428316116333\n",
      "Warning: nan gradient found. The current loss is:  0.3870636224746704\n",
      "Warning: nan gradient found. The current loss is:  0.5341967940330505\n",
      "Warning: nan gradient found. The current loss is:  1.740904688835144\n",
      "Warning: nan gradient found. The current loss is:  0.7042429447174072\n",
      "Warning: nan gradient found. The current loss is:  0.14571762084960938\n",
      "Warning: nan gradient found. The current loss is:  0.531690776348114\n",
      "Warning: nan gradient found. The current loss is:  0.8434218168258667\n",
      "Warning: nan gradient found. The current loss is:  0.23215846717357635\n",
      "Warning: nan gradient found. The current loss is:  0.7505122423171997\n",
      "Warning: nan gradient found. The current loss is:  0.5628297328948975\n",
      "Warning: nan gradient found. The current loss is:  1.0956145524978638\n",
      "Warning: nan gradient found. The current loss is:  1.5542194843292236\n",
      "Warning: nan gradient found. The current loss is:  0.5314463376998901\n",
      "Warning: nan gradient found. The current loss is:  0.8405693769454956\n",
      "Warning: nan gradient found. The current loss is:  1.1319313049316406\n",
      "Warning: nan gradient found. The current loss is:  0.6497191190719604\n",
      "Warning: nan gradient found. The current loss is:  0.43731793761253357\n",
      "Warning: nan gradient found. The current loss is:  1.1416199207305908\n",
      "Warning: nan gradient found. The current loss is:  0.9092487692832947\n",
      "Warning: nan gradient found. The current loss is:  0.275152325630188\n",
      "Warning: nan gradient found. The current loss is:  0.8102330565452576\n",
      "Warning: nan gradient found. The current loss is:  0.10188698768615723\n",
      "Warning: nan gradient found. The current loss is:  0.3134346008300781\n",
      "Warning: nan gradient found. The current loss is:  0.22917284071445465\n",
      "Warning: nan gradient found. The current loss is:  0.4524286091327667\n",
      "Warning: nan gradient found. The current loss is:  0.44834280014038086\n",
      "Warning: nan gradient found. The current loss is:  0.8071977496147156\n",
      "Warning: nan gradient found. The current loss is:  0.6317527294158936\n",
      "Warning: nan gradient found. The current loss is:  0.5645333528518677\n",
      "Warning: nan gradient found. The current loss is:  0.21326804161071777\n",
      "Warning: nan gradient found. The current loss is:  0.3136736750602722\n",
      "Warning: nan gradient found. The current loss is:  0.4272487759590149\n",
      "Warning: nan gradient found. The current loss is:  0.6253541707992554\n",
      "Warning: nan gradient found. The current loss is:  0.5910285711288452\n",
      "Warning: nan gradient found. The current loss is:  0.5895325541496277\n",
      "Warning: nan gradient found. The current loss is:  0.2782577872276306\n",
      "Warning: nan gradient found. The current loss is:  0.4268975257873535\n",
      "Warning: nan gradient found. The current loss is:  0.4085795283317566\n",
      "Warning: nan gradient found. The current loss is:  0.6605439782142639\n",
      "Warning: nan gradient found. The current loss is:  0.6001051068305969\n",
      "Warning: nan gradient found. The current loss is:  0.3482603430747986\n",
      "Warning: nan gradient found. The current loss is:  0.4867791533470154\n",
      "Warning: nan gradient found. The current loss is:  0.5138707160949707\n",
      "Warning: nan gradient found. The current loss is:  -0.04447736218571663\n",
      "Warning: nan gradient found. The current loss is:  0.8517929911613464\n",
      "Warning: nan gradient found. The current loss is:  0.27578145265579224\n",
      "Warning: nan gradient found. The current loss is:  0.008009038865566254\n",
      "Warning: nan gradient found. The current loss is:  0.7444630265235901\n",
      "Warning: nan gradient found. The current loss is:  0.2552374005317688\n",
      "Warning: nan gradient found. The current loss is:  0.3833412230014801\n",
      "Warning: nan gradient found. The current loss is:  0.6810835599899292\n",
      "Warning: nan gradient found. The current loss is:  0.4053354859352112\n",
      "Warning: nan gradient found. The current loss is:  0.7516350746154785\n",
      "Warning: nan gradient found. The current loss is:  1.9883735179901123\n",
      "Warning: nan gradient found. The current loss is:  0.5122896432876587\n",
      "Warning: nan gradient found. The current loss is:  0.42980149388313293\n",
      "Warning: nan gradient found. The current loss is:  0.6991311311721802\n",
      "Warning: nan gradient found. The current loss is:  0.4285956621170044\n",
      "Warning: nan gradient found. The current loss is:  1.0563745498657227\n",
      "Warning: nan gradient found. The current loss is:  0.3997102677822113\n",
      "Warning: nan gradient found. The current loss is:  0.9219537973403931\n",
      "Warning: nan gradient found. The current loss is:  0.4210498034954071\n",
      "Warning: nan gradient found. The current loss is:  0.4516165256500244\n",
      "Warning: nan gradient found. The current loss is:  0.6765966415405273\n",
      "Warning: nan gradient found. The current loss is:  0.4449714422225952\n",
      "Warning: nan gradient found. The current loss is:  0.47627323865890503\n",
      "Warning: nan gradient found. The current loss is:  0.2410265952348709\n",
      "Warning: nan gradient found. The current loss is:  0.6586488485336304\n",
      "Warning: nan gradient found. The current loss is:  0.4111504852771759\n",
      "Warning: nan gradient found. The current loss is:  0.8323415517807007\n",
      "Warning: nan gradient found. The current loss is:  0.4444133937358856\n",
      "Current batch training loss: 0.444413  [2150400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6623449325561523\n",
      "Warning: nan gradient found. The current loss is:  1.592948079109192\n",
      "Warning: nan gradient found. The current loss is:  0.3800397217273712\n",
      "Warning: nan gradient found. The current loss is:  0.49261993169784546\n",
      "Warning: nan gradient found. The current loss is:  0.29646041989326477\n",
      "Warning: nan gradient found. The current loss is:  0.9153984785079956\n",
      "Warning: nan gradient found. The current loss is:  0.5508705973625183\n",
      "Warning: nan gradient found. The current loss is:  0.5599602460861206\n",
      "Warning: nan gradient found. The current loss is:  0.6653497219085693\n",
      "Warning: nan gradient found. The current loss is:  0.35762307047843933\n",
      "Warning: nan gradient found. The current loss is:  0.3077737092971802\n",
      "Warning: nan gradient found. The current loss is:  0.558491051197052\n",
      "Warning: nan gradient found. The current loss is:  0.2236197292804718\n",
      "Warning: nan gradient found. The current loss is:  0.1860010027885437\n",
      "Warning: nan gradient found. The current loss is:  0.2545085847377777\n",
      "Warning: nan gradient found. The current loss is:  1.133603572845459\n",
      "Warning: nan gradient found. The current loss is:  0.4110635221004486\n",
      "Warning: nan gradient found. The current loss is:  0.7680524587631226\n",
      "Warning: nan gradient found. The current loss is:  0.5434016585350037\n",
      "Warning: nan gradient found. The current loss is:  0.7342851758003235\n",
      "Warning: nan gradient found. The current loss is:  0.3410615026950836\n",
      "Warning: nan gradient found. The current loss is:  0.9679452180862427\n",
      "Warning: nan gradient found. The current loss is:  0.5595069527626038\n",
      "Warning: nan gradient found. The current loss is:  0.5967951416969299\n",
      "Warning: nan gradient found. The current loss is:  1.9019767045974731\n",
      "Warning: nan gradient found. The current loss is:  0.6860699653625488\n",
      "Warning: nan gradient found. The current loss is:  1.4803084135055542\n",
      "Warning: nan gradient found. The current loss is:  0.7092380523681641\n",
      "Warning: nan gradient found. The current loss is:  0.32653510570526123\n",
      "Warning: nan gradient found. The current loss is:  0.546899139881134\n",
      "Warning: nan gradient found. The current loss is:  0.5370250940322876\n",
      "Warning: nan gradient found. The current loss is:  0.7199211120605469\n",
      "Warning: nan gradient found. The current loss is:  0.5292466282844543\n",
      "Warning: nan gradient found. The current loss is:  0.743476927280426\n",
      "Warning: nan gradient found. The current loss is:  1.0873161554336548\n",
      "Warning: nan gradient found. The current loss is:  0.38719552755355835\n",
      "Warning: nan gradient found. The current loss is:  0.2054898738861084\n",
      "Warning: nan gradient found. The current loss is:  1.6157257556915283\n",
      "Warning: nan gradient found. The current loss is:  0.6197720766067505\n",
      "Warning: nan gradient found. The current loss is:  0.13406972587108612\n",
      "Warning: nan gradient found. The current loss is:  0.8409881591796875\n",
      "Warning: nan gradient found. The current loss is:  0.8528896570205688\n",
      "Warning: nan gradient found. The current loss is:  0.3759250044822693\n",
      "Warning: nan gradient found. The current loss is:  0.5790523886680603\n",
      "Warning: nan gradient found. The current loss is:  0.3322566747665405\n",
      "Warning: nan gradient found. The current loss is:  0.6618424654006958\n",
      "Warning: nan gradient found. The current loss is:  0.38695213198661804\n",
      "Warning: nan gradient found. The current loss is:  0.6054625511169434\n",
      "Warning: nan gradient found. The current loss is:  0.5917436480522156\n",
      "Warning: nan gradient found. The current loss is:  0.18571247160434723\n",
      "Warning: nan gradient found. The current loss is:  0.8812687993049622\n",
      "Warning: nan gradient found. The current loss is:  0.3440423309803009\n",
      "Warning: nan gradient found. The current loss is:  0.3818306028842926\n",
      "Warning: nan gradient found. The current loss is:  0.2560412883758545\n",
      "Warning: nan gradient found. The current loss is:  1.503892421722412\n",
      "Warning: nan gradient found. The current loss is:  0.24381348490715027\n",
      "Warning: nan gradient found. The current loss is:  0.7914048433303833\n",
      "Warning: nan gradient found. The current loss is:  1.0474457740783691\n",
      "Warning: nan gradient found. The current loss is:  0.7845224738121033\n",
      "Warning: nan gradient found. The current loss is:  0.4419499635696411\n",
      "Warning: nan gradient found. The current loss is:  0.6489133238792419\n",
      "Warning: nan gradient found. The current loss is:  0.256479412317276\n",
      "Warning: nan gradient found. The current loss is:  0.9049673676490784\n",
      "Warning: nan gradient found. The current loss is:  0.24295710027217865\n",
      "Warning: nan gradient found. The current loss is:  0.34065794944763184\n",
      "Warning: nan gradient found. The current loss is:  0.9383222460746765\n",
      "Warning: nan gradient found. The current loss is:  1.147369384765625\n",
      "Warning: nan gradient found. The current loss is:  0.40800151228904724\n",
      "Warning: nan gradient found. The current loss is:  0.9733467102050781\n",
      "Warning: nan gradient found. The current loss is:  0.26909399032592773\n",
      "Warning: nan gradient found. The current loss is:  0.2689748704433441\n",
      "Warning: nan gradient found. The current loss is:  0.6492432355880737\n",
      "Warning: nan gradient found. The current loss is:  1.6901614665985107\n",
      "Warning: nan gradient found. The current loss is:  0.7709137201309204\n",
      "Warning: nan gradient found. The current loss is:  0.6659501194953918\n",
      "Warning: nan gradient found. The current loss is:  0.18483415246009827\n",
      "Warning: nan gradient found. The current loss is:  1.1186994314193726\n",
      "Warning: nan gradient found. The current loss is:  0.20315834879875183\n",
      "Warning: nan gradient found. The current loss is:  0.766425609588623\n",
      "Warning: nan gradient found. The current loss is:  0.7047193646430969\n",
      "Warning: nan gradient found. The current loss is:  0.963837742805481\n",
      "Warning: nan gradient found. The current loss is:  0.7767751216888428\n",
      "Warning: nan gradient found. The current loss is:  0.5844685435295105\n",
      "Warning: nan gradient found. The current loss is:  0.5835787057876587\n",
      "Warning: nan gradient found. The current loss is:  0.5498566627502441\n",
      "Warning: nan gradient found. The current loss is:  0.484860360622406\n",
      "Warning: nan gradient found. The current loss is:  0.5947138071060181\n",
      "Warning: nan gradient found. The current loss is:  1.4817149639129639\n",
      "Warning: nan gradient found. The current loss is:  0.14340326189994812\n",
      "Warning: nan gradient found. The current loss is:  1.1035346984863281\n",
      "Warning: nan gradient found. The current loss is:  0.693105936050415\n",
      "Warning: nan gradient found. The current loss is:  0.20014379918575287\n",
      "Warning: nan gradient found. The current loss is:  0.30951935052871704\n",
      "Warning: nan gradient found. The current loss is:  0.40434831380844116\n",
      "Warning: nan gradient found. The current loss is:  1.337846279144287\n",
      "Warning: nan gradient found. The current loss is:  0.1865690052509308\n",
      "Warning: nan gradient found. The current loss is:  1.1234103441238403\n",
      "Warning: nan gradient found. The current loss is:  0.7805266976356506\n",
      "Warning: nan gradient found. The current loss is:  0.6694576144218445\n",
      "Warning: nan gradient found. The current loss is:  0.29306522011756897\n",
      "Current batch training loss: 0.293065  [2176000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5292119979858398\n",
      "Warning: nan gradient found. The current loss is:  0.4883737862110138\n",
      "Warning: nan gradient found. The current loss is:  0.6229528188705444\n",
      "Warning: nan gradient found. The current loss is:  0.5789111852645874\n",
      "Warning: nan gradient found. The current loss is:  0.22731296718120575\n",
      "Warning: nan gradient found. The current loss is:  0.14348909258842468\n",
      "Warning: nan gradient found. The current loss is:  0.9179716110229492\n",
      "Warning: nan gradient found. The current loss is:  0.6767123937606812\n",
      "Warning: nan gradient found. The current loss is:  0.6269174218177795\n",
      "Warning: nan gradient found. The current loss is:  0.3898216485977173\n",
      "Warning: nan gradient found. The current loss is:  0.6300473809242249\n",
      "Warning: nan gradient found. The current loss is:  0.1774493008852005\n",
      "Warning: nan gradient found. The current loss is:  0.10610829293727875\n",
      "Warning: nan gradient found. The current loss is:  0.7565775513648987\n",
      "Warning: nan gradient found. The current loss is:  0.5199120044708252\n",
      "Warning: nan gradient found. The current loss is:  0.19125978648662567\n",
      "Warning: nan gradient found. The current loss is:  0.36065489053726196\n",
      "Warning: nan gradient found. The current loss is:  0.004683617502450943\n",
      "Warning: nan gradient found. The current loss is:  0.0700552761554718\n",
      "Warning: nan gradient found. The current loss is:  0.21818570792675018\n",
      "Warning: nan gradient found. The current loss is:  0.4746837615966797\n",
      "Warning: nan gradient found. The current loss is:  0.4665437936782837\n",
      "Warning: nan gradient found. The current loss is:  0.48992919921875\n",
      "Warning: nan gradient found. The current loss is:  0.23099583387374878\n",
      "Warning: nan gradient found. The current loss is:  0.84595787525177\n",
      "Warning: nan gradient found. The current loss is:  1.1367077827453613\n",
      "Warning: nan gradient found. The current loss is:  0.5376763343811035\n",
      "Warning: nan gradient found. The current loss is:  0.8014014363288879\n",
      "Warning: nan gradient found. The current loss is:  0.5754584670066833\n",
      "Warning: nan gradient found. The current loss is:  0.5090857148170471\n",
      "Warning: nan gradient found. The current loss is:  0.08193674683570862\n",
      "Warning: nan gradient found. The current loss is:  0.5767176747322083\n",
      "Warning: nan gradient found. The current loss is:  1.0506970882415771\n",
      "Warning: nan gradient found. The current loss is:  0.8443980813026428\n",
      "Warning: nan gradient found. The current loss is:  1.7658122777938843\n",
      "Warning: nan gradient found. The current loss is:  0.9537558555603027\n",
      "Warning: nan gradient found. The current loss is:  0.18004360795021057\n",
      "Warning: nan gradient found. The current loss is:  0.27542001008987427\n",
      "Warning: nan gradient found. The current loss is:  0.4061589539051056\n",
      "Warning: nan gradient found. The current loss is:  0.5444133281707764\n",
      "Warning: nan gradient found. The current loss is:  0.616840124130249\n",
      "Warning: nan gradient found. The current loss is:  0.3289438486099243\n",
      "Warning: nan gradient found. The current loss is:  1.0598117113113403\n",
      "Warning: nan gradient found. The current loss is:  0.1624552458524704\n",
      "Warning: nan gradient found. The current loss is:  0.6363556385040283\n",
      "Warning: nan gradient found. The current loss is:  0.23711881041526794\n",
      "Warning: nan gradient found. The current loss is:  0.17110851407051086\n",
      "Warning: nan gradient found. The current loss is:  0.5445463061332703\n",
      "Warning: nan gradient found. The current loss is:  0.17152953147888184\n",
      "Warning: nan gradient found. The current loss is:  0.8581395745277405\n",
      "Warning: nan gradient found. The current loss is:  0.3502274751663208\n",
      "Warning: nan gradient found. The current loss is:  1.2918645143508911\n",
      "Warning: nan gradient found. The current loss is:  0.2705542743206024\n",
      "Warning: nan gradient found. The current loss is:  0.718051552772522\n",
      "Warning: nan gradient found. The current loss is:  0.16363677382469177\n",
      "Warning: nan gradient found. The current loss is:  1.0900025367736816\n",
      "Warning: nan gradient found. The current loss is:  0.6052697896957397\n",
      "Warning: nan gradient found. The current loss is:  0.636339545249939\n",
      "Warning: nan gradient found. The current loss is:  0.48854729533195496\n",
      "Warning: nan gradient found. The current loss is:  0.8765386939048767\n",
      "Warning: nan gradient found. The current loss is:  0.5605356097221375\n",
      "Warning: nan gradient found. The current loss is:  0.08540693670511246\n",
      "Warning: nan gradient found. The current loss is:  0.19905993342399597\n",
      "Warning: nan gradient found. The current loss is:  0.8671432137489319\n",
      "Warning: nan gradient found. The current loss is:  0.6113684773445129\n",
      "Warning: nan gradient found. The current loss is:  0.053663063794374466\n",
      "Warning: nan gradient found. The current loss is:  0.38977718353271484\n",
      "Warning: nan gradient found. The current loss is:  0.571614682674408\n",
      "Warning: nan gradient found. The current loss is:  0.8332129716873169\n",
      "Warning: nan gradient found. The current loss is:  0.37131401896476746\n",
      "Warning: nan gradient found. The current loss is:  1.357627034187317\n",
      "Warning: nan gradient found. The current loss is:  1.3065470457077026\n",
      "Warning: nan gradient found. The current loss is:  1.3873116970062256\n",
      "Warning: nan gradient found. The current loss is:  0.566912829875946\n",
      "Warning: nan gradient found. The current loss is:  0.8962559700012207\n",
      "Warning: nan gradient found. The current loss is:  0.42777734994888306\n",
      "Warning: nan gradient found. The current loss is:  0.19393761456012726\n",
      "Warning: nan gradient found. The current loss is:  0.233679860830307\n",
      "Warning: nan gradient found. The current loss is:  0.45759376883506775\n",
      "Warning: nan gradient found. The current loss is:  0.43845078349113464\n",
      "Warning: nan gradient found. The current loss is:  0.22138527035713196\n",
      "Warning: nan gradient found. The current loss is:  0.6090971827507019\n",
      "Warning: nan gradient found. The current loss is:  0.5885621309280396\n",
      "Warning: nan gradient found. The current loss is:  0.3494460880756378\n",
      "Warning: nan gradient found. The current loss is:  0.06606873869895935\n",
      "Warning: nan gradient found. The current loss is:  0.8001440763473511\n",
      "Warning: nan gradient found. The current loss is:  0.7499167919158936\n",
      "Warning: nan gradient found. The current loss is:  0.30955466628074646\n",
      "Warning: nan gradient found. The current loss is:  0.9725325703620911\n",
      "Warning: nan gradient found. The current loss is:  0.6298456788063049\n",
      "Warning: nan gradient found. The current loss is:  0.5512570142745972\n",
      "Warning: nan gradient found. The current loss is:  0.8878267407417297\n",
      "Warning: nan gradient found. The current loss is:  0.3142361342906952\n",
      "Warning: nan gradient found. The current loss is:  0.24498972296714783\n",
      "Warning: nan gradient found. The current loss is:  0.9080506563186646\n",
      "Warning: nan gradient found. The current loss is:  0.591461718082428\n",
      "Warning: nan gradient found. The current loss is:  0.47013938426971436\n",
      "Warning: nan gradient found. The current loss is:  0.3670511245727539\n",
      "Warning: nan gradient found. The current loss is:  0.2403971403837204\n",
      "Warning: nan gradient found. The current loss is:  0.3291485905647278\n",
      "Current batch training loss: 0.329149  [2201600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.418556272983551\n",
      "Warning: nan gradient found. The current loss is:  0.49361735582351685\n",
      "Warning: nan gradient found. The current loss is:  0.392683207988739\n",
      "Warning: nan gradient found. The current loss is:  0.17216549813747406\n",
      "Warning: nan gradient found. The current loss is:  0.40270334482192993\n",
      "Warning: nan gradient found. The current loss is:  0.7138965129852295\n",
      "Warning: nan gradient found. The current loss is:  0.5560251474380493\n",
      "Warning: nan gradient found. The current loss is:  0.744519829750061\n",
      "Warning: nan gradient found. The current loss is:  0.4289185106754303\n",
      "Warning: nan gradient found. The current loss is:  0.4635404050350189\n",
      "Warning: nan gradient found. The current loss is:  0.8902800679206848\n",
      "Warning: nan gradient found. The current loss is:  0.9309076070785522\n",
      "Warning: nan gradient found. The current loss is:  0.7728170156478882\n",
      "Warning: nan gradient found. The current loss is:  0.24965430796146393\n",
      "Warning: nan gradient found. The current loss is:  0.682959258556366\n",
      "Warning: nan gradient found. The current loss is:  0.3967552185058594\n",
      "Warning: nan gradient found. The current loss is:  0.3534931540489197\n",
      "Warning: nan gradient found. The current loss is:  0.5118216276168823\n",
      "Warning: nan gradient found. The current loss is:  0.3614513874053955\n",
      "Warning: nan gradient found. The current loss is:  0.6422590017318726\n",
      "Warning: nan gradient found. The current loss is:  0.9105650186538696\n",
      "Warning: nan gradient found. The current loss is:  0.39286044239997864\n",
      "Warning: nan gradient found. The current loss is:  0.8761626482009888\n",
      "Warning: nan gradient found. The current loss is:  0.6851942539215088\n",
      "Warning: nan gradient found. The current loss is:  0.2368917316198349\n",
      "Warning: nan gradient found. The current loss is:  0.6333832144737244\n",
      "Warning: nan gradient found. The current loss is:  0.15812963247299194\n",
      "Warning: nan gradient found. The current loss is:  0.9865201711654663\n",
      "Warning: nan gradient found. The current loss is:  0.8284809589385986\n",
      "Warning: nan gradient found. The current loss is:  0.8009124398231506\n",
      "Warning: nan gradient found. The current loss is:  0.2658793032169342\n",
      "Warning: nan gradient found. The current loss is:  0.5431594848632812\n",
      "Warning: nan gradient found. The current loss is:  0.5539148449897766\n",
      "Warning: nan gradient found. The current loss is:  0.08123825490474701\n",
      "Warning: nan gradient found. The current loss is:  0.3196004629135132\n",
      "Warning: nan gradient found. The current loss is:  0.40565627813339233\n",
      "Warning: nan gradient found. The current loss is:  0.7077155709266663\n",
      "Warning: nan gradient found. The current loss is:  0.3192354440689087\n",
      "Warning: nan gradient found. The current loss is:  0.3668178617954254\n",
      "Warning: nan gradient found. The current loss is:  0.43824803829193115\n",
      "Warning: nan gradient found. The current loss is:  0.712771475315094\n",
      "Warning: nan gradient found. The current loss is:  0.4866345524787903\n",
      "Warning: nan gradient found. The current loss is:  0.3911571502685547\n",
      "Warning: nan gradient found. The current loss is:  0.9546111822128296\n",
      "Warning: nan gradient found. The current loss is:  0.20848968625068665\n",
      "Warning: nan gradient found. The current loss is:  0.4829849600791931\n",
      "Training loss: 0.415052\n",
      "Validation loss: 0.755441 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Warning: nan gradient found. The current loss is:  0.4560803771018982\n",
      "Current batch training loss: 0.456080  [    0/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2746114432811737\n",
      "Warning: nan gradient found. The current loss is:  0.6017366647720337\n",
      "Warning: nan gradient found. The current loss is:  0.43913209438323975\n",
      "Warning: nan gradient found. The current loss is:  0.3336578905582428\n",
      "Warning: nan gradient found. The current loss is:  0.9982653260231018\n",
      "Warning: nan gradient found. The current loss is:  0.6185262203216553\n",
      "Warning: nan gradient found. The current loss is:  0.5070756077766418\n",
      "Warning: nan gradient found. The current loss is:  0.3856440782546997\n",
      "Warning: nan gradient found. The current loss is:  0.4758318066596985\n",
      "Warning: nan gradient found. The current loss is:  0.2852213382720947\n",
      "Warning: nan gradient found. The current loss is:  0.6839078068733215\n",
      "Warning: nan gradient found. The current loss is:  0.9120608568191528\n",
      "Warning: nan gradient found. The current loss is:  0.3352341651916504\n",
      "Warning: nan gradient found. The current loss is:  -0.01970979943871498\n",
      "Warning: nan gradient found. The current loss is:  0.6355098485946655\n",
      "Warning: nan gradient found. The current loss is:  0.4089681804180145\n",
      "Warning: nan gradient found. The current loss is:  0.5048942565917969\n",
      "Warning: nan gradient found. The current loss is:  0.22812522947788239\n",
      "Warning: nan gradient found. The current loss is:  0.5896965861320496\n",
      "Warning: nan gradient found. The current loss is:  0.8618350625038147\n",
      "Warning: nan gradient found. The current loss is:  0.5007953643798828\n",
      "Warning: nan gradient found. The current loss is:  1.1548523902893066\n",
      "Warning: nan gradient found. The current loss is:  0.021713703870773315\n",
      "Warning: nan gradient found. The current loss is:  0.14557698369026184\n",
      "Warning: nan gradient found. The current loss is:  0.680614709854126\n",
      "Warning: nan gradient found. The current loss is:  1.2064213752746582\n",
      "Warning: nan gradient found. The current loss is:  0.3201513886451721\n",
      "Warning: nan gradient found. The current loss is:  1.1596455574035645\n",
      "Warning: nan gradient found. The current loss is:  0.32822975516319275\n",
      "Warning: nan gradient found. The current loss is:  0.6105021238327026\n",
      "Warning: nan gradient found. The current loss is:  0.867461085319519\n",
      "Warning: nan gradient found. The current loss is:  0.5078994035720825\n",
      "Warning: nan gradient found. The current loss is:  0.635546863079071\n",
      "Warning: nan gradient found. The current loss is:  0.9232697486877441\n",
      "Warning: nan gradient found. The current loss is:  0.42753514647483826\n",
      "Warning: nan gradient found. The current loss is:  0.8136736154556274\n",
      "Warning: nan gradient found. The current loss is:  0.5891956090927124\n",
      "Warning: nan gradient found. The current loss is:  2.210530996322632\n",
      "Warning: nan gradient found. The current loss is:  0.2544957399368286\n",
      "Warning: nan gradient found. The current loss is:  0.9549251198768616\n",
      "Warning: nan gradient found. The current loss is:  0.11198091506958008\n",
      "Warning: nan gradient found. The current loss is:  1.237608551979065\n",
      "Warning: nan gradient found. The current loss is:  0.3688125014305115\n",
      "Warning: nan gradient found. The current loss is:  0.173517644405365\n",
      "Warning: nan gradient found. The current loss is:  0.22854377329349518\n",
      "Warning: nan gradient found. The current loss is:  0.8792561292648315\n",
      "Warning: nan gradient found. The current loss is:  0.518173098564148\n",
      "Warning: nan gradient found. The current loss is:  0.5862056016921997\n",
      "Warning: nan gradient found. The current loss is:  0.24183660745620728\n",
      "Warning: nan gradient found. The current loss is:  0.8947480916976929\n",
      "Warning: nan gradient found. The current loss is:  0.7283501625061035\n",
      "Warning: nan gradient found. The current loss is:  0.8139730095863342\n",
      "Warning: nan gradient found. The current loss is:  0.6729453206062317\n",
      "Warning: nan gradient found. The current loss is:  0.23400084674358368\n",
      "Warning: nan gradient found. The current loss is:  0.3260098099708557\n",
      "Warning: nan gradient found. The current loss is:  0.09372235834598541\n",
      "Warning: nan gradient found. The current loss is:  0.5102862119674683\n",
      "Warning: nan gradient found. The current loss is:  0.7772167921066284\n",
      "Warning: nan gradient found. The current loss is:  0.5219365358352661\n",
      "Warning: nan gradient found. The current loss is:  0.5824911594390869\n",
      "Warning: nan gradient found. The current loss is:  1.3644704818725586\n",
      "Warning: nan gradient found. The current loss is:  1.214841365814209\n",
      "Warning: nan gradient found. The current loss is:  0.3045349419116974\n",
      "Warning: nan gradient found. The current loss is:  0.8586829900741577\n",
      "Warning: nan gradient found. The current loss is:  0.4009886682033539\n",
      "Warning: nan gradient found. The current loss is:  0.3865760862827301\n",
      "Warning: nan gradient found. The current loss is:  0.5034992694854736\n",
      "Warning: nan gradient found. The current loss is:  0.6632571220397949\n",
      "Warning: nan gradient found. The current loss is:  0.5829241275787354\n",
      "Warning: nan gradient found. The current loss is:  0.8641719818115234\n",
      "Warning: nan gradient found. The current loss is:  0.38552477955818176\n",
      "Warning: nan gradient found. The current loss is:  0.6566200256347656\n",
      "Warning: nan gradient found. The current loss is:  1.6915916204452515\n",
      "Warning: nan gradient found. The current loss is:  0.35347259044647217\n",
      "Warning: nan gradient found. The current loss is:  0.40532010793685913\n",
      "Warning: nan gradient found. The current loss is:  0.5144988298416138\n",
      "Warning: nan gradient found. The current loss is:  0.1502237319946289\n",
      "Warning: nan gradient found. The current loss is:  0.6963469982147217\n",
      "Warning: nan gradient found. The current loss is:  0.5815412998199463\n",
      "Warning: nan gradient found. The current loss is:  0.5437926054000854\n",
      "Warning: nan gradient found. The current loss is:  0.861696720123291\n",
      "Warning: nan gradient found. The current loss is:  0.909062385559082\n",
      "Warning: nan gradient found. The current loss is:  0.27551066875457764\n",
      "Warning: nan gradient found. The current loss is:  0.5626434087753296\n",
      "Warning: nan gradient found. The current loss is:  0.6611705422401428\n",
      "Warning: nan gradient found. The current loss is:  0.349184513092041\n",
      "Warning: nan gradient found. The current loss is:  0.3297384977340698\n",
      "Warning: nan gradient found. The current loss is:  0.08807392418384552\n",
      "Warning: nan gradient found. The current loss is:  0.596980631351471\n",
      "Warning: nan gradient found. The current loss is:  0.0159616656601429\n",
      "Warning: nan gradient found. The current loss is:  0.413656622171402\n",
      "Warning: nan gradient found. The current loss is:  0.40252482891082764\n",
      "Warning: nan gradient found. The current loss is:  1.6340813636779785\n",
      "Warning: nan gradient found. The current loss is:  0.8753252029418945\n",
      "Warning: nan gradient found. The current loss is:  0.7293238639831543\n",
      "Warning: nan gradient found. The current loss is:  0.5466602444648743\n",
      "Warning: nan gradient found. The current loss is:  0.3128383457660675\n",
      "Warning: nan gradient found. The current loss is:  0.40538790822029114\n",
      "Warning: nan gradient found. The current loss is:  0.9114824533462524\n",
      "Warning: nan gradient found. The current loss is:  0.5568740367889404\n",
      "Current batch training loss: 0.556874  [25600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.025134854018688202\n",
      "Warning: nan gradient found. The current loss is:  0.9922361373901367\n",
      "Warning: nan gradient found. The current loss is:  0.1739296317100525\n",
      "Warning: nan gradient found. The current loss is:  0.4083886742591858\n",
      "Warning: nan gradient found. The current loss is:  0.49174949526786804\n",
      "Warning: nan gradient found. The current loss is:  0.37129583954811096\n",
      "Warning: nan gradient found. The current loss is:  0.1711096465587616\n",
      "Warning: nan gradient found. The current loss is:  0.4033898711204529\n",
      "Warning: nan gradient found. The current loss is:  1.183004379272461\n",
      "Warning: nan gradient found. The current loss is:  1.0923203229904175\n",
      "Warning: nan gradient found. The current loss is:  1.015958309173584\n",
      "Warning: nan gradient found. The current loss is:  0.3412310779094696\n",
      "Warning: nan gradient found. The current loss is:  0.9710294008255005\n",
      "Warning: nan gradient found. The current loss is:  0.35273686051368713\n",
      "Warning: nan gradient found. The current loss is:  0.8018258810043335\n",
      "Warning: nan gradient found. The current loss is:  0.7396787405014038\n",
      "Warning: nan gradient found. The current loss is:  0.7654508352279663\n",
      "Warning: nan gradient found. The current loss is:  0.7456054091453552\n",
      "Warning: nan gradient found. The current loss is:  0.2028261125087738\n",
      "Warning: nan gradient found. The current loss is:  0.9871663451194763\n",
      "Warning: nan gradient found. The current loss is:  0.940748929977417\n",
      "Warning: nan gradient found. The current loss is:  0.1661519706249237\n",
      "Warning: nan gradient found. The current loss is:  0.31809768080711365\n",
      "Warning: nan gradient found. The current loss is:  0.7254410982131958\n",
      "Warning: nan gradient found. The current loss is:  0.3225897550582886\n",
      "Warning: nan gradient found. The current loss is:  0.3305325210094452\n",
      "Warning: nan gradient found. The current loss is:  0.22668537497520447\n",
      "Warning: nan gradient found. The current loss is:  0.48079127073287964\n",
      "Warning: nan gradient found. The current loss is:  0.5403918623924255\n",
      "Warning: nan gradient found. The current loss is:  1.005417823791504\n",
      "Warning: nan gradient found. The current loss is:  0.8936922550201416\n",
      "Warning: nan gradient found. The current loss is:  0.5554172396659851\n",
      "Warning: nan gradient found. The current loss is:  0.44191819429397583\n",
      "Warning: nan gradient found. The current loss is:  2.3663198947906494\n",
      "Warning: nan gradient found. The current loss is:  0.5873997211456299\n",
      "Warning: nan gradient found. The current loss is:  0.591055154800415\n",
      "Warning: nan gradient found. The current loss is:  1.3305976390838623\n",
      "Warning: nan gradient found. The current loss is:  0.7363187074661255\n",
      "Warning: nan gradient found. The current loss is:  0.5049877762794495\n",
      "Warning: nan gradient found. The current loss is:  0.6016223430633545\n",
      "Warning: nan gradient found. The current loss is:  0.3095428943634033\n",
      "Warning: nan gradient found. The current loss is:  0.8750694990158081\n",
      "Warning: nan gradient found. The current loss is:  0.3742924928665161\n",
      "Warning: nan gradient found. The current loss is:  0.3368226885795593\n",
      "Warning: nan gradient found. The current loss is:  1.3915603160858154\n",
      "Warning: nan gradient found. The current loss is:  0.24451254308223724\n",
      "Warning: nan gradient found. The current loss is:  1.0460988283157349\n",
      "Warning: nan gradient found. The current loss is:  0.43613147735595703\n",
      "Warning: nan gradient found. The current loss is:  1.0440995693206787\n",
      "Warning: nan gradient found. The current loss is:  0.29869377613067627\n",
      "Warning: nan gradient found. The current loss is:  0.3131897747516632\n",
      "Warning: nan gradient found. The current loss is:  0.6691678166389465\n",
      "Warning: nan gradient found. The current loss is:  0.45202207565307617\n",
      "Warning: nan gradient found. The current loss is:  0.7356857061386108\n",
      "Warning: nan gradient found. The current loss is:  0.47818079590797424\n",
      "Warning: nan gradient found. The current loss is:  1.202500581741333\n",
      "Warning: nan gradient found. The current loss is:  0.27685821056365967\n",
      "Warning: nan gradient found. The current loss is:  0.4754660129547119\n",
      "Warning: nan gradient found. The current loss is:  0.47928884625434875\n",
      "Warning: nan gradient found. The current loss is:  0.47197073698043823\n",
      "Warning: nan gradient found. The current loss is:  0.2415635734796524\n",
      "Warning: nan gradient found. The current loss is:  0.5226409435272217\n",
      "Warning: nan gradient found. The current loss is:  0.2582927346229553\n",
      "Warning: nan gradient found. The current loss is:  1.3220548629760742\n",
      "Warning: nan gradient found. The current loss is:  1.415979266166687\n",
      "Warning: nan gradient found. The current loss is:  1.2653934955596924\n",
      "Warning: nan gradient found. The current loss is:  0.9912099242210388\n",
      "Warning: nan gradient found. The current loss is:  0.8370521664619446\n",
      "Warning: nan gradient found. The current loss is:  0.381612628698349\n",
      "Warning: nan gradient found. The current loss is:  0.18986280262470245\n",
      "Warning: nan gradient found. The current loss is:  0.5325173139572144\n",
      "Warning: nan gradient found. The current loss is:  0.24938395619392395\n",
      "Warning: nan gradient found. The current loss is:  0.825772225856781\n",
      "Warning: nan gradient found. The current loss is:  0.6418927907943726\n",
      "Warning: nan gradient found. The current loss is:  0.589643657207489\n",
      "Warning: nan gradient found. The current loss is:  0.4320022165775299\n",
      "Warning: nan gradient found. The current loss is:  0.3729403614997864\n",
      "Warning: nan gradient found. The current loss is:  0.3816623091697693\n",
      "Warning: nan gradient found. The current loss is:  0.130470871925354\n",
      "Warning: nan gradient found. The current loss is:  0.16105996072292328\n",
      "Warning: nan gradient found. The current loss is:  0.320539653301239\n",
      "Warning: nan gradient found. The current loss is:  0.6455974578857422\n",
      "Warning: nan gradient found. The current loss is:  0.3358764946460724\n",
      "Warning: nan gradient found. The current loss is:  0.6577242612838745\n",
      "Warning: nan gradient found. The current loss is:  0.09437176585197449\n",
      "Warning: nan gradient found. The current loss is:  0.7630691528320312\n",
      "Warning: nan gradient found. The current loss is:  0.6563801765441895\n",
      "Warning: nan gradient found. The current loss is:  0.4684838056564331\n",
      "Warning: nan gradient found. The current loss is:  0.7302247285842896\n",
      "Warning: nan gradient found. The current loss is:  0.8819553852081299\n",
      "Warning: nan gradient found. The current loss is:  0.025953590869903564\n",
      "Warning: nan gradient found. The current loss is:  0.2260628640651703\n",
      "Warning: nan gradient found. The current loss is:  0.1118178442120552\n",
      "Warning: nan gradient found. The current loss is:  1.4588449001312256\n",
      "Warning: nan gradient found. The current loss is:  0.4822465181350708\n",
      "Warning: nan gradient found. The current loss is:  1.0452662706375122\n",
      "Warning: nan gradient found. The current loss is:  0.35497623682022095\n",
      "Warning: nan gradient found. The current loss is:  0.9780548214912415\n",
      "Warning: nan gradient found. The current loss is:  1.0033721923828125\n",
      "Warning: nan gradient found. The current loss is:  0.746264636516571\n",
      "Current batch training loss: 0.746265  [51200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.14198511838912964\n",
      "Warning: nan gradient found. The current loss is:  0.2350616455078125\n",
      "Warning: nan gradient found. The current loss is:  0.18191304802894592\n",
      "Warning: nan gradient found. The current loss is:  0.24558907747268677\n",
      "Warning: nan gradient found. The current loss is:  1.0619580745697021\n",
      "Warning: nan gradient found. The current loss is:  0.6677759885787964\n",
      "Warning: nan gradient found. The current loss is:  1.1707271337509155\n",
      "Warning: nan gradient found. The current loss is:  0.38016369938850403\n",
      "Warning: nan gradient found. The current loss is:  0.3653061091899872\n",
      "Warning: nan gradient found. The current loss is:  1.9202231168746948\n",
      "Warning: nan gradient found. The current loss is:  0.5275231599807739\n",
      "Warning: nan gradient found. The current loss is:  0.362581729888916\n",
      "Warning: nan gradient found. The current loss is:  0.5184929966926575\n",
      "Warning: nan gradient found. The current loss is:  0.7980120182037354\n",
      "Warning: nan gradient found. The current loss is:  0.46464112401008606\n",
      "Warning: nan gradient found. The current loss is:  0.7941427230834961\n",
      "Warning: nan gradient found. The current loss is:  0.6911837458610535\n",
      "Warning: nan gradient found. The current loss is:  0.390506386756897\n",
      "Warning: nan gradient found. The current loss is:  1.3752182722091675\n",
      "Warning: nan gradient found. The current loss is:  1.1303436756134033\n",
      "Warning: nan gradient found. The current loss is:  0.4564410448074341\n",
      "Warning: nan gradient found. The current loss is:  0.16755244135856628\n",
      "Warning: nan gradient found. The current loss is:  0.4072309136390686\n",
      "Warning: nan gradient found. The current loss is:  0.26524677872657776\n",
      "Warning: nan gradient found. The current loss is:  0.919248104095459\n",
      "Warning: nan gradient found. The current loss is:  0.07537639141082764\n",
      "Warning: nan gradient found. The current loss is:  1.1377471685409546\n",
      "Warning: nan gradient found. The current loss is:  0.3821794092655182\n",
      "Warning: nan gradient found. The current loss is:  1.5911471843719482\n",
      "Warning: nan gradient found. The current loss is:  0.16420473158359528\n",
      "Warning: nan gradient found. The current loss is:  0.7704081535339355\n",
      "Warning: nan gradient found. The current loss is:  0.15883782505989075\n",
      "Warning: nan gradient found. The current loss is:  0.33466967940330505\n",
      "Warning: nan gradient found. The current loss is:  0.06997716426849365\n",
      "Warning: nan gradient found. The current loss is:  0.5659960508346558\n",
      "Warning: nan gradient found. The current loss is:  0.315469890832901\n",
      "Warning: nan gradient found. The current loss is:  0.5798773765563965\n",
      "Warning: nan gradient found. The current loss is:  0.6104476451873779\n",
      "Warning: nan gradient found. The current loss is:  1.4620866775512695\n",
      "Warning: nan gradient found. The current loss is:  0.4895417094230652\n",
      "Warning: nan gradient found. The current loss is:  0.7351800203323364\n",
      "Warning: nan gradient found. The current loss is:  0.41536253690719604\n",
      "Warning: nan gradient found. The current loss is:  0.3074236214160919\n",
      "Warning: nan gradient found. The current loss is:  0.6230568289756775\n",
      "Warning: nan gradient found. The current loss is:  0.28185126185417175\n",
      "Warning: nan gradient found. The current loss is:  0.4450376033782959\n",
      "Warning: nan gradient found. The current loss is:  1.1813361644744873\n",
      "Warning: nan gradient found. The current loss is:  0.6257953643798828\n",
      "Warning: nan gradient found. The current loss is:  0.2255917191505432\n",
      "Warning: nan gradient found. The current loss is:  0.35704362392425537\n",
      "Warning: nan gradient found. The current loss is:  0.16236534714698792\n",
      "Warning: nan gradient found. The current loss is:  0.1769343614578247\n",
      "Warning: nan gradient found. The current loss is:  1.7621748447418213\n",
      "Warning: nan gradient found. The current loss is:  0.7251548171043396\n",
      "Warning: nan gradient found. The current loss is:  0.5720969438552856\n",
      "Warning: nan gradient found. The current loss is:  0.4465878903865814\n",
      "Warning: nan gradient found. The current loss is:  0.6747920513153076\n",
      "Warning: nan gradient found. The current loss is:  1.2050707340240479\n",
      "Warning: nan gradient found. The current loss is:  1.799261212348938\n",
      "Warning: nan gradient found. The current loss is:  0.10584507137537003\n",
      "Warning: nan gradient found. The current loss is:  0.3509995639324188\n",
      "Warning: nan gradient found. The current loss is:  0.6633298993110657\n",
      "Warning: nan gradient found. The current loss is:  0.918989360332489\n",
      "Warning: nan gradient found. The current loss is:  0.5312842726707458\n",
      "Warning: nan gradient found. The current loss is:  0.8614439964294434\n",
      "Warning: nan gradient found. The current loss is:  0.2717888653278351\n",
      "Warning: nan gradient found. The current loss is:  0.35672527551651\n",
      "Warning: nan gradient found. The current loss is:  0.16111871600151062\n",
      "Warning: nan gradient found. The current loss is:  0.46840059757232666\n",
      "Warning: nan gradient found. The current loss is:  0.8474814891815186\n",
      "Warning: nan gradient found. The current loss is:  0.348250150680542\n",
      "Warning: nan gradient found. The current loss is:  0.4076339602470398\n",
      "Warning: nan gradient found. The current loss is:  0.3123161792755127\n",
      "Warning: nan gradient found. The current loss is:  0.23547066748142242\n",
      "Warning: nan gradient found. The current loss is:  0.5510034561157227\n",
      "Warning: nan gradient found. The current loss is:  0.3289567232131958\n",
      "Warning: nan gradient found. The current loss is:  0.530891478061676\n",
      "Warning: nan gradient found. The current loss is:  0.5217823386192322\n",
      "Warning: nan gradient found. The current loss is:  0.04392372444272041\n",
      "Warning: nan gradient found. The current loss is:  0.2493424415588379\n",
      "Warning: nan gradient found. The current loss is:  0.34350067377090454\n",
      "Warning: nan gradient found. The current loss is:  0.7599799633026123\n",
      "Warning: nan gradient found. The current loss is:  1.0221076011657715\n",
      "Warning: nan gradient found. The current loss is:  0.8609166145324707\n",
      "Warning: nan gradient found. The current loss is:  0.4688502550125122\n",
      "Warning: nan gradient found. The current loss is:  0.5828933715820312\n",
      "Warning: nan gradient found. The current loss is:  0.7314220666885376\n",
      "Warning: nan gradient found. The current loss is:  0.4203583002090454\n",
      "Warning: nan gradient found. The current loss is:  0.8308926820755005\n",
      "Warning: nan gradient found. The current loss is:  0.35448288917541504\n",
      "Warning: nan gradient found. The current loss is:  0.5005121827125549\n",
      "Warning: nan gradient found. The current loss is:  0.10919979214668274\n",
      "Warning: nan gradient found. The current loss is:  1.3093358278274536\n",
      "Warning: nan gradient found. The current loss is:  0.5448695421218872\n",
      "Warning: nan gradient found. The current loss is:  0.368652880191803\n",
      "Warning: nan gradient found. The current loss is:  1.0082690715789795\n",
      "Warning: nan gradient found. The current loss is:  0.5704395771026611\n",
      "Warning: nan gradient found. The current loss is:  0.11934086680412292\n",
      "Warning: nan gradient found. The current loss is:  0.22328904271125793\n",
      "Warning: nan gradient found. The current loss is:  0.1152990311384201\n",
      "Current batch training loss: 0.115299  [76800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.03279615938663483\n",
      "Warning: nan gradient found. The current loss is:  1.10089111328125\n",
      "Warning: nan gradient found. The current loss is:  1.8734102249145508\n",
      "Warning: nan gradient found. The current loss is:  0.572124719619751\n",
      "Warning: nan gradient found. The current loss is:  0.43262454867362976\n",
      "Warning: nan gradient found. The current loss is:  0.22990962862968445\n",
      "Warning: nan gradient found. The current loss is:  0.5301862359046936\n",
      "Warning: nan gradient found. The current loss is:  0.4506276845932007\n",
      "Warning: nan gradient found. The current loss is:  0.10298452526330948\n",
      "Warning: nan gradient found. The current loss is:  0.40124860405921936\n",
      "Warning: nan gradient found. The current loss is:  0.5405724048614502\n",
      "Warning: nan gradient found. The current loss is:  0.8789665699005127\n",
      "Warning: nan gradient found. The current loss is:  0.6783391237258911\n",
      "Warning: nan gradient found. The current loss is:  0.5269140005111694\n",
      "Warning: nan gradient found. The current loss is:  0.36657214164733887\n",
      "Warning: nan gradient found. The current loss is:  0.4560876786708832\n",
      "Warning: nan gradient found. The current loss is:  0.35509049892425537\n",
      "Warning: nan gradient found. The current loss is:  1.7251731157302856\n",
      "Warning: nan gradient found. The current loss is:  0.43103909492492676\n",
      "Warning: nan gradient found. The current loss is:  0.4425543546676636\n",
      "Warning: nan gradient found. The current loss is:  0.6175565719604492\n",
      "Warning: nan gradient found. The current loss is:  0.5759445428848267\n",
      "Warning: nan gradient found. The current loss is:  0.4290516972541809\n",
      "Warning: nan gradient found. The current loss is:  0.16919392347335815\n",
      "Warning: nan gradient found. The current loss is:  1.3236160278320312\n",
      "Warning: nan gradient found. The current loss is:  0.6000289916992188\n",
      "Warning: nan gradient found. The current loss is:  0.322759747505188\n",
      "Warning: nan gradient found. The current loss is:  0.4112575650215149\n",
      "Warning: nan gradient found. The current loss is:  0.5904238820075989\n",
      "Warning: nan gradient found. The current loss is:  0.590042233467102\n",
      "Warning: nan gradient found. The current loss is:  1.7561761140823364\n",
      "Warning: nan gradient found. The current loss is:  0.27053922414779663\n",
      "Warning: nan gradient found. The current loss is:  0.2855581045150757\n",
      "Warning: nan gradient found. The current loss is:  1.0149303674697876\n",
      "Warning: nan gradient found. The current loss is:  0.4523801803588867\n",
      "Warning: nan gradient found. The current loss is:  0.7500564455986023\n",
      "Warning: nan gradient found. The current loss is:  0.419719934463501\n",
      "Warning: nan gradient found. The current loss is:  1.0532838106155396\n",
      "Warning: nan gradient found. The current loss is:  0.26021596789360046\n",
      "Warning: nan gradient found. The current loss is:  0.8176642656326294\n",
      "Warning: nan gradient found. The current loss is:  0.9448131918907166\n",
      "Warning: nan gradient found. The current loss is:  0.5775182247161865\n",
      "Warning: nan gradient found. The current loss is:  0.8061451315879822\n",
      "Warning: nan gradient found. The current loss is:  0.5193811655044556\n",
      "Warning: nan gradient found. The current loss is:  0.3908993601799011\n",
      "Warning: nan gradient found. The current loss is:  0.39835935831069946\n",
      "Warning: nan gradient found. The current loss is:  0.4261818826198578\n",
      "Warning: nan gradient found. The current loss is:  0.9369875192642212\n",
      "Warning: nan gradient found. The current loss is:  0.7626761198043823\n",
      "Warning: nan gradient found. The current loss is:  0.38383927941322327\n",
      "Warning: nan gradient found. The current loss is:  0.6582174897193909\n",
      "Warning: nan gradient found. The current loss is:  0.15682631731033325\n",
      "Warning: nan gradient found. The current loss is:  0.5445595979690552\n",
      "Warning: nan gradient found. The current loss is:  0.31846457719802856\n",
      "Warning: nan gradient found. The current loss is:  0.5611667037010193\n",
      "Warning: nan gradient found. The current loss is:  0.22699756920337677\n",
      "Warning: nan gradient found. The current loss is:  0.5037139654159546\n",
      "Warning: nan gradient found. The current loss is:  1.023208498954773\n",
      "Warning: nan gradient found. The current loss is:  0.38365820050239563\n",
      "Warning: nan gradient found. The current loss is:  0.3622594475746155\n",
      "Warning: nan gradient found. The current loss is:  0.8311911821365356\n",
      "Warning: nan gradient found. The current loss is:  0.05716007947921753\n",
      "Warning: nan gradient found. The current loss is:  0.33385446667671204\n",
      "Warning: nan gradient found. The current loss is:  1.3993256092071533\n",
      "Warning: nan gradient found. The current loss is:  0.4306013584136963\n",
      "Warning: nan gradient found. The current loss is:  0.5757120847702026\n",
      "Warning: nan gradient found. The current loss is:  0.33058634400367737\n",
      "Warning: nan gradient found. The current loss is:  0.7973695397377014\n",
      "Warning: nan gradient found. The current loss is:  1.171370506286621\n",
      "Warning: nan gradient found. The current loss is:  0.251570463180542\n",
      "Warning: nan gradient found. The current loss is:  0.37519770860671997\n",
      "Warning: nan gradient found. The current loss is:  1.0643386840820312\n",
      "Warning: nan gradient found. The current loss is:  0.1858627051115036\n",
      "Warning: nan gradient found. The current loss is:  0.3376505374908447\n",
      "Warning: nan gradient found. The current loss is:  0.9616286754608154\n",
      "Warning: nan gradient found. The current loss is:  0.5992944836616516\n",
      "Warning: nan gradient found. The current loss is:  0.08262305706739426\n",
      "Warning: nan gradient found. The current loss is:  0.4192758798599243\n",
      "Warning: nan gradient found. The current loss is:  0.40712350606918335\n",
      "Warning: nan gradient found. The current loss is:  0.8100042343139648\n",
      "Warning: nan gradient found. The current loss is:  0.848434567451477\n",
      "Warning: nan gradient found. The current loss is:  0.7506886720657349\n",
      "Warning: nan gradient found. The current loss is:  0.8898282051086426\n",
      "Warning: nan gradient found. The current loss is:  0.41845881938934326\n",
      "Warning: nan gradient found. The current loss is:  0.4268234372138977\n",
      "Warning: nan gradient found. The current loss is:  0.4578458070755005\n",
      "Warning: nan gradient found. The current loss is:  0.2856946587562561\n",
      "Warning: nan gradient found. The current loss is:  0.18056601285934448\n",
      "Warning: nan gradient found. The current loss is:  0.9826256632804871\n",
      "Warning: nan gradient found. The current loss is:  0.26252299547195435\n",
      "Warning: nan gradient found. The current loss is:  0.5227624773979187\n",
      "Warning: nan gradient found. The current loss is:  0.5268427729606628\n",
      "Warning: nan gradient found. The current loss is:  0.17374807596206665\n",
      "Warning: nan gradient found. The current loss is:  0.5180217623710632\n",
      "Warning: nan gradient found. The current loss is:  0.06355857849121094\n",
      "Warning: nan gradient found. The current loss is:  0.22147271037101746\n",
      "Warning: nan gradient found. The current loss is:  0.7437490820884705\n",
      "Warning: nan gradient found. The current loss is:  0.8392516374588013\n",
      "Warning: nan gradient found. The current loss is:  0.8473808169364929\n",
      "Warning: nan gradient found. The current loss is:  0.7170177698135376\n",
      "Current batch training loss: 0.717018  [102400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5756033658981323\n",
      "Warning: nan gradient found. The current loss is:  0.22146214544773102\n",
      "Warning: nan gradient found. The current loss is:  0.8242121338844299\n",
      "Warning: nan gradient found. The current loss is:  0.43767863512039185\n",
      "Warning: nan gradient found. The current loss is:  0.35733112692832947\n",
      "Warning: nan gradient found. The current loss is:  0.37160712480545044\n",
      "Warning: nan gradient found. The current loss is:  0.5839986205101013\n",
      "Warning: nan gradient found. The current loss is:  0.5298766493797302\n",
      "Warning: nan gradient found. The current loss is:  0.83235764503479\n",
      "Warning: nan gradient found. The current loss is:  0.2769194543361664\n",
      "Warning: nan gradient found. The current loss is:  0.4768136739730835\n",
      "Warning: nan gradient found. The current loss is:  0.6725815534591675\n",
      "Warning: nan gradient found. The current loss is:  0.7943553924560547\n",
      "Warning: nan gradient found. The current loss is:  0.32470208406448364\n",
      "Warning: nan gradient found. The current loss is:  0.5339680910110474\n",
      "Warning: nan gradient found. The current loss is:  1.539581060409546\n",
      "Warning: nan gradient found. The current loss is:  0.5650559663772583\n",
      "Warning: nan gradient found. The current loss is:  0.022031862288713455\n",
      "Warning: nan gradient found. The current loss is:  0.8589755296707153\n",
      "Warning: nan gradient found. The current loss is:  0.17244520783424377\n",
      "Warning: nan gradient found. The current loss is:  0.6152798533439636\n",
      "Warning: nan gradient found. The current loss is:  0.2885453402996063\n",
      "Warning: nan gradient found. The current loss is:  0.04454319179058075\n",
      "Warning: nan gradient found. The current loss is:  0.426124632358551\n",
      "Warning: nan gradient found. The current loss is:  0.03803248703479767\n",
      "Warning: nan gradient found. The current loss is:  0.319126695394516\n",
      "Warning: nan gradient found. The current loss is:  0.40198513865470886\n",
      "Warning: nan gradient found. The current loss is:  0.14683681726455688\n",
      "Warning: nan gradient found. The current loss is:  0.611812949180603\n",
      "Warning: nan gradient found. The current loss is:  0.11051677912473679\n",
      "Warning: nan gradient found. The current loss is:  0.37812337279319763\n",
      "Warning: nan gradient found. The current loss is:  1.336677074432373\n",
      "Warning: nan gradient found. The current loss is:  0.6365958452224731\n",
      "Warning: nan gradient found. The current loss is:  0.5088303089141846\n",
      "Warning: nan gradient found. The current loss is:  0.7891607880592346\n",
      "Warning: nan gradient found. The current loss is:  0.41072383522987366\n",
      "Warning: nan gradient found. The current loss is:  0.3230154812335968\n",
      "Warning: nan gradient found. The current loss is:  0.701724112033844\n",
      "Warning: nan gradient found. The current loss is:  0.8561261892318726\n",
      "Warning: nan gradient found. The current loss is:  0.6332529783248901\n",
      "Warning: nan gradient found. The current loss is:  0.5900702476501465\n",
      "Warning: nan gradient found. The current loss is:  0.27976542711257935\n",
      "Warning: nan gradient found. The current loss is:  0.5268206596374512\n",
      "Warning: nan gradient found. The current loss is:  0.14091743528842926\n",
      "Warning: nan gradient found. The current loss is:  0.7484743595123291\n",
      "Warning: nan gradient found. The current loss is:  0.05239251255989075\n",
      "Warning: nan gradient found. The current loss is:  0.3728300631046295\n",
      "Warning: nan gradient found. The current loss is:  0.16246755421161652\n",
      "Warning: nan gradient found. The current loss is:  0.11147894710302353\n",
      "Warning: nan gradient found. The current loss is:  0.42623186111450195\n",
      "Warning: nan gradient found. The current loss is:  0.5181646347045898\n",
      "Warning: nan gradient found. The current loss is:  0.39489907026290894\n",
      "Warning: nan gradient found. The current loss is:  0.15504106879234314\n",
      "Warning: nan gradient found. The current loss is:  0.503445029258728\n",
      "Warning: nan gradient found. The current loss is:  0.038406819105148315\n",
      "Warning: nan gradient found. The current loss is:  1.2727959156036377\n",
      "Warning: nan gradient found. The current loss is:  1.2133522033691406\n",
      "Warning: nan gradient found. The current loss is:  0.36153078079223633\n",
      "Warning: nan gradient found. The current loss is:  0.5605666637420654\n",
      "Warning: nan gradient found. The current loss is:  0.25094401836395264\n",
      "Warning: nan gradient found. The current loss is:  0.6065329909324646\n",
      "Warning: nan gradient found. The current loss is:  0.48706746101379395\n",
      "Warning: nan gradient found. The current loss is:  0.2706357538700104\n",
      "Warning: nan gradient found. The current loss is:  0.09864038974046707\n",
      "Warning: nan gradient found. The current loss is:  0.5671273469924927\n",
      "Warning: nan gradient found. The current loss is:  0.29468220472335815\n",
      "Warning: nan gradient found. The current loss is:  0.4848155975341797\n",
      "Warning: nan gradient found. The current loss is:  0.6708009243011475\n",
      "Warning: nan gradient found. The current loss is:  0.2954694628715515\n",
      "Warning: nan gradient found. The current loss is:  0.8011052012443542\n",
      "Warning: nan gradient found. The current loss is:  0.6839306354522705\n",
      "Warning: nan gradient found. The current loss is:  1.3745365142822266\n",
      "Warning: nan gradient found. The current loss is:  0.9261375665664673\n",
      "Warning: nan gradient found. The current loss is:  0.35311001539230347\n",
      "Warning: nan gradient found. The current loss is:  0.4298822283744812\n",
      "Warning: nan gradient found. The current loss is:  0.9657745957374573\n",
      "Warning: nan gradient found. The current loss is:  0.5147524476051331\n",
      "Warning: nan gradient found. The current loss is:  0.7078567743301392\n",
      "Warning: nan gradient found. The current loss is:  0.6658778190612793\n",
      "Warning: nan gradient found. The current loss is:  0.766769289970398\n",
      "Warning: nan gradient found. The current loss is:  0.9680193662643433\n",
      "Warning: nan gradient found. The current loss is:  -0.035385455936193466\n",
      "Warning: nan gradient found. The current loss is:  0.5604892373085022\n",
      "Warning: nan gradient found. The current loss is:  0.21785710752010345\n",
      "Warning: nan gradient found. The current loss is:  0.302931547164917\n",
      "Warning: nan gradient found. The current loss is:  0.24813193082809448\n",
      "Warning: nan gradient found. The current loss is:  0.3129638135433197\n",
      "Warning: nan gradient found. The current loss is:  0.29938027262687683\n",
      "Warning: nan gradient found. The current loss is:  0.5709092020988464\n",
      "Warning: nan gradient found. The current loss is:  1.2747282981872559\n",
      "Warning: nan gradient found. The current loss is:  0.44103294610977173\n",
      "Warning: nan gradient found. The current loss is:  0.35232973098754883\n",
      "Warning: nan gradient found. The current loss is:  0.8405251502990723\n",
      "Warning: nan gradient found. The current loss is:  0.027609985321760178\n",
      "Warning: nan gradient found. The current loss is:  0.6578294038772583\n",
      "Warning: nan gradient found. The current loss is:  0.7105742692947388\n",
      "Warning: nan gradient found. The current loss is:  1.2286664247512817\n",
      "Warning: nan gradient found. The current loss is:  0.46165701746940613\n",
      "Warning: nan gradient found. The current loss is:  0.828200101852417\n",
      "Warning: nan gradient found. The current loss is:  0.44897013902664185\n",
      "Current batch training loss: 0.448970  [128000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.37617766857147217\n",
      "Warning: nan gradient found. The current loss is:  0.933354914188385\n",
      "Warning: nan gradient found. The current loss is:  0.4198473393917084\n",
      "Warning: nan gradient found. The current loss is:  0.13075362145900726\n",
      "Warning: nan gradient found. The current loss is:  1.9589412212371826\n",
      "Warning: nan gradient found. The current loss is:  0.4581521153450012\n",
      "Warning: nan gradient found. The current loss is:  1.0812124013900757\n",
      "Warning: nan gradient found. The current loss is:  0.6190845966339111\n",
      "Warning: nan gradient found. The current loss is:  0.58319091796875\n",
      "Warning: nan gradient found. The current loss is:  1.2377465963363647\n",
      "Warning: nan gradient found. The current loss is:  0.8647433519363403\n",
      "Warning: nan gradient found. The current loss is:  0.2937370240688324\n",
      "Warning: nan gradient found. The current loss is:  0.9106441736221313\n",
      "Warning: nan gradient found. The current loss is:  0.4379935562610626\n",
      "Warning: nan gradient found. The current loss is:  1.068953275680542\n",
      "Warning: nan gradient found. The current loss is:  0.4266301989555359\n",
      "Warning: nan gradient found. The current loss is:  0.460122287273407\n",
      "Warning: nan gradient found. The current loss is:  0.3857906460762024\n",
      "Warning: nan gradient found. The current loss is:  0.6400035619735718\n",
      "Warning: nan gradient found. The current loss is:  0.6685936450958252\n",
      "Warning: nan gradient found. The current loss is:  0.6990082859992981\n",
      "Warning: nan gradient found. The current loss is:  0.5624788403511047\n",
      "Warning: nan gradient found. The current loss is:  0.6469290256500244\n",
      "Warning: nan gradient found. The current loss is:  0.5078222751617432\n",
      "Warning: nan gradient found. The current loss is:  0.4787139296531677\n",
      "Warning: nan gradient found. The current loss is:  0.3874413073062897\n",
      "Warning: nan gradient found. The current loss is:  0.4843704104423523\n",
      "Warning: nan gradient found. The current loss is:  0.5116610527038574\n",
      "Warning: nan gradient found. The current loss is:  0.38573920726776123\n",
      "Warning: nan gradient found. The current loss is:  0.7593048214912415\n",
      "Warning: nan gradient found. The current loss is:  0.6523185968399048\n",
      "Warning: nan gradient found. The current loss is:  0.2979894280433655\n",
      "Warning: nan gradient found. The current loss is:  0.16957367956638336\n",
      "Warning: nan gradient found. The current loss is:  0.6697826385498047\n",
      "Warning: nan gradient found. The current loss is:  0.42672890424728394\n",
      "Warning: nan gradient found. The current loss is:  0.5197873115539551\n",
      "Warning: nan gradient found. The current loss is:  1.024584412574768\n",
      "Warning: nan gradient found. The current loss is:  0.6447851657867432\n",
      "Warning: nan gradient found. The current loss is:  0.05153041332960129\n",
      "Warning: nan gradient found. The current loss is:  0.579639732837677\n",
      "Warning: nan gradient found. The current loss is:  0.5183213949203491\n",
      "Warning: nan gradient found. The current loss is:  0.37817883491516113\n",
      "Warning: nan gradient found. The current loss is:  0.8331543803215027\n",
      "Warning: nan gradient found. The current loss is:  0.44183069467544556\n",
      "Warning: nan gradient found. The current loss is:  0.1362622082233429\n",
      "Warning: nan gradient found. The current loss is:  0.3763178288936615\n",
      "Warning: nan gradient found. The current loss is:  0.5216071605682373\n",
      "Warning: nan gradient found. The current loss is:  0.5664718747138977\n",
      "Warning: nan gradient found. The current loss is:  0.0958811342716217\n",
      "Warning: nan gradient found. The current loss is:  0.7502858638763428\n",
      "Warning: nan gradient found. The current loss is:  0.5877423286437988\n",
      "Warning: nan gradient found. The current loss is:  0.38300833106040955\n",
      "Warning: nan gradient found. The current loss is:  0.44159525632858276\n",
      "Warning: nan gradient found. The current loss is:  0.09937949478626251\n",
      "Warning: nan gradient found. The current loss is:  0.5052747130393982\n",
      "Warning: nan gradient found. The current loss is:  1.2771484851837158\n",
      "Warning: nan gradient found. The current loss is:  0.4702770709991455\n",
      "Warning: nan gradient found. The current loss is:  0.7578276991844177\n",
      "Warning: nan gradient found. The current loss is:  1.4020402431488037\n",
      "Warning: nan gradient found. The current loss is:  1.2052932977676392\n",
      "Warning: nan gradient found. The current loss is:  0.2225993275642395\n",
      "Warning: nan gradient found. The current loss is:  0.8363271951675415\n",
      "Warning: nan gradient found. The current loss is:  1.0222156047821045\n",
      "Warning: nan gradient found. The current loss is:  0.8530876636505127\n",
      "Warning: nan gradient found. The current loss is:  1.1373472213745117\n",
      "Warning: nan gradient found. The current loss is:  1.1106171607971191\n",
      "Warning: nan gradient found. The current loss is:  1.5958642959594727\n",
      "Warning: nan gradient found. The current loss is:  0.6597881317138672\n",
      "Warning: nan gradient found. The current loss is:  -0.005940582603216171\n",
      "Warning: nan gradient found. The current loss is:  0.7346086502075195\n",
      "Warning: nan gradient found. The current loss is:  0.3618810176849365\n",
      "Warning: nan gradient found. The current loss is:  1.2113102674484253\n",
      "Warning: nan gradient found. The current loss is:  0.33707818388938904\n",
      "Warning: nan gradient found. The current loss is:  0.23580190539360046\n",
      "Warning: nan gradient found. The current loss is:  0.16895723342895508\n",
      "Warning: nan gradient found. The current loss is:  0.3248670995235443\n",
      "Warning: nan gradient found. The current loss is:  0.3236673176288605\n",
      "Warning: nan gradient found. The current loss is:  0.31799399852752686\n",
      "Warning: nan gradient found. The current loss is:  0.5578938722610474\n",
      "Warning: nan gradient found. The current loss is:  0.4092864394187927\n",
      "Warning: nan gradient found. The current loss is:  0.8449203372001648\n",
      "Warning: nan gradient found. The current loss is:  0.7725255489349365\n",
      "Warning: nan gradient found. The current loss is:  0.8697130680084229\n",
      "Warning: nan gradient found. The current loss is:  0.2508239448070526\n",
      "Warning: nan gradient found. The current loss is:  0.641685962677002\n",
      "Warning: nan gradient found. The current loss is:  0.8266561031341553\n",
      "Warning: nan gradient found. The current loss is:  0.07804091274738312\n",
      "Warning: nan gradient found. The current loss is:  0.4021543562412262\n",
      "Warning: nan gradient found. The current loss is:  1.0245729684829712\n",
      "Warning: nan gradient found. The current loss is:  0.7941844463348389\n",
      "Warning: nan gradient found. The current loss is:  0.31551092863082886\n",
      "Warning: nan gradient found. The current loss is:  0.6819815039634705\n",
      "Warning: nan gradient found. The current loss is:  0.5321101546287537\n",
      "Warning: nan gradient found. The current loss is:  0.5443328022956848\n",
      "Warning: nan gradient found. The current loss is:  0.47372227907180786\n",
      "Warning: nan gradient found. The current loss is:  0.11282287538051605\n",
      "Warning: nan gradient found. The current loss is:  0.24763135612010956\n",
      "Warning: nan gradient found. The current loss is:  0.04718293249607086\n",
      "Warning: nan gradient found. The current loss is:  0.46536046266555786\n",
      "Warning: nan gradient found. The current loss is:  1.0523409843444824\n",
      "Current batch training loss: 1.052341  [153600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6333825588226318\n",
      "Warning: nan gradient found. The current loss is:  1.1327095031738281\n",
      "Warning: nan gradient found. The current loss is:  0.760088324546814\n",
      "Warning: nan gradient found. The current loss is:  0.7175487279891968\n",
      "Warning: nan gradient found. The current loss is:  1.0135903358459473\n",
      "Warning: nan gradient found. The current loss is:  0.45365768671035767\n",
      "Warning: nan gradient found. The current loss is:  0.9478632807731628\n",
      "Warning: nan gradient found. The current loss is:  0.1122259870171547\n",
      "Warning: nan gradient found. The current loss is:  0.4043753743171692\n",
      "Warning: nan gradient found. The current loss is:  0.872351884841919\n",
      "Warning: nan gradient found. The current loss is:  0.2415376901626587\n",
      "Warning: nan gradient found. The current loss is:  0.6718262434005737\n",
      "Warning: nan gradient found. The current loss is:  0.11674825102090836\n",
      "Warning: nan gradient found. The current loss is:  0.7063367962837219\n",
      "Warning: nan gradient found. The current loss is:  1.0090590715408325\n",
      "Warning: nan gradient found. The current loss is:  0.4679935574531555\n",
      "Warning: nan gradient found. The current loss is:  0.28271788358688354\n",
      "Warning: nan gradient found. The current loss is:  0.7368378639221191\n",
      "Warning: nan gradient found. The current loss is:  0.18893198668956757\n",
      "Warning: nan gradient found. The current loss is:  0.541547417640686\n",
      "Warning: nan gradient found. The current loss is:  0.6844196319580078\n",
      "Warning: nan gradient found. The current loss is:  0.8971912264823914\n",
      "Warning: nan gradient found. The current loss is:  1.0577199459075928\n",
      "Warning: nan gradient found. The current loss is:  0.5532403588294983\n",
      "Warning: nan gradient found. The current loss is:  0.5625127553939819\n",
      "Warning: nan gradient found. The current loss is:  0.45464447140693665\n",
      "Warning: nan gradient found. The current loss is:  0.9522005915641785\n",
      "Warning: nan gradient found. The current loss is:  0.35791248083114624\n",
      "Warning: nan gradient found. The current loss is:  0.552668035030365\n",
      "Warning: nan gradient found. The current loss is:  0.3456442654132843\n",
      "Warning: nan gradient found. The current loss is:  0.4822685718536377\n",
      "Warning: nan gradient found. The current loss is:  0.3476760685443878\n",
      "Warning: nan gradient found. The current loss is:  0.7364453077316284\n",
      "Warning: nan gradient found. The current loss is:  0.30610135197639465\n",
      "Warning: nan gradient found. The current loss is:  0.7686023712158203\n",
      "Warning: nan gradient found. The current loss is:  0.28574103116989136\n",
      "Warning: nan gradient found. The current loss is:  0.2758786976337433\n",
      "Warning: nan gradient found. The current loss is:  0.23193028569221497\n",
      "Warning: nan gradient found. The current loss is:  0.36753109097480774\n",
      "Warning: nan gradient found. The current loss is:  0.727059543132782\n",
      "Warning: nan gradient found. The current loss is:  0.5412970781326294\n",
      "Warning: nan gradient found. The current loss is:  0.716084897518158\n",
      "Warning: nan gradient found. The current loss is:  0.46580561995506287\n",
      "Warning: nan gradient found. The current loss is:  0.5197950601577759\n",
      "Warning: nan gradient found. The current loss is:  0.1867806613445282\n",
      "Warning: nan gradient found. The current loss is:  0.33228206634521484\n",
      "Warning: nan gradient found. The current loss is:  0.3888007402420044\n",
      "Warning: nan gradient found. The current loss is:  0.4098256230354309\n",
      "Warning: nan gradient found. The current loss is:  0.625026524066925\n",
      "Warning: nan gradient found. The current loss is:  0.6665810346603394\n",
      "Warning: nan gradient found. The current loss is:  0.5894141793251038\n",
      "Warning: nan gradient found. The current loss is:  0.4962943196296692\n",
      "Warning: nan gradient found. The current loss is:  0.47027868032455444\n",
      "Warning: nan gradient found. The current loss is:  0.674906313419342\n",
      "Warning: nan gradient found. The current loss is:  0.3255693316459656\n",
      "Warning: nan gradient found. The current loss is:  0.6433133482933044\n",
      "Warning: nan gradient found. The current loss is:  1.28895902633667\n",
      "Warning: nan gradient found. The current loss is:  0.9071340560913086\n",
      "Warning: nan gradient found. The current loss is:  0.29268673062324524\n",
      "Warning: nan gradient found. The current loss is:  0.930181086063385\n",
      "Warning: nan gradient found. The current loss is:  0.6946723461151123\n",
      "Warning: nan gradient found. The current loss is:  0.1306142807006836\n",
      "Warning: nan gradient found. The current loss is:  0.4442351460456848\n",
      "Warning: nan gradient found. The current loss is:  0.475152850151062\n",
      "Warning: nan gradient found. The current loss is:  0.44980642199516296\n",
      "Warning: nan gradient found. The current loss is:  1.662333369255066\n",
      "Warning: nan gradient found. The current loss is:  0.010580375790596008\n",
      "Warning: nan gradient found. The current loss is:  0.6117687821388245\n",
      "Warning: nan gradient found. The current loss is:  0.6121840476989746\n",
      "Warning: nan gradient found. The current loss is:  1.5909188985824585\n",
      "Warning: nan gradient found. The current loss is:  0.46419262886047363\n",
      "Warning: nan gradient found. The current loss is:  0.9295856952667236\n",
      "Warning: nan gradient found. The current loss is:  1.3353865146636963\n",
      "Warning: nan gradient found. The current loss is:  0.46335887908935547\n",
      "Warning: nan gradient found. The current loss is:  0.5366935133934021\n",
      "Warning: nan gradient found. The current loss is:  0.07322658598423004\n",
      "Warning: nan gradient found. The current loss is:  1.0814487934112549\n",
      "Warning: nan gradient found. The current loss is:  0.08003393560647964\n",
      "Warning: nan gradient found. The current loss is:  0.2633069157600403\n",
      "Warning: nan gradient found. The current loss is:  0.685279369354248\n",
      "Warning: nan gradient found. The current loss is:  0.5159813165664673\n",
      "Warning: nan gradient found. The current loss is:  0.4991980195045471\n",
      "Warning: nan gradient found. The current loss is:  0.5490372776985168\n",
      "Warning: nan gradient found. The current loss is:  1.361811876296997\n",
      "Warning: nan gradient found. The current loss is:  0.6020908355712891\n",
      "Warning: nan gradient found. The current loss is:  0.5659739971160889\n",
      "Warning: nan gradient found. The current loss is:  0.7905797362327576\n",
      "Warning: nan gradient found. The current loss is:  0.24051380157470703\n",
      "Warning: nan gradient found. The current loss is:  0.17626027762889862\n",
      "Warning: nan gradient found. The current loss is:  0.4991287589073181\n",
      "Warning: nan gradient found. The current loss is:  0.6247475147247314\n",
      "Warning: nan gradient found. The current loss is:  0.4118189215660095\n",
      "Warning: nan gradient found. The current loss is:  0.5215656757354736\n",
      "Warning: nan gradient found. The current loss is:  0.46305087208747864\n",
      "Warning: nan gradient found. The current loss is:  0.3042064607143402\n",
      "Warning: nan gradient found. The current loss is:  0.6646088361740112\n",
      "Warning: nan gradient found. The current loss is:  1.0200159549713135\n",
      "Warning: nan gradient found. The current loss is:  0.5148717164993286\n",
      "Warning: nan gradient found. The current loss is:  0.4420231282711029\n",
      "Warning: nan gradient found. The current loss is:  0.711039125919342\n",
      "Current batch training loss: 0.711039  [179200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4754464626312256\n",
      "Warning: nan gradient found. The current loss is:  0.7769279479980469\n",
      "Warning: nan gradient found. The current loss is:  1.3859611749649048\n",
      "Warning: nan gradient found. The current loss is:  0.48822489380836487\n",
      "Warning: nan gradient found. The current loss is:  0.401363730430603\n",
      "Warning: nan gradient found. The current loss is:  0.868372917175293\n",
      "Warning: nan gradient found. The current loss is:  0.15894216299057007\n",
      "Warning: nan gradient found. The current loss is:  0.7245439291000366\n",
      "Warning: nan gradient found. The current loss is:  0.4490358829498291\n",
      "Warning: nan gradient found. The current loss is:  0.7486439347267151\n",
      "Warning: nan gradient found. The current loss is:  0.01917548105120659\n",
      "Warning: nan gradient found. The current loss is:  0.26814860105514526\n",
      "Warning: nan gradient found. The current loss is:  0.39735686779022217\n",
      "Warning: nan gradient found. The current loss is:  1.084736704826355\n",
      "Warning: nan gradient found. The current loss is:  0.2897094786167145\n",
      "Warning: nan gradient found. The current loss is:  0.10464012622833252\n",
      "Warning: nan gradient found. The current loss is:  0.9441921710968018\n",
      "Warning: nan gradient found. The current loss is:  0.15714877843856812\n",
      "Warning: nan gradient found. The current loss is:  -0.02745702862739563\n",
      "Warning: nan gradient found. The current loss is:  1.031102180480957\n",
      "Warning: nan gradient found. The current loss is:  0.16283780336380005\n",
      "Warning: nan gradient found. The current loss is:  0.38433122634887695\n",
      "Warning: nan gradient found. The current loss is:  0.5590111017227173\n",
      "Warning: nan gradient found. The current loss is:  0.595032274723053\n",
      "Warning: nan gradient found. The current loss is:  0.010191835463047028\n",
      "Warning: nan gradient found. The current loss is:  0.34107837080955505\n",
      "Warning: nan gradient found. The current loss is:  0.7553988695144653\n",
      "Warning: nan gradient found. The current loss is:  0.46026185154914856\n",
      "Warning: nan gradient found. The current loss is:  0.3890158236026764\n",
      "Warning: nan gradient found. The current loss is:  0.7911682724952698\n",
      "Warning: nan gradient found. The current loss is:  0.8442825675010681\n",
      "Warning: nan gradient found. The current loss is:  0.23947349190711975\n",
      "Warning: nan gradient found. The current loss is:  0.24626590311527252\n",
      "Warning: nan gradient found. The current loss is:  0.2483091652393341\n",
      "Warning: nan gradient found. The current loss is:  0.29502373933792114\n",
      "Warning: nan gradient found. The current loss is:  1.6769859790802002\n",
      "Warning: nan gradient found. The current loss is:  0.08471710979938507\n",
      "Warning: nan gradient found. The current loss is:  0.5075541734695435\n",
      "Warning: nan gradient found. The current loss is:  0.4319712519645691\n",
      "Warning: nan gradient found. The current loss is:  0.29325878620147705\n",
      "Warning: nan gradient found. The current loss is:  0.5039403438568115\n",
      "Warning: nan gradient found. The current loss is:  0.22608810663223267\n",
      "Warning: nan gradient found. The current loss is:  0.409222275018692\n",
      "Warning: nan gradient found. The current loss is:  0.4558771252632141\n",
      "Warning: nan gradient found. The current loss is:  0.6538596153259277\n",
      "Warning: nan gradient found. The current loss is:  -0.034552980214357376\n",
      "Warning: nan gradient found. The current loss is:  0.5982792377471924\n",
      "Warning: nan gradient found. The current loss is:  0.8621818423271179\n",
      "Warning: nan gradient found. The current loss is:  0.60796058177948\n",
      "Warning: nan gradient found. The current loss is:  0.6786648631095886\n",
      "Warning: nan gradient found. The current loss is:  0.4426769018173218\n",
      "Warning: nan gradient found. The current loss is:  0.5133255124092102\n",
      "Warning: nan gradient found. The current loss is:  0.1739145815372467\n",
      "Warning: nan gradient found. The current loss is:  0.9366967082023621\n",
      "Warning: nan gradient found. The current loss is:  0.6591885685920715\n",
      "Warning: nan gradient found. The current loss is:  0.476509690284729\n",
      "Warning: nan gradient found. The current loss is:  0.2302541583776474\n",
      "Warning: nan gradient found. The current loss is:  0.16056707501411438\n",
      "Warning: nan gradient found. The current loss is:  0.435043603181839\n",
      "Warning: nan gradient found. The current loss is:  0.05266844481229782\n",
      "Warning: nan gradient found. The current loss is:  0.7320840954780579\n",
      "Warning: nan gradient found. The current loss is:  0.6359506845474243\n",
      "Warning: nan gradient found. The current loss is:  0.2432464361190796\n",
      "Warning: nan gradient found. The current loss is:  0.39350658655166626\n",
      "Warning: nan gradient found. The current loss is:  0.711672306060791\n",
      "Warning: nan gradient found. The current loss is:  0.5094211101531982\n",
      "Warning: nan gradient found. The current loss is:  0.49490928649902344\n",
      "Warning: nan gradient found. The current loss is:  -0.066856250166893\n",
      "Warning: nan gradient found. The current loss is:  0.31607261300086975\n",
      "Warning: nan gradient found. The current loss is:  0.5185802578926086\n",
      "Warning: nan gradient found. The current loss is:  0.41565555334091187\n",
      "Warning: nan gradient found. The current loss is:  0.5736490488052368\n",
      "Warning: nan gradient found. The current loss is:  0.3283179700374603\n",
      "Warning: nan gradient found. The current loss is:  0.7554147243499756\n",
      "Warning: nan gradient found. The current loss is:  0.37059709429740906\n",
      "Warning: nan gradient found. The current loss is:  1.5809576511383057\n",
      "Warning: nan gradient found. The current loss is:  0.533012866973877\n",
      "Warning: nan gradient found. The current loss is:  1.5678863525390625\n",
      "Warning: nan gradient found. The current loss is:  -0.01356746256351471\n",
      "Warning: nan gradient found. The current loss is:  0.38407251238822937\n",
      "Warning: nan gradient found. The current loss is:  0.30672281980514526\n",
      "Warning: nan gradient found. The current loss is:  2.4060497283935547\n",
      "Warning: nan gradient found. The current loss is:  2.5966925621032715\n",
      "Warning: nan gradient found. The current loss is:  0.9769800901412964\n",
      "Warning: nan gradient found. The current loss is:  0.45486196875572205\n",
      "Warning: nan gradient found. The current loss is:  0.2386513650417328\n",
      "Warning: nan gradient found. The current loss is:  0.7468302845954895\n",
      "Warning: nan gradient found. The current loss is:  0.9887716770172119\n",
      "Warning: nan gradient found. The current loss is:  0.16325046122074127\n",
      "Warning: nan gradient found. The current loss is:  0.2990562915802002\n",
      "Warning: nan gradient found. The current loss is:  0.5123785734176636\n",
      "Warning: nan gradient found. The current loss is:  0.3904837667942047\n",
      "Warning: nan gradient found. The current loss is:  0.402667373418808\n",
      "Warning: nan gradient found. The current loss is:  0.7901567220687866\n",
      "Warning: nan gradient found. The current loss is:  0.6071658134460449\n",
      "Warning: nan gradient found. The current loss is:  0.47023606300354004\n",
      "Warning: nan gradient found. The current loss is:  0.43806684017181396\n",
      "Warning: nan gradient found. The current loss is:  0.2794552445411682\n",
      "Warning: nan gradient found. The current loss is:  0.04259147495031357\n",
      "Warning: nan gradient found. The current loss is:  0.4095737338066101\n",
      "Current batch training loss: 0.409574  [204800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.39624857902526855\n",
      "Warning: nan gradient found. The current loss is:  0.7497345209121704\n",
      "Warning: nan gradient found. The current loss is:  0.8049855828285217\n",
      "Warning: nan gradient found. The current loss is:  0.5815641283988953\n",
      "Warning: nan gradient found. The current loss is:  0.7950423359870911\n",
      "Warning: nan gradient found. The current loss is:  0.5923041701316833\n",
      "Warning: nan gradient found. The current loss is:  0.743860125541687\n",
      "Warning: nan gradient found. The current loss is:  0.39122122526168823\n",
      "Warning: nan gradient found. The current loss is:  0.1262173354625702\n",
      "Warning: nan gradient found. The current loss is:  1.0962872505187988\n",
      "Warning: nan gradient found. The current loss is:  0.4018593430519104\n",
      "Warning: nan gradient found. The current loss is:  1.1800956726074219\n",
      "Warning: nan gradient found. The current loss is:  0.4151375889778137\n",
      "Warning: nan gradient found. The current loss is:  0.5591421723365784\n",
      "Warning: nan gradient found. The current loss is:  0.7563234567642212\n",
      "Warning: nan gradient found. The current loss is:  0.43901368975639343\n",
      "Warning: nan gradient found. The current loss is:  0.2445714771747589\n",
      "Warning: nan gradient found. The current loss is:  0.39477235078811646\n",
      "Warning: nan gradient found. The current loss is:  0.3485208749771118\n",
      "Warning: nan gradient found. The current loss is:  0.6954347491264343\n",
      "Warning: nan gradient found. The current loss is:  0.3396603763103485\n",
      "Warning: nan gradient found. The current loss is:  0.45011597871780396\n",
      "Warning: nan gradient found. The current loss is:  0.19511626660823822\n",
      "Warning: nan gradient found. The current loss is:  0.9205560088157654\n",
      "Warning: nan gradient found. The current loss is:  0.6685517430305481\n",
      "Warning: nan gradient found. The current loss is:  1.9437676668167114\n",
      "Warning: nan gradient found. The current loss is:  1.050595998764038\n",
      "Warning: nan gradient found. The current loss is:  0.11968733370304108\n",
      "Warning: nan gradient found. The current loss is:  0.24356983602046967\n",
      "Warning: nan gradient found. The current loss is:  0.5972711443901062\n",
      "Warning: nan gradient found. The current loss is:  1.3471519947052002\n",
      "Warning: nan gradient found. The current loss is:  0.4794788062572479\n",
      "Warning: nan gradient found. The current loss is:  0.40114641189575195\n",
      "Warning: nan gradient found. The current loss is:  0.41091087460517883\n",
      "Warning: nan gradient found. The current loss is:  0.692245364189148\n",
      "Warning: nan gradient found. The current loss is:  0.52642422914505\n",
      "Warning: nan gradient found. The current loss is:  1.1068391799926758\n",
      "Warning: nan gradient found. The current loss is:  0.7874490022659302\n",
      "Warning: nan gradient found. The current loss is:  0.2903669774532318\n",
      "Warning: nan gradient found. The current loss is:  0.42361438274383545\n",
      "Warning: nan gradient found. The current loss is:  0.22434836626052856\n",
      "Warning: nan gradient found. The current loss is:  0.5294831991195679\n",
      "Warning: nan gradient found. The current loss is:  0.08912082016468048\n",
      "Warning: nan gradient found. The current loss is:  0.399824857711792\n",
      "Warning: nan gradient found. The current loss is:  1.1162651777267456\n",
      "Warning: nan gradient found. The current loss is:  1.2557332515716553\n",
      "Warning: nan gradient found. The current loss is:  1.2921924591064453\n",
      "Warning: nan gradient found. The current loss is:  0.7879602909088135\n",
      "Warning: nan gradient found. The current loss is:  0.6205222606658936\n",
      "Warning: nan gradient found. The current loss is:  0.5761308670043945\n",
      "Warning: nan gradient found. The current loss is:  0.8655036091804504\n",
      "Warning: nan gradient found. The current loss is:  0.3793565630912781\n",
      "Warning: nan gradient found. The current loss is:  0.008860815316438675\n",
      "Warning: nan gradient found. The current loss is:  0.45535996556282043\n",
      "Warning: nan gradient found. The current loss is:  0.3687865138053894\n",
      "Warning: nan gradient found. The current loss is:  0.8884796500205994\n",
      "Warning: nan gradient found. The current loss is:  0.32677504420280457\n",
      "Warning: nan gradient found. The current loss is:  1.1639032363891602\n",
      "Warning: nan gradient found. The current loss is:  0.5116854906082153\n",
      "Warning: nan gradient found. The current loss is:  0.9822203516960144\n",
      "Warning: nan gradient found. The current loss is:  0.5585143566131592\n",
      "Warning: nan gradient found. The current loss is:  0.4934964179992676\n",
      "Warning: nan gradient found. The current loss is:  0.16072548925876617\n",
      "Warning: nan gradient found. The current loss is:  3.7588958740234375\n",
      "Warning: nan gradient found. The current loss is:  0.2861483097076416\n",
      "Warning: nan gradient found. The current loss is:  0.7379103899002075\n",
      "Warning: nan gradient found. The current loss is:  0.4603096842765808\n",
      "Warning: nan gradient found. The current loss is:  0.03272236883640289\n",
      "Warning: nan gradient found. The current loss is:  1.136001706123352\n",
      "Warning: nan gradient found. The current loss is:  0.7642450332641602\n",
      "Warning: nan gradient found. The current loss is:  -0.016667228192090988\n",
      "Warning: nan gradient found. The current loss is:  0.17629948258399963\n",
      "Warning: nan gradient found. The current loss is:  1.1173163652420044\n",
      "Warning: nan gradient found. The current loss is:  0.41301220655441284\n",
      "Warning: nan gradient found. The current loss is:  0.8267778158187866\n",
      "Warning: nan gradient found. The current loss is:  0.6960027813911438\n",
      "Warning: nan gradient found. The current loss is:  0.2867792844772339\n",
      "Warning: nan gradient found. The current loss is:  0.7554529905319214\n",
      "Warning: nan gradient found. The current loss is:  0.9688336849212646\n",
      "Warning: nan gradient found. The current loss is:  0.9077680110931396\n",
      "Warning: nan gradient found. The current loss is:  0.12637245655059814\n",
      "Warning: nan gradient found. The current loss is:  0.8205230236053467\n",
      "Warning: nan gradient found. The current loss is:  0.5837033987045288\n",
      "Warning: nan gradient found. The current loss is:  1.5798410177230835\n",
      "Warning: nan gradient found. The current loss is:  0.5696560144424438\n",
      "Warning: nan gradient found. The current loss is:  0.9579765200614929\n",
      "Warning: nan gradient found. The current loss is:  0.120522141456604\n",
      "Warning: nan gradient found. The current loss is:  0.022829342633485794\n",
      "Warning: nan gradient found. The current loss is:  0.6935105919837952\n",
      "Warning: nan gradient found. The current loss is:  0.40281999111175537\n",
      "Warning: nan gradient found. The current loss is:  1.031362533569336\n",
      "Warning: nan gradient found. The current loss is:  0.7258394360542297\n",
      "Warning: nan gradient found. The current loss is:  0.8222585320472717\n",
      "Warning: nan gradient found. The current loss is:  0.249602273106575\n",
      "Warning: nan gradient found. The current loss is:  0.5483256578445435\n",
      "Warning: nan gradient found. The current loss is:  0.47065502405166626\n",
      "Warning: nan gradient found. The current loss is:  1.1190698146820068\n",
      "Warning: nan gradient found. The current loss is:  0.9455999135971069\n",
      "Warning: nan gradient found. The current loss is:  0.5231031179428101\n",
      "Warning: nan gradient found. The current loss is:  0.4411696195602417\n",
      "Current batch training loss: 0.441170  [230400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5159449577331543\n",
      "Warning: nan gradient found. The current loss is:  0.8249532580375671\n",
      "Warning: nan gradient found. The current loss is:  0.3408350944519043\n",
      "Warning: nan gradient found. The current loss is:  0.6073675155639648\n",
      "Warning: nan gradient found. The current loss is:  0.6569051742553711\n",
      "Warning: nan gradient found. The current loss is:  0.4981021285057068\n",
      "Warning: nan gradient found. The current loss is:  0.6808441877365112\n",
      "Warning: nan gradient found. The current loss is:  0.42781922221183777\n",
      "Warning: nan gradient found. The current loss is:  1.181469202041626\n",
      "Warning: nan gradient found. The current loss is:  0.158439502120018\n",
      "Warning: nan gradient found. The current loss is:  0.46791166067123413\n",
      "Warning: nan gradient found. The current loss is:  0.5899078845977783\n",
      "Warning: nan gradient found. The current loss is:  1.9162156581878662\n",
      "Warning: nan gradient found. The current loss is:  0.7092918157577515\n",
      "Warning: nan gradient found. The current loss is:  0.32002705335617065\n",
      "Warning: nan gradient found. The current loss is:  1.2304677963256836\n",
      "Warning: nan gradient found. The current loss is:  0.4433634281158447\n",
      "Warning: nan gradient found. The current loss is:  0.349214106798172\n",
      "Warning: nan gradient found. The current loss is:  0.4635326862335205\n",
      "Warning: nan gradient found. The current loss is:  0.4802488684654236\n",
      "Warning: nan gradient found. The current loss is:  0.5142110586166382\n",
      "Warning: nan gradient found. The current loss is:  0.14956720173358917\n",
      "Warning: nan gradient found. The current loss is:  0.18143676221370697\n",
      "Warning: nan gradient found. The current loss is:  0.34961891174316406\n",
      "Warning: nan gradient found. The current loss is:  0.2552650272846222\n",
      "Warning: nan gradient found. The current loss is:  0.4546046853065491\n",
      "Warning: nan gradient found. The current loss is:  0.31434375047683716\n",
      "Warning: nan gradient found. The current loss is:  0.5716342329978943\n",
      "Warning: nan gradient found. The current loss is:  1.0535680055618286\n",
      "Warning: nan gradient found. The current loss is:  0.41287949681282043\n",
      "Warning: nan gradient found. The current loss is:  0.2622867822647095\n",
      "Warning: nan gradient found. The current loss is:  0.5274869799613953\n",
      "Warning: nan gradient found. The current loss is:  0.42267274856567383\n",
      "Warning: nan gradient found. The current loss is:  0.2948995530605316\n",
      "Warning: nan gradient found. The current loss is:  0.396514356136322\n",
      "Warning: nan gradient found. The current loss is:  0.3098554015159607\n",
      "Warning: nan gradient found. The current loss is:  1.8243343830108643\n",
      "Warning: nan gradient found. The current loss is:  0.3977079689502716\n",
      "Warning: nan gradient found. The current loss is:  0.7302893400192261\n",
      "Warning: nan gradient found. The current loss is:  0.606974720954895\n",
      "Warning: nan gradient found. The current loss is:  0.44989827275276184\n",
      "Warning: nan gradient found. The current loss is:  1.3603270053863525\n",
      "Warning: nan gradient found. The current loss is:  0.7378466725349426\n",
      "Warning: nan gradient found. The current loss is:  0.47305092215538025\n",
      "Warning: nan gradient found. The current loss is:  0.4925462603569031\n",
      "Warning: nan gradient found. The current loss is:  0.5910293459892273\n",
      "Warning: nan gradient found. The current loss is:  0.31932175159454346\n",
      "Warning: nan gradient found. The current loss is:  0.4463290572166443\n",
      "Warning: nan gradient found. The current loss is:  0.6281418800354004\n",
      "Warning: nan gradient found. The current loss is:  0.705912709236145\n",
      "Warning: nan gradient found. The current loss is:  1.0370360612869263\n",
      "Warning: nan gradient found. The current loss is:  0.3494464159011841\n",
      "Warning: nan gradient found. The current loss is:  0.4316011965274811\n",
      "Warning: nan gradient found. The current loss is:  0.7869444489479065\n",
      "Warning: nan gradient found. The current loss is:  0.9621568918228149\n",
      "Warning: nan gradient found. The current loss is:  0.30285942554473877\n",
      "Warning: nan gradient found. The current loss is:  1.1045705080032349\n",
      "Warning: nan gradient found. The current loss is:  0.27252399921417236\n",
      "Warning: nan gradient found. The current loss is:  0.891703724861145\n",
      "Warning: nan gradient found. The current loss is:  2.455186605453491\n",
      "Warning: nan gradient found. The current loss is:  0.8885661363601685\n",
      "Warning: nan gradient found. The current loss is:  1.4552044868469238\n",
      "Warning: nan gradient found. The current loss is:  1.3141794204711914\n",
      "Warning: nan gradient found. The current loss is:  0.9642168283462524\n",
      "Warning: nan gradient found. The current loss is:  0.2917582094669342\n",
      "Warning: nan gradient found. The current loss is:  0.31103232502937317\n",
      "Warning: nan gradient found. The current loss is:  0.5132230520248413\n",
      "Warning: nan gradient found. The current loss is:  0.16753478348255157\n",
      "Warning: nan gradient found. The current loss is:  -0.02608434110879898\n",
      "Warning: nan gradient found. The current loss is:  0.8495498895645142\n",
      "Warning: nan gradient found. The current loss is:  0.625963032245636\n",
      "Warning: nan gradient found. The current loss is:  0.8798596858978271\n",
      "Warning: nan gradient found. The current loss is:  1.2900660037994385\n",
      "Warning: nan gradient found. The current loss is:  0.7066706418991089\n",
      "Warning: nan gradient found. The current loss is:  0.3250744342803955\n",
      "Warning: nan gradient found. The current loss is:  0.3878291845321655\n",
      "Warning: nan gradient found. The current loss is:  0.16557088494300842\n",
      "Warning: nan gradient found. The current loss is:  0.5362933874130249\n",
      "Warning: nan gradient found. The current loss is:  0.5595302581787109\n",
      "Warning: nan gradient found. The current loss is:  0.5705344676971436\n",
      "Warning: nan gradient found. The current loss is:  0.5699810981750488\n",
      "Warning: nan gradient found. The current loss is:  0.41769102215766907\n",
      "Warning: nan gradient found. The current loss is:  0.5723896026611328\n",
      "Warning: nan gradient found. The current loss is:  0.6343345046043396\n",
      "Warning: nan gradient found. The current loss is:  0.3933106064796448\n",
      "Warning: nan gradient found. The current loss is:  0.6344977021217346\n",
      "Warning: nan gradient found. The current loss is:  0.520330548286438\n",
      "Warning: nan gradient found. The current loss is:  0.293732613325119\n",
      "Warning: nan gradient found. The current loss is:  0.7165919542312622\n",
      "Warning: nan gradient found. The current loss is:  0.7340923547744751\n",
      "Warning: nan gradient found. The current loss is:  0.8470494151115417\n",
      "Warning: nan gradient found. The current loss is:  0.8318502902984619\n",
      "Warning: nan gradient found. The current loss is:  0.42043739557266235\n",
      "Warning: nan gradient found. The current loss is:  0.5228278040885925\n",
      "Warning: nan gradient found. The current loss is:  0.06453900784254074\n",
      "Warning: nan gradient found. The current loss is:  0.06535059213638306\n",
      "Warning: nan gradient found. The current loss is:  0.27969643473625183\n",
      "Warning: nan gradient found. The current loss is:  0.18906183540821075\n",
      "Warning: nan gradient found. The current loss is:  0.8518491387367249\n",
      "Warning: nan gradient found. The current loss is:  0.0986127108335495\n",
      "Current batch training loss: 0.098613  [256000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5498366951942444\n",
      "Warning: nan gradient found. The current loss is:  0.5551448464393616\n",
      "Warning: nan gradient found. The current loss is:  0.41000083088874817\n",
      "Warning: nan gradient found. The current loss is:  0.11108122020959854\n",
      "Warning: nan gradient found. The current loss is:  0.00577734038233757\n",
      "Warning: nan gradient found. The current loss is:  0.6105489730834961\n",
      "Warning: nan gradient found. The current loss is:  1.1325688362121582\n",
      "Warning: nan gradient found. The current loss is:  0.2167372703552246\n",
      "Warning: nan gradient found. The current loss is:  0.03185804933309555\n",
      "Warning: nan gradient found. The current loss is:  0.527554988861084\n",
      "Warning: nan gradient found. The current loss is:  0.4837108850479126\n",
      "Warning: nan gradient found. The current loss is:  0.638390839099884\n",
      "Warning: nan gradient found. The current loss is:  0.8888758420944214\n",
      "Warning: nan gradient found. The current loss is:  0.3423202633857727\n",
      "Warning: nan gradient found. The current loss is:  1.2288366556167603\n",
      "Warning: nan gradient found. The current loss is:  0.3232229948043823\n",
      "Warning: nan gradient found. The current loss is:  0.35185784101486206\n",
      "Warning: nan gradient found. The current loss is:  0.20880669355392456\n",
      "Warning: nan gradient found. The current loss is:  0.9117788672447205\n",
      "Warning: nan gradient found. The current loss is:  0.4033079743385315\n",
      "Warning: nan gradient found. The current loss is:  0.3894343376159668\n",
      "Warning: nan gradient found. The current loss is:  0.35937201976776123\n",
      "Warning: nan gradient found. The current loss is:  0.6589453220367432\n",
      "Warning: nan gradient found. The current loss is:  0.6830177903175354\n",
      "Warning: nan gradient found. The current loss is:  0.8571840524673462\n",
      "Warning: nan gradient found. The current loss is:  0.5818805694580078\n",
      "Warning: nan gradient found. The current loss is:  1.0033807754516602\n",
      "Warning: nan gradient found. The current loss is:  0.22537358105182648\n",
      "Warning: nan gradient found. The current loss is:  0.549634575843811\n",
      "Warning: nan gradient found. The current loss is:  0.2330855131149292\n",
      "Warning: nan gradient found. The current loss is:  0.3493933081626892\n",
      "Warning: nan gradient found. The current loss is:  0.6993730068206787\n",
      "Warning: nan gradient found. The current loss is:  0.14897963404655457\n",
      "Warning: nan gradient found. The current loss is:  0.47742989659309387\n",
      "Warning: nan gradient found. The current loss is:  0.7320994138717651\n",
      "Warning: nan gradient found. The current loss is:  0.6063425540924072\n",
      "Warning: nan gradient found. The current loss is:  0.6434419751167297\n",
      "Warning: nan gradient found. The current loss is:  0.3969697952270508\n",
      "Warning: nan gradient found. The current loss is:  0.2923886179924011\n",
      "Warning: nan gradient found. The current loss is:  0.5563479065895081\n",
      "Warning: nan gradient found. The current loss is:  0.7395905256271362\n",
      "Warning: nan gradient found. The current loss is:  0.6463773250579834\n",
      "Warning: nan gradient found. The current loss is:  0.7471208572387695\n",
      "Warning: nan gradient found. The current loss is:  0.9700936079025269\n",
      "Warning: nan gradient found. The current loss is:  1.0300287008285522\n",
      "Warning: nan gradient found. The current loss is:  0.6419408321380615\n",
      "Warning: nan gradient found. The current loss is:  0.6394966244697571\n",
      "Warning: nan gradient found. The current loss is:  0.40978601574897766\n",
      "Warning: nan gradient found. The current loss is:  0.6964995861053467\n",
      "Warning: nan gradient found. The current loss is:  0.8089955449104309\n",
      "Warning: nan gradient found. The current loss is:  0.5920622944831848\n",
      "Warning: nan gradient found. The current loss is:  1.3276952505111694\n",
      "Warning: nan gradient found. The current loss is:  0.8100295066833496\n",
      "Warning: nan gradient found. The current loss is:  0.8313255310058594\n",
      "Warning: nan gradient found. The current loss is:  0.35254475474357605\n",
      "Warning: nan gradient found. The current loss is:  0.2762252986431122\n",
      "Warning: nan gradient found. The current loss is:  0.36910128593444824\n",
      "Warning: nan gradient found. The current loss is:  0.5463640093803406\n",
      "Warning: nan gradient found. The current loss is:  0.2884350121021271\n",
      "Warning: nan gradient found. The current loss is:  0.15352188050746918\n",
      "Warning: nan gradient found. The current loss is:  0.509378969669342\n",
      "Warning: nan gradient found. The current loss is:  0.9436318278312683\n",
      "Warning: nan gradient found. The current loss is:  0.10466018319129944\n",
      "Warning: nan gradient found. The current loss is:  1.0628705024719238\n",
      "Warning: nan gradient found. The current loss is:  0.5253692865371704\n",
      "Warning: nan gradient found. The current loss is:  0.5979143977165222\n",
      "Warning: nan gradient found. The current loss is:  0.635195791721344\n",
      "Warning: nan gradient found. The current loss is:  0.5396520495414734\n",
      "Warning: nan gradient found. The current loss is:  0.9362691640853882\n",
      "Warning: nan gradient found. The current loss is:  1.3732006549835205\n",
      "Warning: nan gradient found. The current loss is:  0.42715156078338623\n",
      "Warning: nan gradient found. The current loss is:  0.8886198997497559\n",
      "Warning: nan gradient found. The current loss is:  0.7975792288780212\n",
      "Warning: nan gradient found. The current loss is:  0.5857070088386536\n",
      "Warning: nan gradient found. The current loss is:  0.951581597328186\n",
      "Warning: nan gradient found. The current loss is:  0.23128965497016907\n",
      "Warning: nan gradient found. The current loss is:  0.32610857486724854\n",
      "Warning: nan gradient found. The current loss is:  0.9236621856689453\n",
      "Warning: nan gradient found. The current loss is:  0.5024956464767456\n",
      "Warning: nan gradient found. The current loss is:  0.28180670738220215\n",
      "Warning: nan gradient found. The current loss is:  0.4724521040916443\n",
      "Warning: nan gradient found. The current loss is:  0.4586021900177002\n",
      "Warning: nan gradient found. The current loss is:  0.3721068203449249\n",
      "Warning: nan gradient found. The current loss is:  0.30568838119506836\n",
      "Warning: nan gradient found. The current loss is:  0.3804299235343933\n",
      "Warning: nan gradient found. The current loss is:  0.21013745665550232\n",
      "Warning: nan gradient found. The current loss is:  1.6011202335357666\n",
      "Warning: nan gradient found. The current loss is:  0.9729671478271484\n",
      "Warning: nan gradient found. The current loss is:  0.3144763112068176\n",
      "Warning: nan gradient found. The current loss is:  0.27219080924987793\n",
      "Warning: nan gradient found. The current loss is:  0.6923601031303406\n",
      "Warning: nan gradient found. The current loss is:  0.47555768489837646\n",
      "Warning: nan gradient found. The current loss is:  0.36098340153694153\n",
      "Warning: nan gradient found. The current loss is:  0.8526700139045715\n",
      "Warning: nan gradient found. The current loss is:  0.5212551355361938\n",
      "Warning: nan gradient found. The current loss is:  0.1933969259262085\n",
      "Warning: nan gradient found. The current loss is:  0.3587534427642822\n",
      "Warning: nan gradient found. The current loss is:  0.34462034702301025\n",
      "Warning: nan gradient found. The current loss is:  0.33291420340538025\n",
      "Warning: nan gradient found. The current loss is:  0.21615490317344666\n",
      "Current batch training loss: 0.216155  [281600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.49997854232788086\n",
      "Warning: nan gradient found. The current loss is:  0.9893263578414917\n",
      "Warning: nan gradient found. The current loss is:  0.377341628074646\n",
      "Warning: nan gradient found. The current loss is:  0.28617343306541443\n",
      "Warning: nan gradient found. The current loss is:  1.5293514728546143\n",
      "Warning: nan gradient found. The current loss is:  0.31734713912010193\n",
      "Warning: nan gradient found. The current loss is:  0.6771970391273499\n",
      "Warning: nan gradient found. The current loss is:  0.4818546772003174\n",
      "Warning: nan gradient found. The current loss is:  0.3307102620601654\n",
      "Warning: nan gradient found. The current loss is:  0.4320273995399475\n",
      "Warning: nan gradient found. The current loss is:  0.20813100039958954\n",
      "Warning: nan gradient found. The current loss is:  0.5362719297409058\n",
      "Warning: nan gradient found. The current loss is:  0.5945670008659363\n",
      "Warning: nan gradient found. The current loss is:  1.3978610038757324\n",
      "Warning: nan gradient found. The current loss is:  0.55223548412323\n",
      "Warning: nan gradient found. The current loss is:  0.5295838713645935\n",
      "Warning: nan gradient found. The current loss is:  0.33233657479286194\n",
      "Warning: nan gradient found. The current loss is:  0.5631359219551086\n",
      "Warning: nan gradient found. The current loss is:  1.3737807273864746\n",
      "Warning: nan gradient found. The current loss is:  0.4537814259529114\n",
      "Warning: nan gradient found. The current loss is:  0.2692682147026062\n",
      "Warning: nan gradient found. The current loss is:  0.4277561902999878\n",
      "Warning: nan gradient found. The current loss is:  0.1511453092098236\n",
      "Warning: nan gradient found. The current loss is:  0.41679278016090393\n",
      "Warning: nan gradient found. The current loss is:  0.945741593837738\n",
      "Warning: nan gradient found. The current loss is:  0.26961055397987366\n",
      "Warning: nan gradient found. The current loss is:  0.6839801073074341\n",
      "Warning: nan gradient found. The current loss is:  0.3066571056842804\n",
      "Warning: nan gradient found. The current loss is:  0.946282684803009\n",
      "Warning: nan gradient found. The current loss is:  0.5558112263679504\n",
      "Warning: nan gradient found. The current loss is:  0.9541811943054199\n",
      "Warning: nan gradient found. The current loss is:  0.28058475255966187\n",
      "Warning: nan gradient found. The current loss is:  0.9819871187210083\n",
      "Warning: nan gradient found. The current loss is:  0.24239125847816467\n",
      "Warning: nan gradient found. The current loss is:  0.24072715640068054\n",
      "Warning: nan gradient found. The current loss is:  0.5366922616958618\n",
      "Warning: nan gradient found. The current loss is:  0.33561646938323975\n",
      "Warning: nan gradient found. The current loss is:  0.45545071363449097\n",
      "Warning: nan gradient found. The current loss is:  0.4587993025779724\n",
      "Warning: nan gradient found. The current loss is:  0.30689698457717896\n",
      "Warning: nan gradient found. The current loss is:  0.7018716335296631\n",
      "Warning: nan gradient found. The current loss is:  0.8402827978134155\n",
      "Warning: nan gradient found. The current loss is:  0.6819037199020386\n",
      "Warning: nan gradient found. The current loss is:  0.5709725618362427\n",
      "Warning: nan gradient found. The current loss is:  0.3038664758205414\n",
      "Warning: nan gradient found. The current loss is:  0.39349237084388733\n",
      "Warning: nan gradient found. The current loss is:  1.0529077053070068\n",
      "Warning: nan gradient found. The current loss is:  0.566049337387085\n",
      "Warning: nan gradient found. The current loss is:  1.0342613458633423\n",
      "Warning: nan gradient found. The current loss is:  0.9697344303131104\n",
      "Warning: nan gradient found. The current loss is:  0.5336657166481018\n",
      "Warning: nan gradient found. The current loss is:  0.46471327543258667\n",
      "Warning: nan gradient found. The current loss is:  0.6413554549217224\n",
      "Warning: nan gradient found. The current loss is:  0.3255907893180847\n",
      "Warning: nan gradient found. The current loss is:  0.5087733268737793\n",
      "Warning: nan gradient found. The current loss is:  0.2978873550891876\n",
      "Warning: nan gradient found. The current loss is:  0.5972923040390015\n",
      "Warning: nan gradient found. The current loss is:  0.7729959487915039\n",
      "Warning: nan gradient found. The current loss is:  0.32687658071517944\n",
      "Warning: nan gradient found. The current loss is:  0.42473405599594116\n",
      "Warning: nan gradient found. The current loss is:  0.49673742055892944\n",
      "Warning: nan gradient found. The current loss is:  0.4948520064353943\n",
      "Warning: nan gradient found. The current loss is:  0.29674506187438965\n",
      "Warning: nan gradient found. The current loss is:  0.19760069251060486\n",
      "Warning: nan gradient found. The current loss is:  0.6131267547607422\n",
      "Warning: nan gradient found. The current loss is:  0.3863537907600403\n",
      "Warning: nan gradient found. The current loss is:  0.5152098536491394\n",
      "Warning: nan gradient found. The current loss is:  0.8458065986633301\n",
      "Warning: nan gradient found. The current loss is:  0.6624543070793152\n",
      "Warning: nan gradient found. The current loss is:  0.10946111381053925\n",
      "Warning: nan gradient found. The current loss is:  1.0012884140014648\n",
      "Warning: nan gradient found. The current loss is:  -0.037825413048267365\n",
      "Warning: nan gradient found. The current loss is:  1.2503643035888672\n",
      "Warning: nan gradient found. The current loss is:  0.9030332565307617\n",
      "Warning: nan gradient found. The current loss is:  0.7985461354255676\n",
      "Warning: nan gradient found. The current loss is:  0.30631670355796814\n",
      "Warning: nan gradient found. The current loss is:  0.8579633831977844\n",
      "Warning: nan gradient found. The current loss is:  0.07025066018104553\n",
      "Warning: nan gradient found. The current loss is:  0.48788002133369446\n",
      "Warning: nan gradient found. The current loss is:  0.8723257184028625\n",
      "Warning: nan gradient found. The current loss is:  0.7415632009506226\n",
      "Warning: nan gradient found. The current loss is:  1.1622737646102905\n",
      "Warning: nan gradient found. The current loss is:  0.6347051858901978\n",
      "Warning: nan gradient found. The current loss is:  2.613076686859131\n",
      "Warning: nan gradient found. The current loss is:  0.39335137605667114\n",
      "Warning: nan gradient found. The current loss is:  1.2837647199630737\n",
      "Warning: nan gradient found. The current loss is:  0.2692883312702179\n",
      "Warning: nan gradient found. The current loss is:  0.40959644317626953\n",
      "Warning: nan gradient found. The current loss is:  0.3613375425338745\n",
      "Warning: nan gradient found. The current loss is:  0.4782373905181885\n",
      "Warning: nan gradient found. The current loss is:  0.5855363011360168\n",
      "Warning: nan gradient found. The current loss is:  0.34680941700935364\n",
      "Warning: nan gradient found. The current loss is:  0.5100245475769043\n",
      "Warning: nan gradient found. The current loss is:  0.43769288063049316\n",
      "Warning: nan gradient found. The current loss is:  0.4235854744911194\n",
      "Warning: nan gradient found. The current loss is:  0.9223376512527466\n",
      "Warning: nan gradient found. The current loss is:  0.5300546884536743\n",
      "Warning: nan gradient found. The current loss is:  0.6440478563308716\n",
      "Warning: nan gradient found. The current loss is:  0.3520994186401367\n",
      "Warning: nan gradient found. The current loss is:  0.21473698318004608\n",
      "Current batch training loss: 0.214737  [307200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9017214775085449\n",
      "Warning: nan gradient found. The current loss is:  1.0666465759277344\n",
      "Warning: nan gradient found. The current loss is:  0.9743134379386902\n",
      "Warning: nan gradient found. The current loss is:  1.2640568017959595\n",
      "Warning: nan gradient found. The current loss is:  0.6303836107254028\n",
      "Warning: nan gradient found. The current loss is:  0.057815391570329666\n",
      "Warning: nan gradient found. The current loss is:  0.26142609119415283\n",
      "Warning: nan gradient found. The current loss is:  0.2757841646671295\n",
      "Warning: nan gradient found. The current loss is:  0.20922163128852844\n",
      "Warning: nan gradient found. The current loss is:  0.6201382875442505\n",
      "Warning: nan gradient found. The current loss is:  0.8123023509979248\n",
      "Warning: nan gradient found. The current loss is:  0.2233821153640747\n",
      "Warning: nan gradient found. The current loss is:  0.6247056722640991\n",
      "Warning: nan gradient found. The current loss is:  0.4290456771850586\n",
      "Warning: nan gradient found. The current loss is:  0.1657368242740631\n",
      "Warning: nan gradient found. The current loss is:  0.1975933462381363\n",
      "Warning: nan gradient found. The current loss is:  0.33430027961730957\n",
      "Warning: nan gradient found. The current loss is:  0.878584086894989\n",
      "Warning: nan gradient found. The current loss is:  0.16043685376644135\n",
      "Warning: nan gradient found. The current loss is:  0.374399870634079\n",
      "Warning: nan gradient found. The current loss is:  0.7274760007858276\n",
      "Warning: nan gradient found. The current loss is:  0.1451622098684311\n",
      "Warning: nan gradient found. The current loss is:  0.9978751540184021\n",
      "Warning: nan gradient found. The current loss is:  0.7088488936424255\n",
      "Warning: nan gradient found. The current loss is:  0.8840325474739075\n",
      "Warning: nan gradient found. The current loss is:  -0.037172868847846985\n",
      "Warning: nan gradient found. The current loss is:  0.6979184150695801\n",
      "Warning: nan gradient found. The current loss is:  0.5904412269592285\n",
      "Warning: nan gradient found. The current loss is:  0.7281292080879211\n",
      "Warning: nan gradient found. The current loss is:  0.2958795428276062\n",
      "Warning: nan gradient found. The current loss is:  0.3717435598373413\n",
      "Warning: nan gradient found. The current loss is:  1.0311981439590454\n",
      "Warning: nan gradient found. The current loss is:  0.5817793011665344\n",
      "Warning: nan gradient found. The current loss is:  0.8085051774978638\n",
      "Warning: nan gradient found. The current loss is:  0.32324254512786865\n",
      "Warning: nan gradient found. The current loss is:  0.03924631327390671\n",
      "Warning: nan gradient found. The current loss is:  0.06962789595127106\n",
      "Warning: nan gradient found. The current loss is:  0.31535422801971436\n",
      "Warning: nan gradient found. The current loss is:  0.566908597946167\n",
      "Warning: nan gradient found. The current loss is:  0.5381807088851929\n",
      "Warning: nan gradient found. The current loss is:  0.8549282550811768\n",
      "Warning: nan gradient found. The current loss is:  0.8444533348083496\n",
      "Warning: nan gradient found. The current loss is:  0.32517337799072266\n",
      "Warning: nan gradient found. The current loss is:  0.19293330609798431\n",
      "Warning: nan gradient found. The current loss is:  0.9747921228408813\n",
      "Warning: nan gradient found. The current loss is:  0.8389061093330383\n",
      "Warning: nan gradient found. The current loss is:  0.6117163300514221\n",
      "Warning: nan gradient found. The current loss is:  0.5390428304672241\n",
      "Warning: nan gradient found. The current loss is:  0.14856068789958954\n",
      "Warning: nan gradient found. The current loss is:  0.4590052366256714\n",
      "Warning: nan gradient found. The current loss is:  0.4788946509361267\n",
      "Warning: nan gradient found. The current loss is:  0.1967717558145523\n",
      "Warning: nan gradient found. The current loss is:  0.5081748962402344\n",
      "Warning: nan gradient found. The current loss is:  1.5641865730285645\n",
      "Warning: nan gradient found. The current loss is:  0.37179267406463623\n",
      "Warning: nan gradient found. The current loss is:  0.43400460481643677\n",
      "Warning: nan gradient found. The current loss is:  0.5628433227539062\n",
      "Warning: nan gradient found. The current loss is:  0.9466850161552429\n",
      "Warning: nan gradient found. The current loss is:  0.5492219924926758\n",
      "Warning: nan gradient found. The current loss is:  0.774257481098175\n",
      "Warning: nan gradient found. The current loss is:  0.3363705277442932\n",
      "Warning: nan gradient found. The current loss is:  0.21077801287174225\n",
      "Warning: nan gradient found. The current loss is:  2.182934284210205\n",
      "Warning: nan gradient found. The current loss is:  0.9815597534179688\n",
      "Warning: nan gradient found. The current loss is:  1.4374759197235107\n",
      "Warning: nan gradient found. The current loss is:  0.306874543428421\n",
      "Warning: nan gradient found. The current loss is:  0.15890225768089294\n",
      "Warning: nan gradient found. The current loss is:  0.5135751366615295\n",
      "Warning: nan gradient found. The current loss is:  0.1729845404624939\n",
      "Warning: nan gradient found. The current loss is:  0.6672956347465515\n",
      "Warning: nan gradient found. The current loss is:  0.6811676025390625\n",
      "Warning: nan gradient found. The current loss is:  0.998208224773407\n",
      "Warning: nan gradient found. The current loss is:  0.03409533575177193\n",
      "Warning: nan gradient found. The current loss is:  0.2867487668991089\n",
      "Warning: nan gradient found. The current loss is:  0.9430432915687561\n",
      "Warning: nan gradient found. The current loss is:  0.34592312574386597\n",
      "Warning: nan gradient found. The current loss is:  1.4727990627288818\n",
      "Warning: nan gradient found. The current loss is:  0.20435765385627747\n",
      "Warning: nan gradient found. The current loss is:  0.851620078086853\n",
      "Warning: nan gradient found. The current loss is:  0.6689838171005249\n",
      "Warning: nan gradient found. The current loss is:  0.158840149641037\n",
      "Warning: nan gradient found. The current loss is:  0.31730371713638306\n",
      "Warning: nan gradient found. The current loss is:  0.44364026188850403\n",
      "Warning: nan gradient found. The current loss is:  0.5996822118759155\n",
      "Warning: nan gradient found. The current loss is:  0.6093841791152954\n",
      "Warning: nan gradient found. The current loss is:  0.9505576491355896\n",
      "Warning: nan gradient found. The current loss is:  0.3531993627548218\n",
      "Warning: nan gradient found. The current loss is:  0.3216136693954468\n",
      "Warning: nan gradient found. The current loss is:  0.45439597964286804\n",
      "Warning: nan gradient found. The current loss is:  0.47242599725723267\n",
      "Warning: nan gradient found. The current loss is:  0.21136273443698883\n",
      "Warning: nan gradient found. The current loss is:  0.2893187403678894\n",
      "Warning: nan gradient found. The current loss is:  0.5112834572792053\n",
      "Warning: nan gradient found. The current loss is:  0.34550103545188904\n",
      "Warning: nan gradient found. The current loss is:  0.3512948155403137\n",
      "Warning: nan gradient found. The current loss is:  1.5693362951278687\n",
      "Warning: nan gradient found. The current loss is:  0.8164260387420654\n",
      "Warning: nan gradient found. The current loss is:  0.2565512955188751\n",
      "Warning: nan gradient found. The current loss is:  0.12504835426807404\n",
      "Warning: nan gradient found. The current loss is:  0.5043898820877075\n",
      "Current batch training loss: 0.504390  [332800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6617110371589661\n",
      "Warning: nan gradient found. The current loss is:  1.1748121976852417\n",
      "Warning: nan gradient found. The current loss is:  0.7842579483985901\n",
      "Warning: nan gradient found. The current loss is:  0.4852221608161926\n",
      "Warning: nan gradient found. The current loss is:  0.3158406913280487\n",
      "Warning: nan gradient found. The current loss is:  0.4788628816604614\n",
      "Warning: nan gradient found. The current loss is:  0.28503450751304626\n",
      "Warning: nan gradient found. The current loss is:  -0.0914585143327713\n",
      "Warning: nan gradient found. The current loss is:  0.3761122226715088\n",
      "Warning: nan gradient found. The current loss is:  0.6243653297424316\n",
      "Warning: nan gradient found. The current loss is:  0.20535191893577576\n",
      "Warning: nan gradient found. The current loss is:  0.1787208467721939\n",
      "Warning: nan gradient found. The current loss is:  0.5064924955368042\n",
      "Warning: nan gradient found. The current loss is:  2.181880474090576\n",
      "Warning: nan gradient found. The current loss is:  0.6427475810050964\n",
      "Warning: nan gradient found. The current loss is:  0.24968568980693817\n",
      "Warning: nan gradient found. The current loss is:  0.5541186332702637\n",
      "Warning: nan gradient found. The current loss is:  0.19801102578639984\n",
      "Warning: nan gradient found. The current loss is:  0.8617289662361145\n",
      "Warning: nan gradient found. The current loss is:  0.9745182991027832\n",
      "Warning: nan gradient found. The current loss is:  0.4464651346206665\n",
      "Warning: nan gradient found. The current loss is:  1.0941336154937744\n",
      "Warning: nan gradient found. The current loss is:  0.5001645088195801\n",
      "Warning: nan gradient found. The current loss is:  0.7835350632667542\n",
      "Warning: nan gradient found. The current loss is:  0.33114898204803467\n",
      "Warning: nan gradient found. The current loss is:  0.8571542501449585\n",
      "Warning: nan gradient found. The current loss is:  1.1117933988571167\n",
      "Warning: nan gradient found. The current loss is:  0.9353010058403015\n",
      "Warning: nan gradient found. The current loss is:  0.76222163438797\n",
      "Warning: nan gradient found. The current loss is:  0.5996214151382446\n",
      "Warning: nan gradient found. The current loss is:  0.4279698133468628\n",
      "Warning: nan gradient found. The current loss is:  0.3181496262550354\n",
      "Warning: nan gradient found. The current loss is:  0.5166680812835693\n",
      "Warning: nan gradient found. The current loss is:  0.22702977061271667\n",
      "Warning: nan gradient found. The current loss is:  0.49554380774497986\n",
      "Warning: nan gradient found. The current loss is:  0.4267335534095764\n",
      "Warning: nan gradient found. The current loss is:  0.8789838552474976\n",
      "Warning: nan gradient found. The current loss is:  0.267524778842926\n",
      "Warning: nan gradient found. The current loss is:  1.0908817052841187\n",
      "Warning: nan gradient found. The current loss is:  0.2573784589767456\n",
      "Warning: nan gradient found. The current loss is:  0.434436559677124\n",
      "Warning: nan gradient found. The current loss is:  0.2436365932226181\n",
      "Warning: nan gradient found. The current loss is:  0.3014910817146301\n",
      "Warning: nan gradient found. The current loss is:  0.6456097364425659\n",
      "Warning: nan gradient found. The current loss is:  0.6801722049713135\n",
      "Warning: nan gradient found. The current loss is:  0.7384664416313171\n",
      "Warning: nan gradient found. The current loss is:  0.7491189241409302\n",
      "Warning: nan gradient found. The current loss is:  0.24382933974266052\n",
      "Warning: nan gradient found. The current loss is:  0.37640202045440674\n",
      "Warning: nan gradient found. The current loss is:  0.310685396194458\n",
      "Warning: nan gradient found. The current loss is:  0.46274417638778687\n",
      "Warning: nan gradient found. The current loss is:  0.16465213894844055\n",
      "Warning: nan gradient found. The current loss is:  0.27944350242614746\n",
      "Warning: nan gradient found. The current loss is:  0.39702466130256653\n",
      "Warning: nan gradient found. The current loss is:  0.5745581984519958\n",
      "Warning: nan gradient found. The current loss is:  1.139236330986023\n",
      "Warning: nan gradient found. The current loss is:  1.4005260467529297\n",
      "Warning: nan gradient found. The current loss is:  0.5597072243690491\n",
      "Warning: nan gradient found. The current loss is:  0.24289897084236145\n",
      "Warning: nan gradient found. The current loss is:  1.2291474342346191\n",
      "Warning: nan gradient found. The current loss is:  0.449371874332428\n",
      "Warning: nan gradient found. The current loss is:  0.2183590978384018\n",
      "Warning: nan gradient found. The current loss is:  1.2460665702819824\n",
      "Warning: nan gradient found. The current loss is:  0.40887418389320374\n",
      "Warning: nan gradient found. The current loss is:  0.6569963693618774\n",
      "Warning: nan gradient found. The current loss is:  1.1853444576263428\n",
      "Warning: nan gradient found. The current loss is:  0.36885303258895874\n",
      "Warning: nan gradient found. The current loss is:  0.28947868943214417\n",
      "Warning: nan gradient found. The current loss is:  0.3978559374809265\n",
      "Warning: nan gradient found. The current loss is:  0.06593877077102661\n",
      "Warning: nan gradient found. The current loss is:  0.2559143900871277\n",
      "Warning: nan gradient found. The current loss is:  0.5370547771453857\n",
      "Warning: nan gradient found. The current loss is:  0.16909687221050262\n",
      "Warning: nan gradient found. The current loss is:  0.5305684804916382\n",
      "Warning: nan gradient found. The current loss is:  0.9793728590011597\n",
      "Warning: nan gradient found. The current loss is:  0.17397694289684296\n",
      "Warning: nan gradient found. The current loss is:  1.1083306074142456\n",
      "Warning: nan gradient found. The current loss is:  0.5516900420188904\n",
      "Warning: nan gradient found. The current loss is:  0.4083249866962433\n",
      "Warning: nan gradient found. The current loss is:  0.44868481159210205\n",
      "Warning: nan gradient found. The current loss is:  0.8763135075569153\n",
      "Warning: nan gradient found. The current loss is:  0.24531304836273193\n",
      "Warning: nan gradient found. The current loss is:  0.365763783454895\n",
      "Warning: nan gradient found. The current loss is:  0.373470276594162\n",
      "Warning: nan gradient found. The current loss is:  0.6868444681167603\n",
      "Warning: nan gradient found. The current loss is:  0.17280566692352295\n",
      "Warning: nan gradient found. The current loss is:  0.47889330983161926\n",
      "Warning: nan gradient found. The current loss is:  0.559283435344696\n",
      "Warning: nan gradient found. The current loss is:  1.2195086479187012\n",
      "Warning: nan gradient found. The current loss is:  1.2805500030517578\n",
      "Warning: nan gradient found. The current loss is:  0.006230168044567108\n",
      "Warning: nan gradient found. The current loss is:  0.28969883918762207\n",
      "Warning: nan gradient found. The current loss is:  0.3487744927406311\n",
      "Warning: nan gradient found. The current loss is:  1.030272364616394\n",
      "Warning: nan gradient found. The current loss is:  0.6033746600151062\n",
      "Warning: nan gradient found. The current loss is:  0.31791895627975464\n",
      "Warning: nan gradient found. The current loss is:  0.6407926678657532\n",
      "Warning: nan gradient found. The current loss is:  0.11990031599998474\n",
      "Warning: nan gradient found. The current loss is:  0.3775256872177124\n",
      "Warning: nan gradient found. The current loss is:  1.3106242418289185\n",
      "Current batch training loss: 1.310624  [358400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.1008307933807373\n",
      "Warning: nan gradient found. The current loss is:  0.7233959436416626\n",
      "Warning: nan gradient found. The current loss is:  0.188222274184227\n",
      "Warning: nan gradient found. The current loss is:  1.036860466003418\n",
      "Warning: nan gradient found. The current loss is:  0.73079913854599\n",
      "Warning: nan gradient found. The current loss is:  0.24607254564762115\n",
      "Warning: nan gradient found. The current loss is:  0.0471324548125267\n",
      "Warning: nan gradient found. The current loss is:  0.44277018308639526\n",
      "Warning: nan gradient found. The current loss is:  0.21123440563678741\n",
      "Warning: nan gradient found. The current loss is:  0.5760443806648254\n",
      "Warning: nan gradient found. The current loss is:  1.2687430381774902\n",
      "Warning: nan gradient found. The current loss is:  0.34077975153923035\n",
      "Warning: nan gradient found. The current loss is:  0.44059205055236816\n",
      "Warning: nan gradient found. The current loss is:  0.1372140347957611\n",
      "Warning: nan gradient found. The current loss is:  0.0923544242978096\n",
      "Warning: nan gradient found. The current loss is:  0.7031255960464478\n",
      "Warning: nan gradient found. The current loss is:  0.4695528745651245\n",
      "Warning: nan gradient found. The current loss is:  0.2820231318473816\n",
      "Warning: nan gradient found. The current loss is:  0.5536928176879883\n",
      "Warning: nan gradient found. The current loss is:  1.1808149814605713\n",
      "Warning: nan gradient found. The current loss is:  0.4760407507419586\n",
      "Warning: nan gradient found. The current loss is:  0.37419241666793823\n",
      "Warning: nan gradient found. The current loss is:  0.48977726697921753\n",
      "Warning: nan gradient found. The current loss is:  0.8127782940864563\n",
      "Warning: nan gradient found. The current loss is:  1.4868865013122559\n",
      "Warning: nan gradient found. The current loss is:  0.6274641752243042\n",
      "Warning: nan gradient found. The current loss is:  0.31428977847099304\n",
      "Warning: nan gradient found. The current loss is:  0.5644283294677734\n",
      "Warning: nan gradient found. The current loss is:  0.20533214509487152\n",
      "Warning: nan gradient found. The current loss is:  0.7239477038383484\n",
      "Warning: nan gradient found. The current loss is:  1.0745809078216553\n",
      "Warning: nan gradient found. The current loss is:  0.26692357659339905\n",
      "Warning: nan gradient found. The current loss is:  0.2753826081752777\n",
      "Warning: nan gradient found. The current loss is:  0.05122891813516617\n",
      "Warning: nan gradient found. The current loss is:  0.2699444890022278\n",
      "Warning: nan gradient found. The current loss is:  0.12191998958587646\n",
      "Warning: nan gradient found. The current loss is:  0.3767153024673462\n",
      "Warning: nan gradient found. The current loss is:  0.21773135662078857\n",
      "Warning: nan gradient found. The current loss is:  0.5144212245941162\n",
      "Warning: nan gradient found. The current loss is:  0.37744346261024475\n",
      "Warning: nan gradient found. The current loss is:  0.8503373861312866\n",
      "Warning: nan gradient found. The current loss is:  0.8040323257446289\n",
      "Warning: nan gradient found. The current loss is:  0.8526535034179688\n",
      "Warning: nan gradient found. The current loss is:  0.4781634211540222\n",
      "Warning: nan gradient found. The current loss is:  1.0447124242782593\n",
      "Warning: nan gradient found. The current loss is:  0.4628845453262329\n",
      "Warning: nan gradient found. The current loss is:  0.35386231541633606\n",
      "Warning: nan gradient found. The current loss is:  0.3013533353805542\n",
      "Warning: nan gradient found. The current loss is:  0.5932394862174988\n",
      "Warning: nan gradient found. The current loss is:  0.6610543727874756\n",
      "Warning: nan gradient found. The current loss is:  0.48838528990745544\n",
      "Warning: nan gradient found. The current loss is:  0.7695847749710083\n",
      "Warning: nan gradient found. The current loss is:  1.1923153400421143\n",
      "Warning: nan gradient found. The current loss is:  0.33341482281684875\n",
      "Warning: nan gradient found. The current loss is:  0.682235836982727\n",
      "Warning: nan gradient found. The current loss is:  0.2672721743583679\n",
      "Warning: nan gradient found. The current loss is:  0.7831190228462219\n",
      "Warning: nan gradient found. The current loss is:  1.2016668319702148\n",
      "Warning: nan gradient found. The current loss is:  0.22398146986961365\n",
      "Warning: nan gradient found. The current loss is:  1.99019193649292\n",
      "Warning: nan gradient found. The current loss is:  0.29060792922973633\n",
      "Warning: nan gradient found. The current loss is:  0.541954755783081\n",
      "Warning: nan gradient found. The current loss is:  0.6424122452735901\n",
      "Warning: nan gradient found. The current loss is:  0.8206130862236023\n",
      "Warning: nan gradient found. The current loss is:  1.0251225233078003\n",
      "Warning: nan gradient found. The current loss is:  0.263412207365036\n",
      "Warning: nan gradient found. The current loss is:  0.7459160685539246\n",
      "Warning: nan gradient found. The current loss is:  0.4283314347267151\n",
      "Warning: nan gradient found. The current loss is:  0.6158434152603149\n",
      "Warning: nan gradient found. The current loss is:  0.4464048147201538\n",
      "Warning: nan gradient found. The current loss is:  0.8607364892959595\n",
      "Warning: nan gradient found. The current loss is:  0.6927053928375244\n",
      "Warning: nan gradient found. The current loss is:  0.18736493587493896\n",
      "Warning: nan gradient found. The current loss is:  0.8105113506317139\n",
      "Warning: nan gradient found. The current loss is:  0.44742393493652344\n",
      "Warning: nan gradient found. The current loss is:  0.6737973690032959\n",
      "Warning: nan gradient found. The current loss is:  0.1820661574602127\n",
      "Warning: nan gradient found. The current loss is:  0.3358607292175293\n",
      "Warning: nan gradient found. The current loss is:  0.37601375579833984\n",
      "Warning: nan gradient found. The current loss is:  0.2377287596464157\n",
      "Warning: nan gradient found. The current loss is:  0.6962306499481201\n",
      "Warning: nan gradient found. The current loss is:  0.8871248364448547\n",
      "Warning: nan gradient found. The current loss is:  0.45011311769485474\n",
      "Warning: nan gradient found. The current loss is:  0.33061254024505615\n",
      "Warning: nan gradient found. The current loss is:  0.17550323903560638\n",
      "Warning: nan gradient found. The current loss is:  0.5556780099868774\n",
      "Warning: nan gradient found. The current loss is:  0.4980156123638153\n",
      "Warning: nan gradient found. The current loss is:  0.774211049079895\n",
      "Warning: nan gradient found. The current loss is:  0.21497614681720734\n",
      "Warning: nan gradient found. The current loss is:  0.2913619875907898\n",
      "Warning: nan gradient found. The current loss is:  0.41595688462257385\n",
      "Warning: nan gradient found. The current loss is:  0.18382911384105682\n",
      "Warning: nan gradient found. The current loss is:  0.6011661291122437\n",
      "Warning: nan gradient found. The current loss is:  1.6789849996566772\n",
      "Warning: nan gradient found. The current loss is:  0.6447362303733826\n",
      "Warning: nan gradient found. The current loss is:  1.8109962940216064\n",
      "Warning: nan gradient found. The current loss is:  0.0993979275226593\n",
      "Warning: nan gradient found. The current loss is:  1.0029855966567993\n",
      "Warning: nan gradient found. The current loss is:  0.6647121906280518\n",
      "Warning: nan gradient found. The current loss is:  0.44830194115638733\n",
      "Current batch training loss: 0.448302  [384000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.21925947070121765\n",
      "Warning: nan gradient found. The current loss is:  0.47969385981559753\n",
      "Warning: nan gradient found. The current loss is:  0.41632819175720215\n",
      "Warning: nan gradient found. The current loss is:  0.07600962370634079\n",
      "Warning: nan gradient found. The current loss is:  0.2835424840450287\n",
      "Warning: nan gradient found. The current loss is:  0.15365341305732727\n",
      "Warning: nan gradient found. The current loss is:  0.20437464118003845\n",
      "Warning: nan gradient found. The current loss is:  0.7027499675750732\n",
      "Warning: nan gradient found. The current loss is:  0.5133301019668579\n",
      "Warning: nan gradient found. The current loss is:  1.0777528285980225\n",
      "Warning: nan gradient found. The current loss is:  0.4255029559135437\n",
      "Warning: nan gradient found. The current loss is:  0.6140041947364807\n",
      "Warning: nan gradient found. The current loss is:  0.5716984868049622\n",
      "Warning: nan gradient found. The current loss is:  0.41610705852508545\n",
      "Warning: nan gradient found. The current loss is:  0.9419288635253906\n",
      "Warning: nan gradient found. The current loss is:  0.7463147640228271\n",
      "Warning: nan gradient found. The current loss is:  0.2970001697540283\n",
      "Warning: nan gradient found. The current loss is:  0.7411316633224487\n",
      "Warning: nan gradient found. The current loss is:  0.09267977625131607\n",
      "Warning: nan gradient found. The current loss is:  0.2999974489212036\n",
      "Warning: nan gradient found. The current loss is:  1.593419075012207\n",
      "Warning: nan gradient found. The current loss is:  0.8048049211502075\n",
      "Warning: nan gradient found. The current loss is:  0.12913630902767181\n",
      "Warning: nan gradient found. The current loss is:  0.4853186011314392\n",
      "Warning: nan gradient found. The current loss is:  0.46652594208717346\n",
      "Warning: nan gradient found. The current loss is:  0.34133246541023254\n",
      "Warning: nan gradient found. The current loss is:  1.6773605346679688\n",
      "Warning: nan gradient found. The current loss is:  0.14025487005710602\n",
      "Warning: nan gradient found. The current loss is:  2.6927449703216553\n",
      "Warning: nan gradient found. The current loss is:  0.38475143909454346\n",
      "Warning: nan gradient found. The current loss is:  0.3626376986503601\n",
      "Warning: nan gradient found. The current loss is:  0.2795778512954712\n",
      "Warning: nan gradient found. The current loss is:  0.6269174814224243\n",
      "Warning: nan gradient found. The current loss is:  0.3783716559410095\n",
      "Warning: nan gradient found. The current loss is:  0.5859709978103638\n",
      "Warning: nan gradient found. The current loss is:  0.5458332300186157\n",
      "Warning: nan gradient found. The current loss is:  0.5492544174194336\n",
      "Warning: nan gradient found. The current loss is:  0.37284064292907715\n",
      "Warning: nan gradient found. The current loss is:  1.288712739944458\n",
      "Warning: nan gradient found. The current loss is:  0.5223395228385925\n",
      "Warning: nan gradient found. The current loss is:  0.5218459963798523\n",
      "Warning: nan gradient found. The current loss is:  0.2522371709346771\n",
      "Warning: nan gradient found. The current loss is:  0.782058835029602\n",
      "Warning: nan gradient found. The current loss is:  0.8091627359390259\n",
      "Warning: nan gradient found. The current loss is:  0.2780245542526245\n",
      "Warning: nan gradient found. The current loss is:  0.7783613801002502\n",
      "Warning: nan gradient found. The current loss is:  0.4875519871711731\n",
      "Warning: nan gradient found. The current loss is:  0.7621160745620728\n",
      "Warning: nan gradient found. The current loss is:  0.6927531361579895\n",
      "Warning: nan gradient found. The current loss is:  0.21571621298789978\n",
      "Warning: nan gradient found. The current loss is:  1.0814390182495117\n",
      "Warning: nan gradient found. The current loss is:  0.6055217385292053\n",
      "Warning: nan gradient found. The current loss is:  0.7443076372146606\n",
      "Warning: nan gradient found. The current loss is:  0.18655315041542053\n",
      "Warning: nan gradient found. The current loss is:  0.2899359464645386\n",
      "Warning: nan gradient found. The current loss is:  0.37632954120635986\n",
      "Warning: nan gradient found. The current loss is:  0.7669380903244019\n",
      "Warning: nan gradient found. The current loss is:  0.055150896310806274\n",
      "Warning: nan gradient found. The current loss is:  0.4246097803115845\n",
      "Warning: nan gradient found. The current loss is:  0.47921887040138245\n",
      "Warning: nan gradient found. The current loss is:  0.07760171592235565\n",
      "Warning: nan gradient found. The current loss is:  -0.0023029036819934845\n",
      "Warning: nan gradient found. The current loss is:  1.4470891952514648\n",
      "Warning: nan gradient found. The current loss is:  0.3664613366127014\n",
      "Warning: nan gradient found. The current loss is:  0.2869037985801697\n",
      "Warning: nan gradient found. The current loss is:  1.1676381826400757\n",
      "Warning: nan gradient found. The current loss is:  0.6138982176780701\n",
      "Warning: nan gradient found. The current loss is:  1.1094248294830322\n",
      "Warning: nan gradient found. The current loss is:  0.1918763965368271\n",
      "Warning: nan gradient found. The current loss is:  0.6573118567466736\n",
      "Warning: nan gradient found. The current loss is:  0.6234689354896545\n",
      "Warning: nan gradient found. The current loss is:  0.5896437764167786\n",
      "Warning: nan gradient found. The current loss is:  1.1817972660064697\n",
      "Warning: nan gradient found. The current loss is:  1.3310328722000122\n",
      "Warning: nan gradient found. The current loss is:  0.3721809387207031\n",
      "Warning: nan gradient found. The current loss is:  -0.09266945719718933\n",
      "Warning: nan gradient found. The current loss is:  0.9664571285247803\n",
      "Warning: nan gradient found. The current loss is:  0.6717739701271057\n",
      "Warning: nan gradient found. The current loss is:  0.213283970952034\n",
      "Warning: nan gradient found. The current loss is:  0.32121583819389343\n",
      "Warning: nan gradient found. The current loss is:  0.3580376207828522\n",
      "Warning: nan gradient found. The current loss is:  0.6369161009788513\n",
      "Warning: nan gradient found. The current loss is:  0.7509047389030457\n",
      "Warning: nan gradient found. The current loss is:  0.29140427708625793\n",
      "Warning: nan gradient found. The current loss is:  0.16951629519462585\n",
      "Warning: nan gradient found. The current loss is:  0.5333386659622192\n",
      "Warning: nan gradient found. The current loss is:  0.5194016098976135\n",
      "Warning: nan gradient found. The current loss is:  0.5965929627418518\n",
      "Warning: nan gradient found. The current loss is:  0.07251959294080734\n",
      "Warning: nan gradient found. The current loss is:  0.47649744153022766\n",
      "Warning: nan gradient found. The current loss is:  1.3202812671661377\n",
      "Warning: nan gradient found. The current loss is:  0.21045418083667755\n",
      "Warning: nan gradient found. The current loss is:  0.7304315567016602\n",
      "Warning: nan gradient found. The current loss is:  0.6912556886672974\n",
      "Warning: nan gradient found. The current loss is:  0.4728372395038605\n",
      "Warning: nan gradient found. The current loss is:  0.7513564825057983\n",
      "Warning: nan gradient found. The current loss is:  0.3362058401107788\n",
      "Warning: nan gradient found. The current loss is:  0.11521892994642258\n",
      "Warning: nan gradient found. The current loss is:  0.37311792373657227\n",
      "Warning: nan gradient found. The current loss is:  0.6430619955062866\n",
      "Current batch training loss: 0.643062  [409600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.3183547854423523\n",
      "Warning: nan gradient found. The current loss is:  0.06843659281730652\n",
      "Warning: nan gradient found. The current loss is:  0.657345175743103\n",
      "Warning: nan gradient found. The current loss is:  0.2723205089569092\n",
      "Warning: nan gradient found. The current loss is:  0.2962605357170105\n",
      "Warning: nan gradient found. The current loss is:  0.1081402376294136\n",
      "Warning: nan gradient found. The current loss is:  0.3483909070491791\n",
      "Warning: nan gradient found. The current loss is:  0.8995946645736694\n",
      "Warning: nan gradient found. The current loss is:  0.21405552327632904\n",
      "Warning: nan gradient found. The current loss is:  0.5412935018539429\n",
      "Warning: nan gradient found. The current loss is:  0.4869843125343323\n",
      "Warning: nan gradient found. The current loss is:  1.1545686721801758\n",
      "Warning: nan gradient found. The current loss is:  1.2112233638763428\n",
      "Warning: nan gradient found. The current loss is:  1.1060271263122559\n",
      "Warning: nan gradient found. The current loss is:  0.8343279361724854\n",
      "Warning: nan gradient found. The current loss is:  1.0545754432678223\n",
      "Warning: nan gradient found. The current loss is:  0.43613380193710327\n",
      "Warning: nan gradient found. The current loss is:  0.19824662804603577\n",
      "Warning: nan gradient found. The current loss is:  0.25810152292251587\n",
      "Warning: nan gradient found. The current loss is:  0.5660421252250671\n",
      "Warning: nan gradient found. The current loss is:  0.7866107225418091\n",
      "Warning: nan gradient found. The current loss is:  0.19034239649772644\n",
      "Warning: nan gradient found. The current loss is:  0.9446075558662415\n",
      "Warning: nan gradient found. The current loss is:  0.46124476194381714\n",
      "Warning: nan gradient found. The current loss is:  1.1098934412002563\n",
      "Warning: nan gradient found. The current loss is:  0.5852290391921997\n",
      "Warning: nan gradient found. The current loss is:  0.02738378942012787\n",
      "Warning: nan gradient found. The current loss is:  0.36306673288345337\n",
      "Warning: nan gradient found. The current loss is:  0.3650236129760742\n",
      "Warning: nan gradient found. The current loss is:  0.40036895871162415\n",
      "Warning: nan gradient found. The current loss is:  0.48533958196640015\n",
      "Warning: nan gradient found. The current loss is:  0.46673041582107544\n",
      "Warning: nan gradient found. The current loss is:  0.4733961820602417\n",
      "Warning: nan gradient found. The current loss is:  0.30958670377731323\n",
      "Warning: nan gradient found. The current loss is:  0.3590248227119446\n",
      "Warning: nan gradient found. The current loss is:  0.3729484975337982\n",
      "Warning: nan gradient found. The current loss is:  0.40539252758026123\n",
      "Warning: nan gradient found. The current loss is:  0.4200800657272339\n",
      "Warning: nan gradient found. The current loss is:  0.7519915699958801\n",
      "Warning: nan gradient found. The current loss is:  0.5599304437637329\n",
      "Warning: nan gradient found. The current loss is:  0.5818185210227966\n",
      "Warning: nan gradient found. The current loss is:  0.3541376292705536\n",
      "Warning: nan gradient found. The current loss is:  0.24591164290905\n",
      "Warning: nan gradient found. The current loss is:  1.0637130737304688\n",
      "Warning: nan gradient found. The current loss is:  0.7870817184448242\n",
      "Warning: nan gradient found. The current loss is:  0.5977399945259094\n",
      "Warning: nan gradient found. The current loss is:  1.5608551502227783\n",
      "Warning: nan gradient found. The current loss is:  0.8527423143386841\n",
      "Warning: nan gradient found. The current loss is:  0.7993828058242798\n",
      "Warning: nan gradient found. The current loss is:  0.5993806719779968\n",
      "Warning: nan gradient found. The current loss is:  0.41364210844039917\n",
      "Warning: nan gradient found. The current loss is:  0.5966958403587341\n",
      "Warning: nan gradient found. The current loss is:  0.4729465842247009\n",
      "Warning: nan gradient found. The current loss is:  0.3004927635192871\n",
      "Warning: nan gradient found. The current loss is:  0.5997508764266968\n",
      "Warning: nan gradient found. The current loss is:  0.101213239133358\n",
      "Warning: nan gradient found. The current loss is:  1.7456778287887573\n",
      "Warning: nan gradient found. The current loss is:  0.6777819991111755\n",
      "Warning: nan gradient found. The current loss is:  0.8050209283828735\n",
      "Warning: nan gradient found. The current loss is:  0.6901704668998718\n",
      "Warning: nan gradient found. The current loss is:  0.40430158376693726\n",
      "Warning: nan gradient found. The current loss is:  0.6483187079429626\n",
      "Warning: nan gradient found. The current loss is:  0.7871285080909729\n",
      "Warning: nan gradient found. The current loss is:  0.5660436153411865\n",
      "Warning: nan gradient found. The current loss is:  0.1233394593000412\n",
      "Warning: nan gradient found. The current loss is:  0.8997238278388977\n",
      "Warning: nan gradient found. The current loss is:  0.6088752746582031\n",
      "Warning: nan gradient found. The current loss is:  0.06327401101589203\n",
      "Warning: nan gradient found. The current loss is:  0.5156022310256958\n",
      "Warning: nan gradient found. The current loss is:  0.7041721343994141\n",
      "Warning: nan gradient found. The current loss is:  0.4633810520172119\n",
      "Warning: nan gradient found. The current loss is:  0.8955755829811096\n",
      "Warning: nan gradient found. The current loss is:  0.2873838543891907\n",
      "Warning: nan gradient found. The current loss is:  1.073926568031311\n",
      "Warning: nan gradient found. The current loss is:  0.28252169489860535\n",
      "Warning: nan gradient found. The current loss is:  1.3576105833053589\n",
      "Warning: nan gradient found. The current loss is:  0.4062729775905609\n",
      "Warning: nan gradient found. The current loss is:  0.34445974230766296\n",
      "Warning: nan gradient found. The current loss is:  0.8224201202392578\n",
      "Warning: nan gradient found. The current loss is:  1.4259275197982788\n",
      "Warning: nan gradient found. The current loss is:  0.7010871171951294\n",
      "Warning: nan gradient found. The current loss is:  1.715726613998413\n",
      "Warning: nan gradient found. The current loss is:  0.9289253950119019\n",
      "Warning: nan gradient found. The current loss is:  0.3136899471282959\n",
      "Warning: nan gradient found. The current loss is:  0.1978585124015808\n",
      "Warning: nan gradient found. The current loss is:  0.38974642753601074\n",
      "Warning: nan gradient found. The current loss is:  0.30253756046295166\n",
      "Warning: nan gradient found. The current loss is:  0.03861452266573906\n",
      "Warning: nan gradient found. The current loss is:  0.5402673482894897\n",
      "Warning: nan gradient found. The current loss is:  1.0752599239349365\n",
      "Warning: nan gradient found. The current loss is:  0.4818076491355896\n",
      "Warning: nan gradient found. The current loss is:  1.0408921241760254\n",
      "Warning: nan gradient found. The current loss is:  0.6187757253646851\n",
      "Warning: nan gradient found. The current loss is:  0.5433496236801147\n",
      "Warning: nan gradient found. The current loss is:  0.8003389239311218\n",
      "Warning: nan gradient found. The current loss is:  0.15674930810928345\n",
      "Warning: nan gradient found. The current loss is:  0.6498810648918152\n",
      "Warning: nan gradient found. The current loss is:  0.4234965443611145\n",
      "Warning: nan gradient found. The current loss is:  0.22413098812103271\n",
      "Warning: nan gradient found. The current loss is:  0.06251302361488342\n",
      "Current batch training loss: 0.062513  [435200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9290554523468018\n",
      "Warning: nan gradient found. The current loss is:  0.7560584545135498\n",
      "Warning: nan gradient found. The current loss is:  0.3612343668937683\n",
      "Warning: nan gradient found. The current loss is:  0.2548026442527771\n",
      "Warning: nan gradient found. The current loss is:  0.31415778398513794\n",
      "Warning: nan gradient found. The current loss is:  0.6465697288513184\n",
      "Warning: nan gradient found. The current loss is:  0.38639193773269653\n",
      "Warning: nan gradient found. The current loss is:  0.1809980869293213\n",
      "Warning: nan gradient found. The current loss is:  0.4224415123462677\n",
      "Warning: nan gradient found. The current loss is:  0.299907922744751\n",
      "Warning: nan gradient found. The current loss is:  0.41604870557785034\n",
      "Warning: nan gradient found. The current loss is:  0.25436216592788696\n",
      "Warning: nan gradient found. The current loss is:  -0.020035196095705032\n",
      "Warning: nan gradient found. The current loss is:  1.2570455074310303\n",
      "Warning: nan gradient found. The current loss is:  0.4238075911998749\n",
      "Warning: nan gradient found. The current loss is:  0.6007905006408691\n",
      "Warning: nan gradient found. The current loss is:  1.4834396839141846\n",
      "Warning: nan gradient found. The current loss is:  2.2283079624176025\n",
      "Warning: nan gradient found. The current loss is:  0.18041154742240906\n",
      "Warning: nan gradient found. The current loss is:  1.1411314010620117\n",
      "Warning: nan gradient found. The current loss is:  0.7680809497833252\n",
      "Warning: nan gradient found. The current loss is:  2.03145694732666\n",
      "Warning: nan gradient found. The current loss is:  1.315136194229126\n",
      "Warning: nan gradient found. The current loss is:  0.3392239212989807\n",
      "Warning: nan gradient found. The current loss is:  0.45545652508735657\n",
      "Warning: nan gradient found. The current loss is:  0.23994798958301544\n",
      "Warning: nan gradient found. The current loss is:  0.4408484101295471\n",
      "Warning: nan gradient found. The current loss is:  0.7441936135292053\n",
      "Warning: nan gradient found. The current loss is:  0.42005592584609985\n",
      "Warning: nan gradient found. The current loss is:  0.920892596244812\n",
      "Warning: nan gradient found. The current loss is:  0.5304385423660278\n",
      "Warning: nan gradient found. The current loss is:  0.0073888711631298065\n",
      "Warning: nan gradient found. The current loss is:  0.4283536970615387\n",
      "Warning: nan gradient found. The current loss is:  0.6260954141616821\n",
      "Warning: nan gradient found. The current loss is:  0.47571900486946106\n",
      "Warning: nan gradient found. The current loss is:  0.7160015106201172\n",
      "Warning: nan gradient found. The current loss is:  0.6690258383750916\n",
      "Warning: nan gradient found. The current loss is:  0.30200040340423584\n",
      "Warning: nan gradient found. The current loss is:  0.4217965006828308\n",
      "Warning: nan gradient found. The current loss is:  0.15557393431663513\n",
      "Warning: nan gradient found. The current loss is:  0.18644098937511444\n",
      "Warning: nan gradient found. The current loss is:  0.1924009621143341\n",
      "Warning: nan gradient found. The current loss is:  0.18360714614391327\n",
      "Warning: nan gradient found. The current loss is:  0.4748760461807251\n",
      "Warning: nan gradient found. The current loss is:  0.39204761385917664\n",
      "Warning: nan gradient found. The current loss is:  0.045139603316783905\n",
      "Warning: nan gradient found. The current loss is:  0.3698011338710785\n",
      "Warning: nan gradient found. The current loss is:  0.3783077895641327\n",
      "Warning: nan gradient found. The current loss is:  0.28106817603111267\n",
      "Warning: nan gradient found. The current loss is:  0.45295852422714233\n",
      "Warning: nan gradient found. The current loss is:  0.7837950587272644\n",
      "Warning: nan gradient found. The current loss is:  0.2178572714328766\n",
      "Warning: nan gradient found. The current loss is:  0.32327911257743835\n",
      "Warning: nan gradient found. The current loss is:  0.2760201096534729\n",
      "Warning: nan gradient found. The current loss is:  0.5907810926437378\n",
      "Warning: nan gradient found. The current loss is:  0.2698252201080322\n",
      "Warning: nan gradient found. The current loss is:  0.7731741070747375\n",
      "Warning: nan gradient found. The current loss is:  0.9000135660171509\n",
      "Warning: nan gradient found. The current loss is:  0.2113579362630844\n",
      "Warning: nan gradient found. The current loss is:  0.3058772087097168\n",
      "Warning: nan gradient found. The current loss is:  0.23712539672851562\n",
      "Warning: nan gradient found. The current loss is:  0.07104659080505371\n",
      "Warning: nan gradient found. The current loss is:  0.38061660528182983\n",
      "Warning: nan gradient found. The current loss is:  0.5682331919670105\n",
      "Warning: nan gradient found. The current loss is:  0.40570318698883057\n",
      "Warning: nan gradient found. The current loss is:  0.29517218470573425\n",
      "Warning: nan gradient found. The current loss is:  0.48444557189941406\n",
      "Warning: nan gradient found. The current loss is:  0.28368455171585083\n",
      "Warning: nan gradient found. The current loss is:  0.6791319251060486\n",
      "Warning: nan gradient found. The current loss is:  -0.014237508177757263\n",
      "Warning: nan gradient found. The current loss is:  0.1302272379398346\n",
      "Warning: nan gradient found. The current loss is:  0.195817232131958\n",
      "Warning: nan gradient found. The current loss is:  0.45852169394493103\n",
      "Warning: nan gradient found. The current loss is:  0.2264854907989502\n",
      "Warning: nan gradient found. The current loss is:  0.38746213912963867\n",
      "Warning: nan gradient found. The current loss is:  0.32110899686813354\n",
      "Warning: nan gradient found. The current loss is:  0.18776066601276398\n",
      "Warning: nan gradient found. The current loss is:  1.4777815341949463\n",
      "Warning: nan gradient found. The current loss is:  0.10961389541625977\n",
      "Warning: nan gradient found. The current loss is:  1.084778904914856\n",
      "Warning: nan gradient found. The current loss is:  0.6663255095481873\n",
      "Warning: nan gradient found. The current loss is:  0.20414139330387115\n",
      "Warning: nan gradient found. The current loss is:  0.46086519956588745\n",
      "Warning: nan gradient found. The current loss is:  0.9357173442840576\n",
      "Warning: nan gradient found. The current loss is:  0.5654647350311279\n",
      "Warning: nan gradient found. The current loss is:  0.15364038944244385\n",
      "Warning: nan gradient found. The current loss is:  0.055986713618040085\n",
      "Warning: nan gradient found. The current loss is:  0.5599608421325684\n",
      "Warning: nan gradient found. The current loss is:  0.1365058571100235\n",
      "Warning: nan gradient found. The current loss is:  1.097569227218628\n",
      "Warning: nan gradient found. The current loss is:  0.32905542850494385\n",
      "Warning: nan gradient found. The current loss is:  0.16502507030963898\n",
      "Warning: nan gradient found. The current loss is:  0.8417129516601562\n",
      "Warning: nan gradient found. The current loss is:  1.1014189720153809\n",
      "Warning: nan gradient found. The current loss is:  0.5508328676223755\n",
      "Warning: nan gradient found. The current loss is:  0.42224442958831787\n",
      "Warning: nan gradient found. The current loss is:  0.6273491382598877\n",
      "Warning: nan gradient found. The current loss is:  0.45090553164482117\n",
      "Warning: nan gradient found. The current loss is:  0.5460671186447144\n",
      "Warning: nan gradient found. The current loss is:  0.27152219414711\n",
      "Current batch training loss: 0.271522  [460800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4726356565952301\n",
      "Warning: nan gradient found. The current loss is:  0.6833670139312744\n",
      "Warning: nan gradient found. The current loss is:  0.7229975461959839\n",
      "Warning: nan gradient found. The current loss is:  0.330411434173584\n",
      "Warning: nan gradient found. The current loss is:  0.7279052734375\n",
      "Warning: nan gradient found. The current loss is:  0.6394442319869995\n",
      "Warning: nan gradient found. The current loss is:  0.8577991724014282\n",
      "Warning: nan gradient found. The current loss is:  0.43067413568496704\n",
      "Warning: nan gradient found. The current loss is:  0.4995059370994568\n",
      "Warning: nan gradient found. The current loss is:  0.29161936044692993\n",
      "Warning: nan gradient found. The current loss is:  0.5370899438858032\n",
      "Warning: nan gradient found. The current loss is:  0.5016651749610901\n",
      "Warning: nan gradient found. The current loss is:  0.5835471153259277\n",
      "Warning: nan gradient found. The current loss is:  0.2732934355735779\n",
      "Warning: nan gradient found. The current loss is:  0.37401092052459717\n",
      "Warning: nan gradient found. The current loss is:  0.275162935256958\n",
      "Warning: nan gradient found. The current loss is:  0.46317219734191895\n",
      "Warning: nan gradient found. The current loss is:  0.32485145330429077\n",
      "Warning: nan gradient found. The current loss is:  0.5816205739974976\n",
      "Warning: nan gradient found. The current loss is:  1.1652201414108276\n",
      "Warning: nan gradient found. The current loss is:  1.153765082359314\n",
      "Warning: nan gradient found. The current loss is:  0.4858859181404114\n",
      "Warning: nan gradient found. The current loss is:  0.860605001449585\n",
      "Warning: nan gradient found. The current loss is:  0.8236332535743713\n",
      "Warning: nan gradient found. The current loss is:  0.7106750011444092\n",
      "Warning: nan gradient found. The current loss is:  0.5496329069137573\n",
      "Warning: nan gradient found. The current loss is:  0.34133046865463257\n",
      "Warning: nan gradient found. The current loss is:  0.6050983667373657\n",
      "Warning: nan gradient found. The current loss is:  0.4024432599544525\n",
      "Warning: nan gradient found. The current loss is:  0.42610281705856323\n",
      "Warning: nan gradient found. The current loss is:  0.547446072101593\n",
      "Warning: nan gradient found. The current loss is:  0.4570677876472473\n",
      "Warning: nan gradient found. The current loss is:  0.8848310708999634\n",
      "Warning: nan gradient found. The current loss is:  1.3052761554718018\n",
      "Warning: nan gradient found. The current loss is:  0.26418015360832214\n",
      "Warning: nan gradient found. The current loss is:  0.426876425743103\n",
      "Warning: nan gradient found. The current loss is:  1.10395085811615\n",
      "Warning: nan gradient found. The current loss is:  0.6180879473686218\n",
      "Warning: nan gradient found. The current loss is:  0.19242891669273376\n",
      "Warning: nan gradient found. The current loss is:  0.7388128638267517\n",
      "Warning: nan gradient found. The current loss is:  0.42365559935569763\n",
      "Warning: nan gradient found. The current loss is:  0.39518916606903076\n",
      "Warning: nan gradient found. The current loss is:  3.937908887863159\n",
      "Warning: nan gradient found. The current loss is:  0.2596140503883362\n",
      "Warning: nan gradient found. The current loss is:  0.6895362734794617\n",
      "Warning: nan gradient found. The current loss is:  0.5044922232627869\n",
      "Warning: nan gradient found. The current loss is:  0.11478456854820251\n",
      "Warning: nan gradient found. The current loss is:  0.13510604202747345\n",
      "Warning: nan gradient found. The current loss is:  0.20241603255271912\n",
      "Warning: nan gradient found. The current loss is:  0.4828794002532959\n",
      "Warning: nan gradient found. The current loss is:  0.02719661220908165\n",
      "Warning: nan gradient found. The current loss is:  0.25495678186416626\n",
      "Warning: nan gradient found. The current loss is:  0.9095613956451416\n",
      "Warning: nan gradient found. The current loss is:  0.33536475896835327\n",
      "Warning: nan gradient found. The current loss is:  0.45967161655426025\n",
      "Warning: nan gradient found. The current loss is:  0.44528141617774963\n",
      "Warning: nan gradient found. The current loss is:  0.8082714080810547\n",
      "Warning: nan gradient found. The current loss is:  0.5893370509147644\n",
      "Warning: nan gradient found. The current loss is:  0.7131268978118896\n",
      "Warning: nan gradient found. The current loss is:  0.9984992742538452\n",
      "Warning: nan gradient found. The current loss is:  2.2446420192718506\n",
      "Warning: nan gradient found. The current loss is:  0.7068222761154175\n",
      "Warning: nan gradient found. The current loss is:  0.3342563509941101\n",
      "Warning: nan gradient found. The current loss is:  0.5451342463493347\n",
      "Warning: nan gradient found. The current loss is:  0.18950532376766205\n",
      "Warning: nan gradient found. The current loss is:  0.8472944498062134\n",
      "Warning: nan gradient found. The current loss is:  0.8409746289253235\n",
      "Warning: nan gradient found. The current loss is:  0.8064467906951904\n",
      "Warning: nan gradient found. The current loss is:  0.7098134756088257\n",
      "Warning: nan gradient found. The current loss is:  0.39589327573776245\n",
      "Warning: nan gradient found. The current loss is:  0.5323307514190674\n",
      "Warning: nan gradient found. The current loss is:  0.3865005373954773\n",
      "Warning: nan gradient found. The current loss is:  0.4895273447036743\n",
      "Warning: nan gradient found. The current loss is:  0.7028478384017944\n",
      "Warning: nan gradient found. The current loss is:  0.6433100700378418\n",
      "Warning: nan gradient found. The current loss is:  0.24130141735076904\n",
      "Warning: nan gradient found. The current loss is:  2.156514883041382\n",
      "Warning: nan gradient found. The current loss is:  0.3612319231033325\n",
      "Warning: nan gradient found. The current loss is:  0.1458088755607605\n",
      "Warning: nan gradient found. The current loss is:  0.3025910258293152\n",
      "Warning: nan gradient found. The current loss is:  0.5094468593597412\n",
      "Warning: nan gradient found. The current loss is:  0.6584113240242004\n",
      "Warning: nan gradient found. The current loss is:  0.7881134748458862\n",
      "Warning: nan gradient found. The current loss is:  0.546570360660553\n",
      "Warning: nan gradient found. The current loss is:  0.41930729150772095\n",
      "Warning: nan gradient found. The current loss is:  0.5930482745170593\n",
      "Warning: nan gradient found. The current loss is:  0.6942989826202393\n",
      "Warning: nan gradient found. The current loss is:  0.0730227380990982\n",
      "Warning: nan gradient found. The current loss is:  0.8304898738861084\n",
      "Warning: nan gradient found. The current loss is:  0.6985851526260376\n",
      "Warning: nan gradient found. The current loss is:  -0.06877052038908005\n",
      "Warning: nan gradient found. The current loss is:  0.9534721374511719\n",
      "Warning: nan gradient found. The current loss is:  0.5791610479354858\n",
      "Warning: nan gradient found. The current loss is:  0.45337584614753723\n",
      "Warning: nan gradient found. The current loss is:  0.40199965238571167\n",
      "Warning: nan gradient found. The current loss is:  0.1670856475830078\n",
      "Warning: nan gradient found. The current loss is:  0.8207632303237915\n",
      "Warning: nan gradient found. The current loss is:  1.6081594228744507\n",
      "Warning: nan gradient found. The current loss is:  0.3320651948451996\n",
      "Warning: nan gradient found. The current loss is:  0.6141826510429382\n",
      "Current batch training loss: 0.614183  [486400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6077925562858582\n",
      "Warning: nan gradient found. The current loss is:  0.8426446914672852\n",
      "Warning: nan gradient found. The current loss is:  0.3648618757724762\n",
      "Warning: nan gradient found. The current loss is:  0.5071032643318176\n",
      "Warning: nan gradient found. The current loss is:  0.7997674942016602\n",
      "Warning: nan gradient found. The current loss is:  0.6133022308349609\n",
      "Warning: nan gradient found. The current loss is:  1.1111235618591309\n",
      "Warning: nan gradient found. The current loss is:  0.22095438838005066\n",
      "Warning: nan gradient found. The current loss is:  1.183213472366333\n",
      "Warning: nan gradient found. The current loss is:  0.36020445823669434\n",
      "Warning: nan gradient found. The current loss is:  0.9152888655662537\n",
      "Warning: nan gradient found. The current loss is:  0.44734764099121094\n",
      "Warning: nan gradient found. The current loss is:  0.16878682374954224\n",
      "Warning: nan gradient found. The current loss is:  0.7096222639083862\n",
      "Warning: nan gradient found. The current loss is:  0.17902983725070953\n",
      "Warning: nan gradient found. The current loss is:  0.4470691382884979\n",
      "Warning: nan gradient found. The current loss is:  0.2172260284423828\n",
      "Warning: nan gradient found. The current loss is:  0.71701580286026\n",
      "Warning: nan gradient found. The current loss is:  0.46358197927474976\n",
      "Warning: nan gradient found. The current loss is:  0.670844554901123\n",
      "Warning: nan gradient found. The current loss is:  0.26873648166656494\n",
      "Warning: nan gradient found. The current loss is:  0.47205695509910583\n",
      "Warning: nan gradient found. The current loss is:  1.5667970180511475\n",
      "Warning: nan gradient found. The current loss is:  1.3149617910385132\n",
      "Warning: nan gradient found. The current loss is:  0.2535091042518616\n",
      "Warning: nan gradient found. The current loss is:  0.4696205258369446\n",
      "Warning: nan gradient found. The current loss is:  0.18116025626659393\n",
      "Warning: nan gradient found. The current loss is:  0.537567138671875\n",
      "Warning: nan gradient found. The current loss is:  0.5109404921531677\n",
      "Warning: nan gradient found. The current loss is:  0.1800420880317688\n",
      "Warning: nan gradient found. The current loss is:  0.5184371471405029\n",
      "Warning: nan gradient found. The current loss is:  0.8426243662834167\n",
      "Warning: nan gradient found. The current loss is:  0.0756663978099823\n",
      "Warning: nan gradient found. The current loss is:  0.8064870834350586\n",
      "Warning: nan gradient found. The current loss is:  0.20091335475444794\n",
      "Warning: nan gradient found. The current loss is:  0.9093219041824341\n",
      "Warning: nan gradient found. The current loss is:  0.37110552191734314\n",
      "Warning: nan gradient found. The current loss is:  0.5693808794021606\n",
      "Warning: nan gradient found. The current loss is:  0.8712109327316284\n",
      "Warning: nan gradient found. The current loss is:  0.3420409560203552\n",
      "Warning: nan gradient found. The current loss is:  0.23881253600120544\n",
      "Warning: nan gradient found. The current loss is:  0.8261966705322266\n",
      "Warning: nan gradient found. The current loss is:  0.43413275480270386\n",
      "Warning: nan gradient found. The current loss is:  0.5452775955200195\n",
      "Warning: nan gradient found. The current loss is:  0.21980085968971252\n",
      "Warning: nan gradient found. The current loss is:  0.37180864810943604\n",
      "Warning: nan gradient found. The current loss is:  0.6782048940658569\n",
      "Warning: nan gradient found. The current loss is:  0.9433825016021729\n",
      "Warning: nan gradient found. The current loss is:  0.8708696365356445\n",
      "Warning: nan gradient found. The current loss is:  -0.06447401642799377\n",
      "Warning: nan gradient found. The current loss is:  0.10230042785406113\n",
      "Warning: nan gradient found. The current loss is:  0.6147854924201965\n",
      "Warning: nan gradient found. The current loss is:  0.608443021774292\n",
      "Warning: nan gradient found. The current loss is:  0.5513665676116943\n",
      "Warning: nan gradient found. The current loss is:  -0.017076339572668076\n",
      "Warning: nan gradient found. The current loss is:  0.5696947574615479\n",
      "Warning: nan gradient found. The current loss is:  0.46575430035591125\n",
      "Warning: nan gradient found. The current loss is:  0.5750206708908081\n",
      "Warning: nan gradient found. The current loss is:  0.593706488609314\n",
      "Warning: nan gradient found. The current loss is:  0.32577502727508545\n",
      "Warning: nan gradient found. The current loss is:  1.0792092084884644\n",
      "Warning: nan gradient found. The current loss is:  1.0871331691741943\n",
      "Warning: nan gradient found. The current loss is:  0.5165348649024963\n",
      "Warning: nan gradient found. The current loss is:  0.8041646480560303\n",
      "Warning: nan gradient found. The current loss is:  0.5930648446083069\n",
      "Warning: nan gradient found. The current loss is:  0.6324949264526367\n",
      "Warning: nan gradient found. The current loss is:  0.821358859539032\n",
      "Warning: nan gradient found. The current loss is:  0.8282101154327393\n",
      "Warning: nan gradient found. The current loss is:  0.3280111253261566\n",
      "Warning: nan gradient found. The current loss is:  1.3647524118423462\n",
      "Warning: nan gradient found. The current loss is:  0.2662564814090729\n",
      "Warning: nan gradient found. The current loss is:  0.2618885636329651\n",
      "Warning: nan gradient found. The current loss is:  0.3704240918159485\n",
      "Warning: nan gradient found. The current loss is:  0.45782673358917236\n",
      "Warning: nan gradient found. The current loss is:  0.2844346761703491\n",
      "Warning: nan gradient found. The current loss is:  0.501946747303009\n",
      "Warning: nan gradient found. The current loss is:  1.0204031467437744\n",
      "Warning: nan gradient found. The current loss is:  1.0308518409729004\n",
      "Warning: nan gradient found. The current loss is:  1.2514152526855469\n",
      "Warning: nan gradient found. The current loss is:  0.1755755990743637\n",
      "Warning: nan gradient found. The current loss is:  0.22399714589118958\n",
      "Warning: nan gradient found. The current loss is:  0.7276797294616699\n",
      "Warning: nan gradient found. The current loss is:  0.34866955876350403\n",
      "Warning: nan gradient found. The current loss is:  0.1351180076599121\n",
      "Warning: nan gradient found. The current loss is:  0.32980430126190186\n",
      "Warning: nan gradient found. The current loss is:  0.2992439866065979\n",
      "Warning: nan gradient found. The current loss is:  0.8005458116531372\n",
      "Warning: nan gradient found. The current loss is:  0.015859074890613556\n",
      "Warning: nan gradient found. The current loss is:  0.13769973814487457\n",
      "Warning: nan gradient found. The current loss is:  0.38329753279685974\n",
      "Warning: nan gradient found. The current loss is:  0.37495237588882446\n",
      "Warning: nan gradient found. The current loss is:  0.6768825650215149\n",
      "Warning: nan gradient found. The current loss is:  0.612463653087616\n",
      "Warning: nan gradient found. The current loss is:  0.586794376373291\n",
      "Warning: nan gradient found. The current loss is:  0.7181978821754456\n",
      "Warning: nan gradient found. The current loss is:  0.9350640773773193\n",
      "Warning: nan gradient found. The current loss is:  0.7034006118774414\n",
      "Warning: nan gradient found. The current loss is:  0.47504594922065735\n",
      "Warning: nan gradient found. The current loss is:  0.22225485742092133\n",
      "Warning: nan gradient found. The current loss is:  0.7444784641265869\n",
      "Current batch training loss: 0.744478  [512000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.3163783550262451\n",
      "Warning: nan gradient found. The current loss is:  0.45670285820961\n",
      "Warning: nan gradient found. The current loss is:  0.8637926578521729\n",
      "Warning: nan gradient found. The current loss is:  0.12883257865905762\n",
      "Warning: nan gradient found. The current loss is:  0.31958281993865967\n",
      "Warning: nan gradient found. The current loss is:  0.6668362617492676\n",
      "Warning: nan gradient found. The current loss is:  0.6557824611663818\n",
      "Warning: nan gradient found. The current loss is:  0.7084195017814636\n",
      "Warning: nan gradient found. The current loss is:  0.45639950037002563\n",
      "Warning: nan gradient found. The current loss is:  0.8516597151756287\n",
      "Warning: nan gradient found. The current loss is:  0.8813507556915283\n",
      "Warning: nan gradient found. The current loss is:  0.4799245595932007\n",
      "Warning: nan gradient found. The current loss is:  0.12668468058109283\n",
      "Warning: nan gradient found. The current loss is:  0.7841765880584717\n",
      "Warning: nan gradient found. The current loss is:  0.3367242217063904\n",
      "Warning: nan gradient found. The current loss is:  0.8620889186859131\n",
      "Warning: nan gradient found. The current loss is:  0.6088852286338806\n",
      "Warning: nan gradient found. The current loss is:  0.4787764549255371\n",
      "Warning: nan gradient found. The current loss is:  0.4324273467063904\n",
      "Warning: nan gradient found. The current loss is:  0.8004836440086365\n",
      "Warning: nan gradient found. The current loss is:  1.5957852602005005\n",
      "Warning: nan gradient found. The current loss is:  0.5011369585990906\n",
      "Warning: nan gradient found. The current loss is:  0.02319984883069992\n",
      "Warning: nan gradient found. The current loss is:  0.23648062348365784\n",
      "Warning: nan gradient found. The current loss is:  0.513841450214386\n",
      "Warning: nan gradient found. The current loss is:  0.557532787322998\n",
      "Warning: nan gradient found. The current loss is:  0.15671873092651367\n",
      "Warning: nan gradient found. The current loss is:  1.1951574087142944\n",
      "Warning: nan gradient found. The current loss is:  1.0453258752822876\n",
      "Warning: nan gradient found. The current loss is:  0.6028138399124146\n",
      "Warning: nan gradient found. The current loss is:  0.8683629631996155\n",
      "Warning: nan gradient found. The current loss is:  0.7019816040992737\n",
      "Warning: nan gradient found. The current loss is:  0.12305246293544769\n",
      "Warning: nan gradient found. The current loss is:  0.11552664637565613\n",
      "Warning: nan gradient found. The current loss is:  0.5882512331008911\n",
      "Warning: nan gradient found. The current loss is:  0.5761950612068176\n",
      "Warning: nan gradient found. The current loss is:  0.8048868179321289\n",
      "Warning: nan gradient found. The current loss is:  0.5823042392730713\n",
      "Warning: nan gradient found. The current loss is:  0.4748933017253876\n",
      "Warning: nan gradient found. The current loss is:  0.40884971618652344\n",
      "Warning: nan gradient found. The current loss is:  0.4845936894416809\n",
      "Warning: nan gradient found. The current loss is:  1.1783547401428223\n",
      "Warning: nan gradient found. The current loss is:  1.0092813968658447\n",
      "Warning: nan gradient found. The current loss is:  0.42167991399765015\n",
      "Warning: nan gradient found. The current loss is:  0.46567100286483765\n",
      "Warning: nan gradient found. The current loss is:  1.2662053108215332\n",
      "Warning: nan gradient found. The current loss is:  0.25192776322364807\n",
      "Warning: nan gradient found. The current loss is:  0.6796526312828064\n",
      "Warning: nan gradient found. The current loss is:  0.7718625664710999\n",
      "Warning: nan gradient found. The current loss is:  0.8946985006332397\n",
      "Warning: nan gradient found. The current loss is:  0.21212920546531677\n",
      "Warning: nan gradient found. The current loss is:  0.7982922196388245\n",
      "Warning: nan gradient found. The current loss is:  0.5334268808364868\n",
      "Warning: nan gradient found. The current loss is:  0.37187549471855164\n",
      "Warning: nan gradient found. The current loss is:  0.6504307985305786\n",
      "Warning: nan gradient found. The current loss is:  0.11271768063306808\n",
      "Warning: nan gradient found. The current loss is:  0.1466926485300064\n",
      "Warning: nan gradient found. The current loss is:  0.05762918293476105\n",
      "Warning: nan gradient found. The current loss is:  0.43425053358078003\n",
      "Warning: nan gradient found. The current loss is:  0.9772698879241943\n",
      "Warning: nan gradient found. The current loss is:  0.40513771772384644\n",
      "Warning: nan gradient found. The current loss is:  0.4980393648147583\n",
      "Warning: nan gradient found. The current loss is:  0.8439735770225525\n",
      "Warning: nan gradient found. The current loss is:  0.23800542950630188\n",
      "Warning: nan gradient found. The current loss is:  0.6632949113845825\n",
      "Warning: nan gradient found. The current loss is:  1.0001702308654785\n",
      "Warning: nan gradient found. The current loss is:  0.21588578820228577\n",
      "Warning: nan gradient found. The current loss is:  0.6002217531204224\n",
      "Warning: nan gradient found. The current loss is:  0.37789982557296753\n",
      "Warning: nan gradient found. The current loss is:  0.6638497114181519\n",
      "Warning: nan gradient found. The current loss is:  0.19522279500961304\n",
      "Warning: nan gradient found. The current loss is:  0.5784602761268616\n",
      "Warning: nan gradient found. The current loss is:  1.2472538948059082\n",
      "Warning: nan gradient found. The current loss is:  0.2651670575141907\n",
      "Warning: nan gradient found. The current loss is:  1.344517707824707\n",
      "Warning: nan gradient found. The current loss is:  0.4891267418861389\n",
      "Warning: nan gradient found. The current loss is:  0.454483300447464\n",
      "Warning: nan gradient found. The current loss is:  0.2007446438074112\n",
      "Warning: nan gradient found. The current loss is:  0.36242902278900146\n",
      "Warning: nan gradient found. The current loss is:  0.708543062210083\n",
      "Warning: nan gradient found. The current loss is:  0.3575548231601715\n",
      "Warning: nan gradient found. The current loss is:  0.22037237882614136\n",
      "Warning: nan gradient found. The current loss is:  0.31176748871803284\n",
      "Warning: nan gradient found. The current loss is:  0.18685346841812134\n",
      "Warning: nan gradient found. The current loss is:  1.5019030570983887\n",
      "Warning: nan gradient found. The current loss is:  0.9149514436721802\n",
      "Warning: nan gradient found. The current loss is:  0.3565501272678375\n",
      "Warning: nan gradient found. The current loss is:  0.6413103938102722\n",
      "Warning: nan gradient found. The current loss is:  0.983517050743103\n",
      "Warning: nan gradient found. The current loss is:  0.3692088723182678\n",
      "Warning: nan gradient found. The current loss is:  0.4035537540912628\n",
      "Warning: nan gradient found. The current loss is:  0.6037395596504211\n",
      "Warning: nan gradient found. The current loss is:  2.760643243789673\n",
      "Warning: nan gradient found. The current loss is:  0.5367103815078735\n",
      "Warning: nan gradient found. The current loss is:  0.24724040925502777\n",
      "Warning: nan gradient found. The current loss is:  1.333707571029663\n",
      "Warning: nan gradient found. The current loss is:  0.8810030221939087\n",
      "Warning: nan gradient found. The current loss is:  0.5079920887947083\n",
      "Warning: nan gradient found. The current loss is:  1.4100289344787598\n",
      "Warning: nan gradient found. The current loss is:  0.8044630885124207\n",
      "Current batch training loss: 0.804463  [537600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.12299168109893799\n",
      "Warning: nan gradient found. The current loss is:  1.4053034782409668\n",
      "Warning: nan gradient found. The current loss is:  1.0337426662445068\n",
      "Warning: nan gradient found. The current loss is:  0.502537727355957\n",
      "Warning: nan gradient found. The current loss is:  0.06960582733154297\n",
      "Warning: nan gradient found. The current loss is:  0.6021639108657837\n",
      "Warning: nan gradient found. The current loss is:  0.22497254610061646\n",
      "Warning: nan gradient found. The current loss is:  0.35433655977249146\n",
      "Warning: nan gradient found. The current loss is:  0.5151440501213074\n",
      "Warning: nan gradient found. The current loss is:  0.339591383934021\n",
      "Warning: nan gradient found. The current loss is:  -0.0008581057190895081\n",
      "Warning: nan gradient found. The current loss is:  0.7254396677017212\n",
      "Warning: nan gradient found. The current loss is:  0.8637455701828003\n",
      "Warning: nan gradient found. The current loss is:  0.35405123233795166\n",
      "Warning: nan gradient found. The current loss is:  0.04722348973155022\n",
      "Warning: nan gradient found. The current loss is:  0.9139692187309265\n",
      "Warning: nan gradient found. The current loss is:  0.7163732051849365\n",
      "Warning: nan gradient found. The current loss is:  0.6329428553581238\n",
      "Warning: nan gradient found. The current loss is:  0.4607250690460205\n",
      "Warning: nan gradient found. The current loss is:  0.7419576644897461\n",
      "Warning: nan gradient found. The current loss is:  0.4480658769607544\n",
      "Warning: nan gradient found. The current loss is:  0.3646336793899536\n",
      "Warning: nan gradient found. The current loss is:  0.4923308789730072\n",
      "Warning: nan gradient found. The current loss is:  0.1056058332324028\n",
      "Warning: nan gradient found. The current loss is:  0.2527563273906708\n",
      "Warning: nan gradient found. The current loss is:  1.0352065563201904\n",
      "Warning: nan gradient found. The current loss is:  0.8047009706497192\n",
      "Warning: nan gradient found. The current loss is:  -0.0991569310426712\n",
      "Warning: nan gradient found. The current loss is:  0.6500868201255798\n",
      "Warning: nan gradient found. The current loss is:  0.230819433927536\n",
      "Warning: nan gradient found. The current loss is:  0.7175804972648621\n",
      "Warning: nan gradient found. The current loss is:  0.5889292359352112\n",
      "Warning: nan gradient found. The current loss is:  0.5287090539932251\n",
      "Warning: nan gradient found. The current loss is:  0.16640934348106384\n",
      "Warning: nan gradient found. The current loss is:  0.45242011547088623\n",
      "Warning: nan gradient found. The current loss is:  0.7886960506439209\n",
      "Warning: nan gradient found. The current loss is:  0.5350581407546997\n",
      "Warning: nan gradient found. The current loss is:  0.8196163177490234\n",
      "Warning: nan gradient found. The current loss is:  0.5184980630874634\n",
      "Warning: nan gradient found. The current loss is:  0.3895118832588196\n",
      "Warning: nan gradient found. The current loss is:  0.38329124450683594\n",
      "Warning: nan gradient found. The current loss is:  1.3581587076187134\n",
      "Warning: nan gradient found. The current loss is:  0.5352165699005127\n",
      "Warning: nan gradient found. The current loss is:  0.429221510887146\n",
      "Warning: nan gradient found. The current loss is:  0.3657292127609253\n",
      "Warning: nan gradient found. The current loss is:  0.3426443040370941\n",
      "Warning: nan gradient found. The current loss is:  0.2251632809638977\n",
      "Warning: nan gradient found. The current loss is:  0.46947360038757324\n",
      "Warning: nan gradient found. The current loss is:  0.0007566940039396286\n",
      "Warning: nan gradient found. The current loss is:  0.2953234016895294\n",
      "Warning: nan gradient found. The current loss is:  0.27540478110313416\n",
      "Warning: nan gradient found. The current loss is:  2.297900915145874\n",
      "Warning: nan gradient found. The current loss is:  0.015431072562932968\n",
      "Warning: nan gradient found. The current loss is:  0.8143532276153564\n",
      "Warning: nan gradient found. The current loss is:  0.5127080678939819\n",
      "Warning: nan gradient found. The current loss is:  0.307378351688385\n",
      "Warning: nan gradient found. The current loss is:  1.0892678499221802\n",
      "Warning: nan gradient found. The current loss is:  0.3821638822555542\n",
      "Warning: nan gradient found. The current loss is:  0.7059104442596436\n",
      "Warning: nan gradient found. The current loss is:  1.0579817295074463\n",
      "Warning: nan gradient found. The current loss is:  0.46768075227737427\n",
      "Warning: nan gradient found. The current loss is:  0.964742124080658\n",
      "Warning: nan gradient found. The current loss is:  0.17439521849155426\n",
      "Warning: nan gradient found. The current loss is:  0.34957003593444824\n",
      "Warning: nan gradient found. The current loss is:  0.34185874462127686\n",
      "Warning: nan gradient found. The current loss is:  0.6807798743247986\n",
      "Warning: nan gradient found. The current loss is:  0.8728123903274536\n",
      "Warning: nan gradient found. The current loss is:  0.32668986916542053\n",
      "Warning: nan gradient found. The current loss is:  0.1479073464870453\n",
      "Warning: nan gradient found. The current loss is:  0.8139787316322327\n",
      "Warning: nan gradient found. The current loss is:  0.3965674042701721\n",
      "Warning: nan gradient found. The current loss is:  0.6183092594146729\n",
      "Warning: nan gradient found. The current loss is:  0.5164049863815308\n",
      "Warning: nan gradient found. The current loss is:  0.41980740427970886\n",
      "Warning: nan gradient found. The current loss is:  0.38025975227355957\n",
      "Warning: nan gradient found. The current loss is:  0.23295824229717255\n",
      "Warning: nan gradient found. The current loss is:  0.5317026376724243\n",
      "Warning: nan gradient found. The current loss is:  0.3330511450767517\n",
      "Warning: nan gradient found. The current loss is:  0.8251088261604309\n",
      "Warning: nan gradient found. The current loss is:  0.7175451517105103\n",
      "Warning: nan gradient found. The current loss is:  0.7372533082962036\n",
      "Warning: nan gradient found. The current loss is:  0.453911155462265\n",
      "Warning: nan gradient found. The current loss is:  0.5545856952667236\n",
      "Warning: nan gradient found. The current loss is:  0.5285284519195557\n",
      "Warning: nan gradient found. The current loss is:  0.7579349279403687\n",
      "Warning: nan gradient found. The current loss is:  0.40845850110054016\n",
      "Warning: nan gradient found. The current loss is:  0.5512273907661438\n",
      "Warning: nan gradient found. The current loss is:  0.48580658435821533\n",
      "Warning: nan gradient found. The current loss is:  0.6512980461120605\n",
      "Warning: nan gradient found. The current loss is:  0.9567861557006836\n",
      "Warning: nan gradient found. The current loss is:  0.14283540844917297\n",
      "Warning: nan gradient found. The current loss is:  0.29839837551116943\n",
      "Warning: nan gradient found. The current loss is:  0.4725017249584198\n",
      "Warning: nan gradient found. The current loss is:  0.7790042161941528\n",
      "Warning: nan gradient found. The current loss is:  0.3610292971134186\n",
      "Warning: nan gradient found. The current loss is:  0.5414737462997437\n",
      "Warning: nan gradient found. The current loss is:  1.1729902029037476\n",
      "Warning: nan gradient found. The current loss is:  0.2484651803970337\n",
      "Warning: nan gradient found. The current loss is:  0.3767675757408142\n",
      "Warning: nan gradient found. The current loss is:  0.6041455268859863\n",
      "Current batch training loss: 0.604146  [563200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6886987686157227\n",
      "Warning: nan gradient found. The current loss is:  0.13930222392082214\n",
      "Warning: nan gradient found. The current loss is:  1.310038685798645\n",
      "Warning: nan gradient found. The current loss is:  0.5630561113357544\n",
      "Warning: nan gradient found. The current loss is:  0.8915042281150818\n",
      "Warning: nan gradient found. The current loss is:  0.33382144570350647\n",
      "Warning: nan gradient found. The current loss is:  0.6158319115638733\n",
      "Warning: nan gradient found. The current loss is:  0.7362250089645386\n",
      "Warning: nan gradient found. The current loss is:  0.4246922433376312\n",
      "Warning: nan gradient found. The current loss is:  2.3866159915924072\n",
      "Warning: nan gradient found. The current loss is:  0.42745816707611084\n",
      "Warning: nan gradient found. The current loss is:  1.069216251373291\n",
      "Warning: nan gradient found. The current loss is:  0.5187537670135498\n",
      "Warning: nan gradient found. The current loss is:  0.2451871931552887\n",
      "Warning: nan gradient found. The current loss is:  0.4259166419506073\n",
      "Warning: nan gradient found. The current loss is:  0.1581077128648758\n",
      "Warning: nan gradient found. The current loss is:  0.5510410070419312\n",
      "Warning: nan gradient found. The current loss is:  0.5209530591964722\n",
      "Warning: nan gradient found. The current loss is:  0.874487042427063\n",
      "Warning: nan gradient found. The current loss is:  0.5169937014579773\n",
      "Warning: nan gradient found. The current loss is:  0.30124610662460327\n",
      "Warning: nan gradient found. The current loss is:  0.7640079259872437\n",
      "Warning: nan gradient found. The current loss is:  0.4754496216773987\n",
      "Warning: nan gradient found. The current loss is:  0.5260170698165894\n",
      "Warning: nan gradient found. The current loss is:  0.4734894037246704\n",
      "Warning: nan gradient found. The current loss is:  0.2938414514064789\n",
      "Warning: nan gradient found. The current loss is:  0.4231226444244385\n",
      "Warning: nan gradient found. The current loss is:  0.7280752062797546\n",
      "Warning: nan gradient found. The current loss is:  0.07758473604917526\n",
      "Warning: nan gradient found. The current loss is:  0.6476554870605469\n",
      "Warning: nan gradient found. The current loss is:  0.11197532713413239\n",
      "Warning: nan gradient found. The current loss is:  0.5219841003417969\n",
      "Warning: nan gradient found. The current loss is:  1.4628959894180298\n",
      "Warning: nan gradient found. The current loss is:  1.4836939573287964\n",
      "Warning: nan gradient found. The current loss is:  0.4955657124519348\n",
      "Warning: nan gradient found. The current loss is:  0.2400234490633011\n",
      "Warning: nan gradient found. The current loss is:  0.7972477078437805\n",
      "Warning: nan gradient found. The current loss is:  0.3275134563446045\n",
      "Warning: nan gradient found. The current loss is:  0.29803258180618286\n",
      "Warning: nan gradient found. The current loss is:  0.5036460161209106\n",
      "Warning: nan gradient found. The current loss is:  0.20468361675739288\n",
      "Warning: nan gradient found. The current loss is:  0.34797871112823486\n",
      "Warning: nan gradient found. The current loss is:  0.42191123962402344\n",
      "Warning: nan gradient found. The current loss is:  0.5965105295181274\n",
      "Warning: nan gradient found. The current loss is:  0.9452056884765625\n",
      "Warning: nan gradient found. The current loss is:  1.341876745223999\n",
      "Warning: nan gradient found. The current loss is:  1.2065503597259521\n",
      "Warning: nan gradient found. The current loss is:  0.054282523691654205\n",
      "Warning: nan gradient found. The current loss is:  0.5705598592758179\n",
      "Warning: nan gradient found. The current loss is:  0.22785454988479614\n",
      "Warning: nan gradient found. The current loss is:  0.693242073059082\n",
      "Warning: nan gradient found. The current loss is:  0.4033486843109131\n",
      "Warning: nan gradient found. The current loss is:  1.0500085353851318\n",
      "Warning: nan gradient found. The current loss is:  0.18695484101772308\n",
      "Warning: nan gradient found. The current loss is:  0.3425457775592804\n",
      "Warning: nan gradient found. The current loss is:  0.6311162710189819\n",
      "Warning: nan gradient found. The current loss is:  0.5123611688613892\n",
      "Warning: nan gradient found. The current loss is:  1.3103106021881104\n",
      "Warning: nan gradient found. The current loss is:  0.6336749792098999\n",
      "Warning: nan gradient found. The current loss is:  0.4749511182308197\n",
      "Warning: nan gradient found. The current loss is:  0.7699181437492371\n",
      "Warning: nan gradient found. The current loss is:  0.06264768540859222\n",
      "Warning: nan gradient found. The current loss is:  0.9431514739990234\n",
      "Warning: nan gradient found. The current loss is:  0.6751815676689148\n",
      "Warning: nan gradient found. The current loss is:  0.3387622535228729\n",
      "Warning: nan gradient found. The current loss is:  0.3067023754119873\n",
      "Warning: nan gradient found. The current loss is:  0.41595005989074707\n",
      "Warning: nan gradient found. The current loss is:  0.43790334463119507\n",
      "Warning: nan gradient found. The current loss is:  0.6672301888465881\n",
      "Warning: nan gradient found. The current loss is:  0.3937043249607086\n",
      "Warning: nan gradient found. The current loss is:  0.21703627705574036\n",
      "Warning: nan gradient found. The current loss is:  0.8318117260932922\n",
      "Warning: nan gradient found. The current loss is:  0.6841264963150024\n",
      "Warning: nan gradient found. The current loss is:  1.7921454906463623\n",
      "Warning: nan gradient found. The current loss is:  0.28612953424453735\n",
      "Warning: nan gradient found. The current loss is:  0.9124568700790405\n",
      "Warning: nan gradient found. The current loss is:  0.26131200790405273\n",
      "Warning: nan gradient found. The current loss is:  1.5126466751098633\n",
      "Warning: nan gradient found. The current loss is:  0.6132975220680237\n",
      "Warning: nan gradient found. The current loss is:  0.6740759611129761\n",
      "Warning: nan gradient found. The current loss is:  1.0436081886291504\n",
      "Warning: nan gradient found. The current loss is:  0.5997731685638428\n",
      "Warning: nan gradient found. The current loss is:  1.2219159603118896\n",
      "Warning: nan gradient found. The current loss is:  0.7972548007965088\n",
      "Warning: nan gradient found. The current loss is:  0.289116770029068\n",
      "Warning: nan gradient found. The current loss is:  0.38233625888824463\n",
      "Warning: nan gradient found. The current loss is:  1.5382225513458252\n",
      "Warning: nan gradient found. The current loss is:  0.5123955011367798\n",
      "Warning: nan gradient found. The current loss is:  0.6630228161811829\n",
      "Warning: nan gradient found. The current loss is:  0.3067922592163086\n",
      "Warning: nan gradient found. The current loss is:  0.2924977242946625\n",
      "Warning: nan gradient found. The current loss is:  0.3422123193740845\n",
      "Warning: nan gradient found. The current loss is:  0.366463303565979\n",
      "Warning: nan gradient found. The current loss is:  0.886775016784668\n",
      "Warning: nan gradient found. The current loss is:  0.39314353466033936\n",
      "Warning: nan gradient found. The current loss is:  0.3684772551059723\n",
      "Warning: nan gradient found. The current loss is:  1.1067256927490234\n",
      "Warning: nan gradient found. The current loss is:  0.4351692795753479\n",
      "Warning: nan gradient found. The current loss is:  0.3174024224281311\n",
      "Warning: nan gradient found. The current loss is:  1.8683732748031616\n",
      "Current batch training loss: 1.868373  [588800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6405373215675354\n",
      "Warning: nan gradient found. The current loss is:  0.8999462723731995\n",
      "Warning: nan gradient found. The current loss is:  0.4591647982597351\n",
      "Warning: nan gradient found. The current loss is:  0.16278040409088135\n",
      "Warning: nan gradient found. The current loss is:  0.030678868293762207\n",
      "Warning: nan gradient found. The current loss is:  0.42647668719291687\n",
      "Warning: nan gradient found. The current loss is:  0.7587987780570984\n",
      "Warning: nan gradient found. The current loss is:  0.52791428565979\n",
      "Warning: nan gradient found. The current loss is:  0.31183964014053345\n",
      "Warning: nan gradient found. The current loss is:  0.3153795897960663\n",
      "Warning: nan gradient found. The current loss is:  0.4401949644088745\n",
      "Warning: nan gradient found. The current loss is:  0.3674941062927246\n",
      "Warning: nan gradient found. The current loss is:  0.19333431124687195\n",
      "Warning: nan gradient found. The current loss is:  0.7321257591247559\n",
      "Warning: nan gradient found. The current loss is:  0.2849576771259308\n",
      "Warning: nan gradient found. The current loss is:  0.7721620798110962\n",
      "Warning: nan gradient found. The current loss is:  1.2796353101730347\n",
      "Warning: nan gradient found. The current loss is:  0.4184475541114807\n",
      "Warning: nan gradient found. The current loss is:  0.5195461511611938\n",
      "Warning: nan gradient found. The current loss is:  0.3761654794216156\n",
      "Warning: nan gradient found. The current loss is:  0.5642815828323364\n",
      "Warning: nan gradient found. The current loss is:  0.5885098576545715\n",
      "Warning: nan gradient found. The current loss is:  0.34194180369377136\n",
      "Warning: nan gradient found. The current loss is:  0.9866269826889038\n",
      "Warning: nan gradient found. The current loss is:  0.6456039547920227\n",
      "Warning: nan gradient found. The current loss is:  0.6520857810974121\n",
      "Warning: nan gradient found. The current loss is:  0.5242331027984619\n",
      "Warning: nan gradient found. The current loss is:  0.9553539752960205\n",
      "Warning: nan gradient found. The current loss is:  0.855554461479187\n",
      "Warning: nan gradient found. The current loss is:  0.31567561626434326\n",
      "Warning: nan gradient found. The current loss is:  0.5320148468017578\n",
      "Warning: nan gradient found. The current loss is:  0.7728632092475891\n",
      "Warning: nan gradient found. The current loss is:  1.0557078123092651\n",
      "Warning: nan gradient found. The current loss is:  0.42942488193511963\n",
      "Warning: nan gradient found. The current loss is:  0.4640519618988037\n",
      "Warning: nan gradient found. The current loss is:  0.29247617721557617\n",
      "Warning: nan gradient found. The current loss is:  0.6608138084411621\n",
      "Warning: nan gradient found. The current loss is:  0.10500979423522949\n",
      "Warning: nan gradient found. The current loss is:  0.560276985168457\n",
      "Warning: nan gradient found. The current loss is:  0.3030964136123657\n",
      "Warning: nan gradient found. The current loss is:  0.2534903287887573\n",
      "Warning: nan gradient found. The current loss is:  1.0678112506866455\n",
      "Warning: nan gradient found. The current loss is:  0.25825169682502747\n",
      "Warning: nan gradient found. The current loss is:  0.5169206857681274\n",
      "Warning: nan gradient found. The current loss is:  0.18005883693695068\n",
      "Warning: nan gradient found. The current loss is:  0.21341192722320557\n",
      "Warning: nan gradient found. The current loss is:  0.4956710636615753\n",
      "Warning: nan gradient found. The current loss is:  0.6498762369155884\n",
      "Warning: nan gradient found. The current loss is:  0.20189015567302704\n",
      "Warning: nan gradient found. The current loss is:  0.9732224941253662\n",
      "Warning: nan gradient found. The current loss is:  0.2894040048122406\n",
      "Warning: nan gradient found. The current loss is:  0.3138211965560913\n",
      "Warning: nan gradient found. The current loss is:  1.496649980545044\n",
      "Warning: nan gradient found. The current loss is:  0.271157830953598\n",
      "Warning: nan gradient found. The current loss is:  0.6169735789299011\n",
      "Warning: nan gradient found. The current loss is:  0.680080771446228\n",
      "Warning: nan gradient found. The current loss is:  0.5740772485733032\n",
      "Warning: nan gradient found. The current loss is:  0.2558645009994507\n",
      "Warning: nan gradient found. The current loss is:  0.520581841468811\n",
      "Warning: nan gradient found. The current loss is:  0.7287225127220154\n",
      "Warning: nan gradient found. The current loss is:  0.8965399265289307\n",
      "Warning: nan gradient found. The current loss is:  0.7108256816864014\n",
      "Warning: nan gradient found. The current loss is:  0.13727492094039917\n",
      "Warning: nan gradient found. The current loss is:  0.9578906297683716\n",
      "Warning: nan gradient found. The current loss is:  1.0796029567718506\n",
      "Warning: nan gradient found. The current loss is:  0.5307062268257141\n",
      "Warning: nan gradient found. The current loss is:  2.696732521057129\n",
      "Warning: nan gradient found. The current loss is:  0.7457515001296997\n",
      "Warning: nan gradient found. The current loss is:  0.6503040194511414\n",
      "Warning: nan gradient found. The current loss is:  2.8645894527435303\n",
      "Warning: nan gradient found. The current loss is:  0.4325636625289917\n",
      "Warning: nan gradient found. The current loss is:  0.668057918548584\n",
      "Warning: nan gradient found. The current loss is:  0.39698076248168945\n",
      "Warning: nan gradient found. The current loss is:  0.3880689740180969\n",
      "Warning: nan gradient found. The current loss is:  0.9655756950378418\n",
      "Warning: nan gradient found. The current loss is:  0.03378106281161308\n",
      "Warning: nan gradient found. The current loss is:  0.34479469060897827\n",
      "Warning: nan gradient found. The current loss is:  0.5780036449432373\n",
      "Warning: nan gradient found. The current loss is:  0.49497154355049133\n",
      "Warning: nan gradient found. The current loss is:  0.9398603439331055\n",
      "Warning: nan gradient found. The current loss is:  0.9369013905525208\n",
      "Warning: nan gradient found. The current loss is:  0.5463123321533203\n",
      "Warning: nan gradient found. The current loss is:  1.578370213508606\n",
      "Warning: nan gradient found. The current loss is:  0.8327959775924683\n",
      "Warning: nan gradient found. The current loss is:  0.7411914467811584\n",
      "Warning: nan gradient found. The current loss is:  0.49325132369995117\n",
      "Warning: nan gradient found. The current loss is:  0.13858658075332642\n",
      "Warning: nan gradient found. The current loss is:  0.7592393159866333\n",
      "Warning: nan gradient found. The current loss is:  0.7829400300979614\n",
      "Warning: nan gradient found. The current loss is:  3.072230815887451\n",
      "Warning: nan gradient found. The current loss is:  0.642284631729126\n",
      "Warning: nan gradient found. The current loss is:  0.14640411734580994\n",
      "Warning: nan gradient found. The current loss is:  0.059456534683704376\n",
      "Warning: nan gradient found. The current loss is:  0.7789254784584045\n",
      "Warning: nan gradient found. The current loss is:  0.5101487636566162\n",
      "Warning: nan gradient found. The current loss is:  0.6589802503585815\n",
      "Warning: nan gradient found. The current loss is:  0.3573035001754761\n",
      "Warning: nan gradient found. The current loss is:  1.622942328453064\n",
      "Warning: nan gradient found. The current loss is:  0.5864519476890564\n",
      "Warning: nan gradient found. The current loss is:  0.6298035383224487\n",
      "Current batch training loss: 0.629804  [614400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8007210493087769\n",
      "Warning: nan gradient found. The current loss is:  0.7888361215591431\n",
      "Warning: nan gradient found. The current loss is:  1.887099027633667\n",
      "Warning: nan gradient found. The current loss is:  0.5574080944061279\n",
      "Warning: nan gradient found. The current loss is:  0.6261789798736572\n",
      "Warning: nan gradient found. The current loss is:  0.3663429915904999\n",
      "Warning: nan gradient found. The current loss is:  0.03322245180606842\n",
      "Warning: nan gradient found. The current loss is:  0.4182514250278473\n",
      "Warning: nan gradient found. The current loss is:  0.387500137090683\n",
      "Warning: nan gradient found. The current loss is:  0.49660688638687134\n",
      "Warning: nan gradient found. The current loss is:  0.21717748045921326\n",
      "Warning: nan gradient found. The current loss is:  0.9099489450454712\n",
      "Warning: nan gradient found. The current loss is:  0.821113646030426\n",
      "Warning: nan gradient found. The current loss is:  0.29476284980773926\n",
      "Warning: nan gradient found. The current loss is:  0.5836501121520996\n",
      "Warning: nan gradient found. The current loss is:  0.7288675308227539\n",
      "Warning: nan gradient found. The current loss is:  0.8675686120986938\n",
      "Warning: nan gradient found. The current loss is:  1.3951493501663208\n",
      "Warning: nan gradient found. The current loss is:  0.4910462498664856\n",
      "Warning: nan gradient found. The current loss is:  0.33124011754989624\n",
      "Warning: nan gradient found. The current loss is:  0.4087546467781067\n",
      "Warning: nan gradient found. The current loss is:  0.43893909454345703\n",
      "Warning: nan gradient found. The current loss is:  2.051851272583008\n",
      "Warning: nan gradient found. The current loss is:  0.2284407764673233\n",
      "Warning: nan gradient found. The current loss is:  0.18907833099365234\n",
      "Warning: nan gradient found. The current loss is:  0.3033953309059143\n",
      "Warning: nan gradient found. The current loss is:  0.30768418312072754\n",
      "Warning: nan gradient found. The current loss is:  1.619941234588623\n",
      "Warning: nan gradient found. The current loss is:  0.5711501836776733\n",
      "Warning: nan gradient found. The current loss is:  0.10615114867687225\n",
      "Warning: nan gradient found. The current loss is:  0.5726103782653809\n",
      "Warning: nan gradient found. The current loss is:  0.6755613088607788\n",
      "Warning: nan gradient found. The current loss is:  0.3639759123325348\n",
      "Warning: nan gradient found. The current loss is:  1.318063497543335\n",
      "Warning: nan gradient found. The current loss is:  0.3179834485054016\n",
      "Warning: nan gradient found. The current loss is:  1.598703384399414\n",
      "Warning: nan gradient found. The current loss is:  0.47696903347969055\n",
      "Warning: nan gradient found. The current loss is:  2.001556158065796\n",
      "Warning: nan gradient found. The current loss is:  0.13231879472732544\n",
      "Warning: nan gradient found. The current loss is:  0.6633519530296326\n",
      "Warning: nan gradient found. The current loss is:  1.9648723602294922\n",
      "Warning: nan gradient found. The current loss is:  0.44606852531433105\n",
      "Warning: nan gradient found. The current loss is:  0.6194548010826111\n",
      "Warning: nan gradient found. The current loss is:  0.793968677520752\n",
      "Warning: nan gradient found. The current loss is:  0.6147785186767578\n",
      "Warning: nan gradient found. The current loss is:  0.8438632488250732\n",
      "Warning: nan gradient found. The current loss is:  0.060019221156835556\n",
      "Warning: nan gradient found. The current loss is:  0.6741051077842712\n",
      "Warning: nan gradient found. The current loss is:  0.919526219367981\n",
      "Warning: nan gradient found. The current loss is:  0.6120636463165283\n",
      "Warning: nan gradient found. The current loss is:  1.5648119449615479\n",
      "Warning: nan gradient found. The current loss is:  0.449506551027298\n",
      "Warning: nan gradient found. The current loss is:  0.6225181221961975\n",
      "Warning: nan gradient found. The current loss is:  0.5153565406799316\n",
      "Warning: nan gradient found. The current loss is:  0.9003078937530518\n",
      "Warning: nan gradient found. The current loss is:  0.4221261739730835\n",
      "Warning: nan gradient found. The current loss is:  0.3018062710762024\n",
      "Warning: nan gradient found. The current loss is:  0.4394180178642273\n",
      "Warning: nan gradient found. The current loss is:  0.39492636919021606\n",
      "Warning: nan gradient found. The current loss is:  0.19102297723293304\n",
      "Warning: nan gradient found. The current loss is:  0.1739518642425537\n",
      "Warning: nan gradient found. The current loss is:  0.6063946485519409\n",
      "Warning: nan gradient found. The current loss is:  0.5333219766616821\n",
      "Warning: nan gradient found. The current loss is:  0.6482694745063782\n",
      "Warning: nan gradient found. The current loss is:  0.15656553208827972\n",
      "Warning: nan gradient found. The current loss is:  0.5536137819290161\n",
      "Warning: nan gradient found. The current loss is:  0.35849541425704956\n",
      "Warning: nan gradient found. The current loss is:  0.6836689710617065\n",
      "Warning: nan gradient found. The current loss is:  0.24622496962547302\n",
      "Warning: nan gradient found. The current loss is:  0.10339425504207611\n",
      "Warning: nan gradient found. The current loss is:  0.4165803790092468\n",
      "Warning: nan gradient found. The current loss is:  0.5642440915107727\n",
      "Warning: nan gradient found. The current loss is:  0.3467933237552643\n",
      "Warning: nan gradient found. The current loss is:  0.16077935695648193\n",
      "Warning: nan gradient found. The current loss is:  0.54277503490448\n",
      "Warning: nan gradient found. The current loss is:  0.728724479675293\n",
      "Warning: nan gradient found. The current loss is:  0.3157595098018646\n",
      "Warning: nan gradient found. The current loss is:  0.43147656321525574\n",
      "Warning: nan gradient found. The current loss is:  0.9326342344284058\n",
      "Warning: nan gradient found. The current loss is:  1.2948740720748901\n",
      "Warning: nan gradient found. The current loss is:  0.6874456405639648\n",
      "Warning: nan gradient found. The current loss is:  0.2698388397693634\n",
      "Warning: nan gradient found. The current loss is:  0.6111938953399658\n",
      "Warning: nan gradient found. The current loss is:  0.046381570398807526\n",
      "Warning: nan gradient found. The current loss is:  1.02212655544281\n",
      "Warning: nan gradient found. The current loss is:  0.36222851276397705\n",
      "Warning: nan gradient found. The current loss is:  0.4490452706813812\n",
      "Warning: nan gradient found. The current loss is:  0.48312902450561523\n",
      "Warning: nan gradient found. The current loss is:  1.1367106437683105\n",
      "Warning: nan gradient found. The current loss is:  0.5339272022247314\n",
      "Warning: nan gradient found. The current loss is:  0.9976121187210083\n",
      "Warning: nan gradient found. The current loss is:  0.5233401656150818\n",
      "Warning: nan gradient found. The current loss is:  0.38666898012161255\n",
      "Warning: nan gradient found. The current loss is:  0.5082921385765076\n",
      "Warning: nan gradient found. The current loss is:  0.7382630109786987\n",
      "Warning: nan gradient found. The current loss is:  0.05185791105031967\n",
      "Warning: nan gradient found. The current loss is:  0.24960093200206757\n",
      "Warning: nan gradient found. The current loss is:  0.9600642323493958\n",
      "Warning: nan gradient found. The current loss is:  1.6774080991744995\n",
      "Warning: nan gradient found. The current loss is:  0.5591760873794556\n",
      "Current batch training loss: 0.559176  [640000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  2.434657573699951\n",
      "Warning: nan gradient found. The current loss is:  0.22891995310783386\n",
      "Warning: nan gradient found. The current loss is:  0.29771989583969116\n",
      "Warning: nan gradient found. The current loss is:  0.6995406746864319\n",
      "Warning: nan gradient found. The current loss is:  0.7391576170921326\n",
      "Warning: nan gradient found. The current loss is:  0.450133740901947\n",
      "Warning: nan gradient found. The current loss is:  1.7991886138916016\n",
      "Warning: nan gradient found. The current loss is:  0.6405720710754395\n",
      "Warning: nan gradient found. The current loss is:  0.20676495134830475\n",
      "Warning: nan gradient found. The current loss is:  1.0792210102081299\n",
      "Warning: nan gradient found. The current loss is:  0.33411237597465515\n",
      "Warning: nan gradient found. The current loss is:  1.2638137340545654\n",
      "Warning: nan gradient found. The current loss is:  0.9039678573608398\n",
      "Warning: nan gradient found. The current loss is:  0.05300427973270416\n",
      "Warning: nan gradient found. The current loss is:  0.3751552700996399\n",
      "Warning: nan gradient found. The current loss is:  0.441847562789917\n",
      "Warning: nan gradient found. The current loss is:  0.41427886486053467\n",
      "Warning: nan gradient found. The current loss is:  0.7020506858825684\n",
      "Warning: nan gradient found. The current loss is:  0.2245364785194397\n",
      "Warning: nan gradient found. The current loss is:  0.26514697074890137\n",
      "Warning: nan gradient found. The current loss is:  1.1023619174957275\n",
      "Warning: nan gradient found. The current loss is:  0.4661858081817627\n",
      "Warning: nan gradient found. The current loss is:  0.4220166802406311\n",
      "Warning: nan gradient found. The current loss is:  0.24026870727539062\n",
      "Warning: nan gradient found. The current loss is:  0.35502976179122925\n",
      "Warning: nan gradient found. The current loss is:  0.6487186551094055\n",
      "Warning: nan gradient found. The current loss is:  1.037862777709961\n",
      "Warning: nan gradient found. The current loss is:  1.7766245603561401\n",
      "Warning: nan gradient found. The current loss is:  0.40696579217910767\n",
      "Warning: nan gradient found. The current loss is:  0.4690030515193939\n",
      "Warning: nan gradient found. The current loss is:  0.8131871223449707\n",
      "Warning: nan gradient found. The current loss is:  0.3188033699989319\n",
      "Warning: nan gradient found. The current loss is:  0.5635332465171814\n",
      "Warning: nan gradient found. The current loss is:  0.44257277250289917\n",
      "Warning: nan gradient found. The current loss is:  0.24420562386512756\n",
      "Warning: nan gradient found. The current loss is:  0.45678234100341797\n",
      "Warning: nan gradient found. The current loss is:  0.952627420425415\n",
      "Warning: nan gradient found. The current loss is:  0.15623053908348083\n",
      "Warning: nan gradient found. The current loss is:  0.39587661623954773\n",
      "Warning: nan gradient found. The current loss is:  0.0452408567070961\n",
      "Warning: nan gradient found. The current loss is:  0.5490585565567017\n",
      "Warning: nan gradient found. The current loss is:  0.4739822745323181\n",
      "Warning: nan gradient found. The current loss is:  0.35289111733436584\n",
      "Warning: nan gradient found. The current loss is:  0.9092770218849182\n",
      "Warning: nan gradient found. The current loss is:  0.08940216898918152\n",
      "Warning: nan gradient found. The current loss is:  0.4082966446876526\n",
      "Warning: nan gradient found. The current loss is:  0.37754175066947937\n",
      "Warning: nan gradient found. The current loss is:  0.6898462772369385\n",
      "Warning: nan gradient found. The current loss is:  0.5334694981575012\n",
      "Warning: nan gradient found. The current loss is:  0.36782562732696533\n",
      "Warning: nan gradient found. The current loss is:  0.6829874515533447\n",
      "Warning: nan gradient found. The current loss is:  0.23911544680595398\n",
      "Warning: nan gradient found. The current loss is:  0.5412224531173706\n",
      "Warning: nan gradient found. The current loss is:  0.42947256565093994\n",
      "Warning: nan gradient found. The current loss is:  0.1126008927822113\n",
      "Warning: nan gradient found. The current loss is:  1.1146576404571533\n",
      "Warning: nan gradient found. The current loss is:  0.23054923117160797\n",
      "Warning: nan gradient found. The current loss is:  0.7325402498245239\n",
      "Warning: nan gradient found. The current loss is:  0.2751617431640625\n",
      "Warning: nan gradient found. The current loss is:  0.056946761906147\n",
      "Warning: nan gradient found. The current loss is:  0.5923255681991577\n",
      "Warning: nan gradient found. The current loss is:  0.5884692668914795\n",
      "Warning: nan gradient found. The current loss is:  0.5762501358985901\n",
      "Warning: nan gradient found. The current loss is:  0.38241925835609436\n",
      "Warning: nan gradient found. The current loss is:  0.6388415098190308\n",
      "Warning: nan gradient found. The current loss is:  0.5595633387565613\n",
      "Warning: nan gradient found. The current loss is:  0.587627649307251\n",
      "Warning: nan gradient found. The current loss is:  0.9197738766670227\n",
      "Warning: nan gradient found. The current loss is:  0.5432989597320557\n",
      "Warning: nan gradient found. The current loss is:  0.374132364988327\n",
      "Warning: nan gradient found. The current loss is:  0.7301110029220581\n",
      "Warning: nan gradient found. The current loss is:  0.5006605386734009\n",
      "Warning: nan gradient found. The current loss is:  0.3171560764312744\n",
      "Warning: nan gradient found. The current loss is:  0.5125837326049805\n",
      "Warning: nan gradient found. The current loss is:  1.4194904565811157\n",
      "Warning: nan gradient found. The current loss is:  1.8239073753356934\n",
      "Warning: nan gradient found. The current loss is:  0.6369582414627075\n",
      "Warning: nan gradient found. The current loss is:  0.47878414392471313\n",
      "Warning: nan gradient found. The current loss is:  0.3899448812007904\n",
      "Warning: nan gradient found. The current loss is:  0.526494026184082\n",
      "Warning: nan gradient found. The current loss is:  0.37488454580307007\n",
      "Warning: nan gradient found. The current loss is:  0.27082404494285583\n",
      "Warning: nan gradient found. The current loss is:  0.48928895592689514\n",
      "Warning: nan gradient found. The current loss is:  0.3522511422634125\n",
      "Warning: nan gradient found. The current loss is:  1.052901268005371\n",
      "Warning: nan gradient found. The current loss is:  0.41723886132240295\n",
      "Warning: nan gradient found. The current loss is:  0.44872528314590454\n",
      "Warning: nan gradient found. The current loss is:  1.0148353576660156\n",
      "Warning: nan gradient found. The current loss is:  0.44948244094848633\n",
      "Warning: nan gradient found. The current loss is:  0.4510553479194641\n",
      "Warning: nan gradient found. The current loss is:  0.37308257818222046\n",
      "Warning: nan gradient found. The current loss is:  1.0562105178833008\n",
      "Warning: nan gradient found. The current loss is:  0.7918729782104492\n",
      "Warning: nan gradient found. The current loss is:  0.13080821931362152\n",
      "Warning: nan gradient found. The current loss is:  0.587855875492096\n",
      "Warning: nan gradient found. The current loss is:  0.22507740557193756\n",
      "Warning: nan gradient found. The current loss is:  0.4086499810218811\n",
      "Warning: nan gradient found. The current loss is:  0.24321803450584412\n",
      "Warning: nan gradient found. The current loss is:  0.9433233737945557\n",
      "Warning: nan gradient found. The current loss is:  0.47330817580223083\n",
      "Current batch training loss: 0.473308  [665600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5326619744300842\n",
      "Warning: nan gradient found. The current loss is:  0.7978318929672241\n",
      "Warning: nan gradient found. The current loss is:  0.3163627088069916\n",
      "Warning: nan gradient found. The current loss is:  0.14486904442310333\n",
      "Warning: nan gradient found. The current loss is:  0.06392034888267517\n",
      "Warning: nan gradient found. The current loss is:  1.219069242477417\n",
      "Warning: nan gradient found. The current loss is:  0.5308112502098083\n",
      "Warning: nan gradient found. The current loss is:  0.3944215774536133\n",
      "Warning: nan gradient found. The current loss is:  0.5282406806945801\n",
      "Warning: nan gradient found. The current loss is:  0.6066420674324036\n",
      "Warning: nan gradient found. The current loss is:  0.3671591281890869\n",
      "Warning: nan gradient found. The current loss is:  1.2227838039398193\n",
      "Warning: nan gradient found. The current loss is:  0.44189929962158203\n",
      "Warning: nan gradient found. The current loss is:  0.0609985776245594\n",
      "Warning: nan gradient found. The current loss is:  0.3066840171813965\n",
      "Warning: nan gradient found. The current loss is:  0.45564764738082886\n",
      "Warning: nan gradient found. The current loss is:  0.2636028230190277\n",
      "Warning: nan gradient found. The current loss is:  0.3623610734939575\n",
      "Warning: nan gradient found. The current loss is:  0.5227500200271606\n",
      "Warning: nan gradient found. The current loss is:  1.763197660446167\n",
      "Warning: nan gradient found. The current loss is:  0.29275941848754883\n",
      "Warning: nan gradient found. The current loss is:  0.4680348038673401\n",
      "Warning: nan gradient found. The current loss is:  0.1793050616979599\n",
      "Warning: nan gradient found. The current loss is:  0.7898356914520264\n",
      "Warning: nan gradient found. The current loss is:  0.4943690299987793\n",
      "Warning: nan gradient found. The current loss is:  0.8088811635971069\n",
      "Warning: nan gradient found. The current loss is:  0.6304727792739868\n",
      "Warning: nan gradient found. The current loss is:  0.960843026638031\n",
      "Warning: nan gradient found. The current loss is:  0.8965732455253601\n",
      "Warning: nan gradient found. The current loss is:  0.7315937876701355\n",
      "Warning: nan gradient found. The current loss is:  0.4810546338558197\n",
      "Warning: nan gradient found. The current loss is:  0.30729565024375916\n",
      "Warning: nan gradient found. The current loss is:  0.44591253995895386\n",
      "Warning: nan gradient found. The current loss is:  0.6130478382110596\n",
      "Warning: nan gradient found. The current loss is:  0.3219761848449707\n",
      "Warning: nan gradient found. The current loss is:  0.33511972427368164\n",
      "Warning: nan gradient found. The current loss is:  0.33107519149780273\n",
      "Warning: nan gradient found. The current loss is:  0.4525940418243408\n",
      "Warning: nan gradient found. The current loss is:  0.03637080639600754\n",
      "Warning: nan gradient found. The current loss is:  0.18675491213798523\n",
      "Warning: nan gradient found. The current loss is:  1.0393106937408447\n",
      "Warning: nan gradient found. The current loss is:  0.3954887390136719\n",
      "Warning: nan gradient found. The current loss is:  0.2627592980861664\n",
      "Warning: nan gradient found. The current loss is:  0.17214995622634888\n",
      "Warning: nan gradient found. The current loss is:  0.16720931231975555\n",
      "Warning: nan gradient found. The current loss is:  0.2895781695842743\n",
      "Warning: nan gradient found. The current loss is:  0.4602222144603729\n",
      "Warning: nan gradient found. The current loss is:  0.3942103981971741\n",
      "Warning: nan gradient found. The current loss is:  0.33670657873153687\n",
      "Warning: nan gradient found. The current loss is:  0.4478181004524231\n",
      "Warning: nan gradient found. The current loss is:  0.6554754972457886\n",
      "Warning: nan gradient found. The current loss is:  0.786575436592102\n",
      "Warning: nan gradient found. The current loss is:  0.3145598769187927\n",
      "Warning: nan gradient found. The current loss is:  0.13085077702999115\n",
      "Warning: nan gradient found. The current loss is:  0.9209417700767517\n",
      "Warning: nan gradient found. The current loss is:  0.5000167489051819\n",
      "Warning: nan gradient found. The current loss is:  0.190776064991951\n",
      "Warning: nan gradient found. The current loss is:  0.2493259757757187\n",
      "Warning: nan gradient found. The current loss is:  0.1297638714313507\n",
      "Warning: nan gradient found. The current loss is:  0.12656661868095398\n",
      "Warning: nan gradient found. The current loss is:  0.4209873080253601\n",
      "Warning: nan gradient found. The current loss is:  0.5900550484657288\n",
      "Warning: nan gradient found. The current loss is:  0.39959657192230225\n",
      "Warning: nan gradient found. The current loss is:  0.33078277111053467\n",
      "Warning: nan gradient found. The current loss is:  0.697638213634491\n",
      "Warning: nan gradient found. The current loss is:  0.16050617396831512\n",
      "Warning: nan gradient found. The current loss is:  0.10838783532381058\n",
      "Warning: nan gradient found. The current loss is:  0.2638706564903259\n",
      "Warning: nan gradient found. The current loss is:  0.2322494387626648\n",
      "Warning: nan gradient found. The current loss is:  0.8531561493873596\n",
      "Warning: nan gradient found. The current loss is:  0.5004967451095581\n",
      "Warning: nan gradient found. The current loss is:  0.48808133602142334\n",
      "Warning: nan gradient found. The current loss is:  0.3041045665740967\n",
      "Warning: nan gradient found. The current loss is:  0.4167109727859497\n",
      "Warning: nan gradient found. The current loss is:  0.8693444132804871\n",
      "Warning: nan gradient found. The current loss is:  0.43565690517425537\n",
      "Warning: nan gradient found. The current loss is:  0.2473638504743576\n",
      "Warning: nan gradient found. The current loss is:  0.9608228206634521\n",
      "Warning: nan gradient found. The current loss is:  0.6579938530921936\n",
      "Warning: nan gradient found. The current loss is:  0.16213122010231018\n",
      "Warning: nan gradient found. The current loss is:  0.6013911962509155\n",
      "Warning: nan gradient found. The current loss is:  0.8587441444396973\n",
      "Warning: nan gradient found. The current loss is:  0.3982155919075012\n",
      "Warning: nan gradient found. The current loss is:  0.12766021490097046\n",
      "Warning: nan gradient found. The current loss is:  0.16286490857601166\n",
      "Warning: nan gradient found. The current loss is:  0.6691255569458008\n",
      "Warning: nan gradient found. The current loss is:  0.19988548755645752\n",
      "Warning: nan gradient found. The current loss is:  0.2889963984489441\n",
      "Warning: nan gradient found. The current loss is:  0.7221965789794922\n",
      "Warning: nan gradient found. The current loss is:  0.7149889469146729\n",
      "Warning: nan gradient found. The current loss is:  0.8659635782241821\n",
      "Warning: nan gradient found. The current loss is:  0.19938941299915314\n",
      "Warning: nan gradient found. The current loss is:  1.109734058380127\n",
      "Warning: nan gradient found. The current loss is:  0.19833552837371826\n",
      "Warning: nan gradient found. The current loss is:  0.4476168155670166\n",
      "Warning: nan gradient found. The current loss is:  0.2533896267414093\n",
      "Warning: nan gradient found. The current loss is:  0.21204788982868195\n",
      "Warning: nan gradient found. The current loss is:  -0.01410132646560669\n",
      "Warning: nan gradient found. The current loss is:  0.7903134822845459\n",
      "Warning: nan gradient found. The current loss is:  0.12351218611001968\n",
      "Current batch training loss: 0.123512  [691200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.30372172594070435\n",
      "Warning: nan gradient found. The current loss is:  0.6555541753768921\n",
      "Warning: nan gradient found. The current loss is:  0.32684212923049927\n",
      "Warning: nan gradient found. The current loss is:  0.7592151165008545\n",
      "Warning: nan gradient found. The current loss is:  0.435208261013031\n",
      "Warning: nan gradient found. The current loss is:  1.2193611860275269\n",
      "Warning: nan gradient found. The current loss is:  0.46301692724227905\n",
      "Warning: nan gradient found. The current loss is:  0.3701530992984772\n",
      "Warning: nan gradient found. The current loss is:  0.2828657031059265\n",
      "Warning: nan gradient found. The current loss is:  0.23271575570106506\n",
      "Warning: nan gradient found. The current loss is:  0.0831320583820343\n",
      "Warning: nan gradient found. The current loss is:  0.12230182439088821\n",
      "Warning: nan gradient found. The current loss is:  0.1553756296634674\n",
      "Warning: nan gradient found. The current loss is:  0.39442336559295654\n",
      "Warning: nan gradient found. The current loss is:  0.5217649936676025\n",
      "Warning: nan gradient found. The current loss is:  -0.0032378733158111572\n",
      "Warning: nan gradient found. The current loss is:  0.23184555768966675\n",
      "Warning: nan gradient found. The current loss is:  0.35354286432266235\n",
      "Warning: nan gradient found. The current loss is:  0.20895719528198242\n",
      "Warning: nan gradient found. The current loss is:  0.6833896040916443\n",
      "Warning: nan gradient found. The current loss is:  0.4439869523048401\n",
      "Warning: nan gradient found. The current loss is:  1.2585787773132324\n",
      "Warning: nan gradient found. The current loss is:  0.542568564414978\n",
      "Warning: nan gradient found. The current loss is:  1.6701908111572266\n",
      "Warning: nan gradient found. The current loss is:  -0.0024178922176361084\n",
      "Warning: nan gradient found. The current loss is:  0.5726969838142395\n",
      "Warning: nan gradient found. The current loss is:  0.8328354358673096\n",
      "Warning: nan gradient found. The current loss is:  0.6123679876327515\n",
      "Warning: nan gradient found. The current loss is:  0.639737069606781\n",
      "Warning: nan gradient found. The current loss is:  0.442283034324646\n",
      "Warning: nan gradient found. The current loss is:  0.7990731000900269\n",
      "Warning: nan gradient found. The current loss is:  0.8773419857025146\n",
      "Warning: nan gradient found. The current loss is:  0.9457108974456787\n",
      "Warning: nan gradient found. The current loss is:  0.37708762288093567\n",
      "Warning: nan gradient found. The current loss is:  1.177788257598877\n",
      "Warning: nan gradient found. The current loss is:  0.4006468653678894\n",
      "Warning: nan gradient found. The current loss is:  0.8020591735839844\n",
      "Warning: nan gradient found. The current loss is:  0.7573025822639465\n",
      "Warning: nan gradient found. The current loss is:  0.4505642056465149\n",
      "Warning: nan gradient found. The current loss is:  0.2914833426475525\n",
      "Warning: nan gradient found. The current loss is:  1.1528578996658325\n",
      "Warning: nan gradient found. The current loss is:  0.4905950129032135\n",
      "Warning: nan gradient found. The current loss is:  0.3760994076728821\n",
      "Warning: nan gradient found. The current loss is:  0.3962738513946533\n",
      "Warning: nan gradient found. The current loss is:  0.19281649589538574\n",
      "Warning: nan gradient found. The current loss is:  0.236038476228714\n",
      "Warning: nan gradient found. The current loss is:  0.4588603377342224\n",
      "Warning: nan gradient found. The current loss is:  0.6631280779838562\n",
      "Warning: nan gradient found. The current loss is:  0.22702616453170776\n",
      "Warning: nan gradient found. The current loss is:  0.6363699436187744\n",
      "Warning: nan gradient found. The current loss is:  0.1961696743965149\n",
      "Warning: nan gradient found. The current loss is:  0.3698093295097351\n",
      "Warning: nan gradient found. The current loss is:  0.9302423000335693\n",
      "Warning: nan gradient found. The current loss is:  0.957055389881134\n",
      "Warning: nan gradient found. The current loss is:  0.5969424843788147\n",
      "Warning: nan gradient found. The current loss is:  0.1034531220793724\n",
      "Warning: nan gradient found. The current loss is:  0.5416638851165771\n",
      "Warning: nan gradient found. The current loss is:  0.4446861147880554\n",
      "Warning: nan gradient found. The current loss is:  0.49390363693237305\n",
      "Warning: nan gradient found. The current loss is:  0.514689564704895\n",
      "Warning: nan gradient found. The current loss is:  0.9225564002990723\n",
      "Warning: nan gradient found. The current loss is:  0.2152770459651947\n",
      "Warning: nan gradient found. The current loss is:  0.063251793384552\n",
      "Warning: nan gradient found. The current loss is:  0.5801982879638672\n",
      "Warning: nan gradient found. The current loss is:  0.6109128594398499\n",
      "Warning: nan gradient found. The current loss is:  0.5114864110946655\n",
      "Warning: nan gradient found. The current loss is:  0.8069183230400085\n",
      "Warning: nan gradient found. The current loss is:  1.0602822303771973\n",
      "Warning: nan gradient found. The current loss is:  0.5464884042739868\n",
      "Warning: nan gradient found. The current loss is:  0.1918896734714508\n",
      "Warning: nan gradient found. The current loss is:  0.3522931933403015\n",
      "Warning: nan gradient found. The current loss is:  0.2243463546037674\n",
      "Warning: nan gradient found. The current loss is:  0.5104119777679443\n",
      "Warning: nan gradient found. The current loss is:  1.2617900371551514\n",
      "Warning: nan gradient found. The current loss is:  0.7924966812133789\n",
      "Warning: nan gradient found. The current loss is:  0.3892282545566559\n",
      "Warning: nan gradient found. The current loss is:  0.8130545616149902\n",
      "Warning: nan gradient found. The current loss is:  0.25313031673431396\n",
      "Warning: nan gradient found. The current loss is:  0.1258830726146698\n",
      "Warning: nan gradient found. The current loss is:  0.5733795762062073\n",
      "Warning: nan gradient found. The current loss is:  0.2897390127182007\n",
      "Warning: nan gradient found. The current loss is:  0.21605676412582397\n",
      "Warning: nan gradient found. The current loss is:  1.6140480041503906\n",
      "Warning: nan gradient found. The current loss is:  0.5117294788360596\n",
      "Warning: nan gradient found. The current loss is:  0.13296297192573547\n",
      "Warning: nan gradient found. The current loss is:  1.086771011352539\n",
      "Warning: nan gradient found. The current loss is:  0.42803439497947693\n",
      "Warning: nan gradient found. The current loss is:  0.5483971834182739\n",
      "Warning: nan gradient found. The current loss is:  0.28643691539764404\n",
      "Warning: nan gradient found. The current loss is:  0.042154040187597275\n",
      "Warning: nan gradient found. The current loss is:  1.2124320268630981\n",
      "Warning: nan gradient found. The current loss is:  0.33076417446136475\n",
      "Warning: nan gradient found. The current loss is:  0.2931927740573883\n",
      "Warning: nan gradient found. The current loss is:  0.13222187757492065\n",
      "Warning: nan gradient found. The current loss is:  0.45983994007110596\n",
      "Warning: nan gradient found. The current loss is:  0.2015894651412964\n",
      "Warning: nan gradient found. The current loss is:  1.074601173400879\n",
      "Warning: nan gradient found. The current loss is:  0.3817319869995117\n",
      "Warning: nan gradient found. The current loss is:  0.08718395233154297\n",
      "Warning: nan gradient found. The current loss is:  0.5326813459396362\n",
      "Current batch training loss: 0.532681  [716800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.49695903062820435\n",
      "Warning: nan gradient found. The current loss is:  -0.08565492928028107\n",
      "Warning: nan gradient found. The current loss is:  0.6647852659225464\n",
      "Warning: nan gradient found. The current loss is:  0.397038996219635\n",
      "Warning: nan gradient found. The current loss is:  0.04514108598232269\n",
      "Warning: nan gradient found. The current loss is:  0.6039699912071228\n",
      "Warning: nan gradient found. The current loss is:  0.9458627700805664\n",
      "Warning: nan gradient found. The current loss is:  0.817299485206604\n",
      "Warning: nan gradient found. The current loss is:  0.26762405037879944\n",
      "Warning: nan gradient found. The current loss is:  0.2634721100330353\n",
      "Warning: nan gradient found. The current loss is:  1.056053638458252\n",
      "Warning: nan gradient found. The current loss is:  0.7947626113891602\n",
      "Warning: nan gradient found. The current loss is:  0.3935382068157196\n",
      "Warning: nan gradient found. The current loss is:  1.1316804885864258\n",
      "Warning: nan gradient found. The current loss is:  1.0166603326797485\n",
      "Warning: nan gradient found. The current loss is:  0.6302253007888794\n",
      "Warning: nan gradient found. The current loss is:  0.35270607471466064\n",
      "Warning: nan gradient found. The current loss is:  0.23708513379096985\n",
      "Warning: nan gradient found. The current loss is:  0.3679388761520386\n",
      "Warning: nan gradient found. The current loss is:  0.46729734539985657\n",
      "Warning: nan gradient found. The current loss is:  1.2797069549560547\n",
      "Warning: nan gradient found. The current loss is:  0.19913271069526672\n",
      "Warning: nan gradient found. The current loss is:  1.1140189170837402\n",
      "Warning: nan gradient found. The current loss is:  0.37253865599632263\n",
      "Warning: nan gradient found. The current loss is:  0.37771350145339966\n",
      "Warning: nan gradient found. The current loss is:  0.4790579676628113\n",
      "Warning: nan gradient found. The current loss is:  0.9518851041793823\n",
      "Warning: nan gradient found. The current loss is:  0.6117409467697144\n",
      "Warning: nan gradient found. The current loss is:  0.5138438940048218\n",
      "Warning: nan gradient found. The current loss is:  0.7246105670928955\n",
      "Warning: nan gradient found. The current loss is:  0.9970772862434387\n",
      "Warning: nan gradient found. The current loss is:  0.18173277378082275\n",
      "Warning: nan gradient found. The current loss is:  0.41530171036720276\n",
      "Warning: nan gradient found. The current loss is:  0.49057695269584656\n",
      "Warning: nan gradient found. The current loss is:  1.1038092374801636\n",
      "Warning: nan gradient found. The current loss is:  0.27084216475486755\n",
      "Warning: nan gradient found. The current loss is:  0.4621499180793762\n",
      "Warning: nan gradient found. The current loss is:  0.7336689233779907\n",
      "Warning: nan gradient found. The current loss is:  0.018759265542030334\n",
      "Warning: nan gradient found. The current loss is:  0.6262159943580627\n",
      "Warning: nan gradient found. The current loss is:  0.712820291519165\n",
      "Warning: nan gradient found. The current loss is:  0.8742733597755432\n",
      "Warning: nan gradient found. The current loss is:  2.0968902111053467\n",
      "Warning: nan gradient found. The current loss is:  0.25542646646499634\n",
      "Warning: nan gradient found. The current loss is:  0.07564923912286758\n",
      "Warning: nan gradient found. The current loss is:  0.8515543341636658\n",
      "Warning: nan gradient found. The current loss is:  0.6234310865402222\n",
      "Warning: nan gradient found. The current loss is:  1.5190355777740479\n",
      "Warning: nan gradient found. The current loss is:  0.5666027665138245\n",
      "Warning: nan gradient found. The current loss is:  0.06784750521183014\n",
      "Warning: nan gradient found. The current loss is:  0.5778946876525879\n",
      "Warning: nan gradient found. The current loss is:  0.4604787230491638\n",
      "Warning: nan gradient found. The current loss is:  2.1683599948883057\n",
      "Warning: nan gradient found. The current loss is:  0.8119266033172607\n",
      "Warning: nan gradient found. The current loss is:  0.2925685942173004\n",
      "Warning: nan gradient found. The current loss is:  0.2903534770011902\n",
      "Warning: nan gradient found. The current loss is:  0.19627614319324493\n",
      "Warning: nan gradient found. The current loss is:  0.33351853489875793\n",
      "Warning: nan gradient found. The current loss is:  0.2788044214248657\n",
      "Warning: nan gradient found. The current loss is:  0.48462730646133423\n",
      "Warning: nan gradient found. The current loss is:  0.2737237215042114\n",
      "Warning: nan gradient found. The current loss is:  0.6162412166595459\n",
      "Warning: nan gradient found. The current loss is:  0.7544697523117065\n",
      "Warning: nan gradient found. The current loss is:  1.2540712356567383\n",
      "Warning: nan gradient found. The current loss is:  0.3827366828918457\n",
      "Warning: nan gradient found. The current loss is:  0.7268704175949097\n",
      "Warning: nan gradient found. The current loss is:  0.4729362428188324\n",
      "Warning: nan gradient found. The current loss is:  0.2426513433456421\n",
      "Warning: nan gradient found. The current loss is:  0.777948260307312\n",
      "Warning: nan gradient found. The current loss is:  0.10305024683475494\n",
      "Warning: nan gradient found. The current loss is:  0.3555923402309418\n",
      "Warning: nan gradient found. The current loss is:  0.6234942674636841\n",
      "Warning: nan gradient found. The current loss is:  0.8921800851821899\n",
      "Warning: nan gradient found. The current loss is:  0.06020060181617737\n",
      "Warning: nan gradient found. The current loss is:  0.3941132724285126\n",
      "Warning: nan gradient found. The current loss is:  0.8935233354568481\n",
      "Warning: nan gradient found. The current loss is:  0.7536892890930176\n",
      "Warning: nan gradient found. The current loss is:  0.13332995772361755\n",
      "Warning: nan gradient found. The current loss is:  0.13021329045295715\n",
      "Warning: nan gradient found. The current loss is:  0.1351471245288849\n",
      "Warning: nan gradient found. The current loss is:  0.6705021858215332\n",
      "Warning: nan gradient found. The current loss is:  0.025823846459388733\n",
      "Warning: nan gradient found. The current loss is:  1.2659080028533936\n",
      "Warning: nan gradient found. The current loss is:  0.7093237638473511\n",
      "Warning: nan gradient found. The current loss is:  0.522184431552887\n",
      "Warning: nan gradient found. The current loss is:  0.7834920287132263\n",
      "Warning: nan gradient found. The current loss is:  0.24356995522975922\n",
      "Warning: nan gradient found. The current loss is:  0.8109129071235657\n",
      "Warning: nan gradient found. The current loss is:  0.1813381016254425\n",
      "Warning: nan gradient found. The current loss is:  0.5348227620124817\n",
      "Warning: nan gradient found. The current loss is:  1.024550437927246\n",
      "Warning: nan gradient found. The current loss is:  0.4087008833885193\n",
      "Warning: nan gradient found. The current loss is:  0.41151443123817444\n",
      "Warning: nan gradient found. The current loss is:  0.48501670360565186\n",
      "Warning: nan gradient found. The current loss is:  1.5806245803833008\n",
      "Warning: nan gradient found. The current loss is:  0.09477539360523224\n",
      "Warning: nan gradient found. The current loss is:  0.536658525466919\n",
      "Warning: nan gradient found. The current loss is:  0.70716792345047\n",
      "Warning: nan gradient found. The current loss is:  0.49940747022628784\n",
      "Current batch training loss: 0.499407  [742400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6043088436126709\n",
      "Warning: nan gradient found. The current loss is:  0.9875383377075195\n",
      "Warning: nan gradient found. The current loss is:  0.6436686515808105\n",
      "Warning: nan gradient found. The current loss is:  0.8613325357437134\n",
      "Warning: nan gradient found. The current loss is:  0.774915874004364\n",
      "Warning: nan gradient found. The current loss is:  1.4570834636688232\n",
      "Warning: nan gradient found. The current loss is:  0.37391090393066406\n",
      "Warning: nan gradient found. The current loss is:  0.8047678470611572\n",
      "Warning: nan gradient found. The current loss is:  0.5847885608673096\n",
      "Warning: nan gradient found. The current loss is:  2.181962251663208\n",
      "Warning: nan gradient found. The current loss is:  0.3561559021472931\n",
      "Warning: nan gradient found. The current loss is:  1.0958881378173828\n",
      "Warning: nan gradient found. The current loss is:  1.125022292137146\n",
      "Warning: nan gradient found. The current loss is:  0.9053468704223633\n",
      "Warning: nan gradient found. The current loss is:  0.48761001229286194\n",
      "Warning: nan gradient found. The current loss is:  0.5139402747154236\n",
      "Warning: nan gradient found. The current loss is:  0.498584508895874\n",
      "Warning: nan gradient found. The current loss is:  0.7290037274360657\n",
      "Warning: nan gradient found. The current loss is:  0.31940552592277527\n",
      "Warning: nan gradient found. The current loss is:  1.4873696565628052\n",
      "Warning: nan gradient found. The current loss is:  1.7245417833328247\n",
      "Warning: nan gradient found. The current loss is:  0.8134973645210266\n",
      "Warning: nan gradient found. The current loss is:  0.9755962491035461\n",
      "Warning: nan gradient found. The current loss is:  0.3379027545452118\n",
      "Warning: nan gradient found. The current loss is:  0.6390397548675537\n",
      "Warning: nan gradient found. The current loss is:  0.8406047224998474\n",
      "Warning: nan gradient found. The current loss is:  1.5353147983551025\n",
      "Warning: nan gradient found. The current loss is:  0.43688976764678955\n",
      "Warning: nan gradient found. The current loss is:  0.3875194191932678\n",
      "Warning: nan gradient found. The current loss is:  0.628901481628418\n",
      "Warning: nan gradient found. The current loss is:  1.7832646369934082\n",
      "Warning: nan gradient found. The current loss is:  1.0886187553405762\n",
      "Warning: nan gradient found. The current loss is:  0.9028611183166504\n",
      "Warning: nan gradient found. The current loss is:  0.01329263299703598\n",
      "Warning: nan gradient found. The current loss is:  1.060686707496643\n",
      "Warning: nan gradient found. The current loss is:  0.5254402756690979\n",
      "Warning: nan gradient found. The current loss is:  1.0372759103775024\n",
      "Warning: nan gradient found. The current loss is:  0.46215173602104187\n",
      "Warning: nan gradient found. The current loss is:  1.0623433589935303\n",
      "Warning: nan gradient found. The current loss is:  0.5852029919624329\n",
      "Warning: nan gradient found. The current loss is:  0.5416065454483032\n",
      "Warning: nan gradient found. The current loss is:  0.6459940671920776\n",
      "Warning: nan gradient found. The current loss is:  0.3272162675857544\n",
      "Warning: nan gradient found. The current loss is:  1.1693238019943237\n",
      "Warning: nan gradient found. The current loss is:  0.39395609498023987\n",
      "Warning: nan gradient found. The current loss is:  0.2362625002861023\n",
      "Warning: nan gradient found. The current loss is:  1.1246147155761719\n",
      "Warning: nan gradient found. The current loss is:  0.5053948163986206\n",
      "Warning: nan gradient found. The current loss is:  0.8194513320922852\n",
      "Warning: nan gradient found. The current loss is:  1.8160090446472168\n",
      "Warning: nan gradient found. The current loss is:  0.2568221986293793\n",
      "Warning: nan gradient found. The current loss is:  0.7668628692626953\n",
      "Warning: nan gradient found. The current loss is:  0.7283605337142944\n",
      "Warning: nan gradient found. The current loss is:  0.7209973931312561\n",
      "Warning: nan gradient found. The current loss is:  0.5992677211761475\n",
      "Warning: nan gradient found. The current loss is:  0.21876533329486847\n",
      "Warning: nan gradient found. The current loss is:  0.3352559208869934\n",
      "Warning: nan gradient found. The current loss is:  0.3555911183357239\n",
      "Warning: nan gradient found. The current loss is:  0.27787885069847107\n",
      "Warning: nan gradient found. The current loss is:  0.33938008546829224\n",
      "Warning: nan gradient found. The current loss is:  0.786819338798523\n",
      "Warning: nan gradient found. The current loss is:  1.1869537830352783\n",
      "Warning: nan gradient found. The current loss is:  1.4830985069274902\n",
      "Warning: nan gradient found. The current loss is:  0.4514153003692627\n",
      "Warning: nan gradient found. The current loss is:  1.171467900276184\n",
      "Warning: nan gradient found. The current loss is:  1.0531253814697266\n",
      "Warning: nan gradient found. The current loss is:  0.6892316341400146\n",
      "Warning: nan gradient found. The current loss is:  0.43877020478248596\n",
      "Warning: nan gradient found. The current loss is:  0.6487326622009277\n",
      "Warning: nan gradient found. The current loss is:  0.3813021779060364\n",
      "Warning: nan gradient found. The current loss is:  0.45375049114227295\n",
      "Warning: nan gradient found. The current loss is:  1.0359556674957275\n",
      "Warning: nan gradient found. The current loss is:  0.5212377905845642\n",
      "Warning: nan gradient found. The current loss is:  0.222201868891716\n",
      "Warning: nan gradient found. The current loss is:  0.8637528419494629\n",
      "Warning: nan gradient found. The current loss is:  0.7568414211273193\n",
      "Warning: nan gradient found. The current loss is:  0.7326478958129883\n",
      "Warning: nan gradient found. The current loss is:  0.749011218547821\n",
      "Warning: nan gradient found. The current loss is:  0.4559171199798584\n",
      "Warning: nan gradient found. The current loss is:  0.5901126861572266\n",
      "Warning: nan gradient found. The current loss is:  0.7897775769233704\n",
      "Warning: nan gradient found. The current loss is:  0.6833118200302124\n",
      "Warning: nan gradient found. The current loss is:  0.24742776155471802\n",
      "Warning: nan gradient found. The current loss is:  0.8518417477607727\n",
      "Warning: nan gradient found. The current loss is:  0.36479905247688293\n",
      "Warning: nan gradient found. The current loss is:  0.5042254328727722\n",
      "Warning: nan gradient found. The current loss is:  0.8668220043182373\n",
      "Warning: nan gradient found. The current loss is:  0.920616626739502\n",
      "Warning: nan gradient found. The current loss is:  0.6049057841300964\n",
      "Warning: nan gradient found. The current loss is:  0.2881717085838318\n",
      "Warning: nan gradient found. The current loss is:  0.5282663106918335\n",
      "Warning: nan gradient found. The current loss is:  0.9363101720809937\n",
      "Warning: nan gradient found. The current loss is:  1.6084794998168945\n",
      "Warning: nan gradient found. The current loss is:  0.7720133066177368\n",
      "Warning: nan gradient found. The current loss is:  0.39738088846206665\n",
      "Warning: nan gradient found. The current loss is:  0.550851583480835\n",
      "Warning: nan gradient found. The current loss is:  1.1122453212738037\n",
      "Warning: nan gradient found. The current loss is:  0.3057977557182312\n",
      "Warning: nan gradient found. The current loss is:  0.5279538631439209\n",
      "Warning: nan gradient found. The current loss is:  0.7491829991340637\n",
      "Current batch training loss: 0.749183  [768000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  2.6249403953552246\n",
      "Warning: nan gradient found. The current loss is:  0.4781244397163391\n",
      "Warning: nan gradient found. The current loss is:  0.6270593404769897\n",
      "Warning: nan gradient found. The current loss is:  0.7063837647438049\n",
      "Warning: nan gradient found. The current loss is:  0.5758192539215088\n",
      "Warning: nan gradient found. The current loss is:  0.4860079884529114\n",
      "Warning: nan gradient found. The current loss is:  0.718323826789856\n",
      "Warning: nan gradient found. The current loss is:  1.0302131175994873\n",
      "Warning: nan gradient found. The current loss is:  0.3378859758377075\n",
      "Warning: nan gradient found. The current loss is:  0.24848560988903046\n",
      "Warning: nan gradient found. The current loss is:  0.4345489740371704\n",
      "Warning: nan gradient found. The current loss is:  0.7777115106582642\n",
      "Warning: nan gradient found. The current loss is:  0.7275251746177673\n",
      "Warning: nan gradient found. The current loss is:  1.245950698852539\n",
      "Warning: nan gradient found. The current loss is:  0.05934824422001839\n",
      "Warning: nan gradient found. The current loss is:  1.1190282106399536\n",
      "Warning: nan gradient found. The current loss is:  0.8415576815605164\n",
      "Warning: nan gradient found. The current loss is:  0.6157494783401489\n",
      "Warning: nan gradient found. The current loss is:  0.0222274512052536\n",
      "Warning: nan gradient found. The current loss is:  1.0118420124053955\n",
      "Warning: nan gradient found. The current loss is:  0.8520544767379761\n",
      "Warning: nan gradient found. The current loss is:  0.5382218956947327\n",
      "Warning: nan gradient found. The current loss is:  0.9103906154632568\n",
      "Warning: nan gradient found. The current loss is:  0.7620679140090942\n",
      "Warning: nan gradient found. The current loss is:  1.0789226293563843\n",
      "Warning: nan gradient found. The current loss is:  0.5529611706733704\n",
      "Warning: nan gradient found. The current loss is:  1.1803654432296753\n",
      "Warning: nan gradient found. The current loss is:  0.7442190647125244\n",
      "Warning: nan gradient found. The current loss is:  1.6055594682693481\n",
      "Warning: nan gradient found. The current loss is:  0.6975483894348145\n",
      "Warning: nan gradient found. The current loss is:  0.2259868085384369\n",
      "Warning: nan gradient found. The current loss is:  0.5058214068412781\n",
      "Warning: nan gradient found. The current loss is:  1.0306512117385864\n",
      "Warning: nan gradient found. The current loss is:  0.5771760940551758\n",
      "Warning: nan gradient found. The current loss is:  0.528039276599884\n",
      "Warning: nan gradient found. The current loss is:  1.1689114570617676\n",
      "Warning: nan gradient found. The current loss is:  1.2624785900115967\n",
      "Warning: nan gradient found. The current loss is:  0.7306992411613464\n",
      "Warning: nan gradient found. The current loss is:  0.5971278548240662\n",
      "Warning: nan gradient found. The current loss is:  0.5221875309944153\n",
      "Warning: nan gradient found. The current loss is:  0.44368183612823486\n",
      "Warning: nan gradient found. The current loss is:  0.3353953957557678\n",
      "Warning: nan gradient found. The current loss is:  0.5837552547454834\n",
      "Warning: nan gradient found. The current loss is:  0.3907407522201538\n",
      "Warning: nan gradient found. The current loss is:  1.1994152069091797\n",
      "Warning: nan gradient found. The current loss is:  0.5330710411071777\n",
      "Warning: nan gradient found. The current loss is:  1.2589051723480225\n",
      "Warning: nan gradient found. The current loss is:  1.034368634223938\n",
      "Warning: nan gradient found. The current loss is:  1.5611991882324219\n",
      "Warning: nan gradient found. The current loss is:  0.9174747467041016\n",
      "Warning: nan gradient found. The current loss is:  0.2955183982849121\n",
      "Warning: nan gradient found. The current loss is:  0.9632153511047363\n",
      "Warning: nan gradient found. The current loss is:  0.39300116896629333\n",
      "Warning: nan gradient found. The current loss is:  0.7645080089569092\n",
      "Warning: nan gradient found. The current loss is:  0.12334834784269333\n",
      "Warning: nan gradient found. The current loss is:  0.36454829573631287\n",
      "Warning: nan gradient found. The current loss is:  0.3416783809661865\n",
      "Warning: nan gradient found. The current loss is:  0.3459324240684509\n",
      "Warning: nan gradient found. The current loss is:  0.6763578057289124\n",
      "Warning: nan gradient found. The current loss is:  0.8769075870513916\n",
      "Warning: nan gradient found. The current loss is:  1.0451130867004395\n",
      "Warning: nan gradient found. The current loss is:  0.30681559443473816\n",
      "Warning: nan gradient found. The current loss is:  0.022734269499778748\n",
      "Warning: nan gradient found. The current loss is:  0.8949549794197083\n",
      "Warning: nan gradient found. The current loss is:  2.482664108276367\n",
      "Warning: nan gradient found. The current loss is:  0.4866713285446167\n",
      "Warning: nan gradient found. The current loss is:  0.9942411184310913\n",
      "Warning: nan gradient found. The current loss is:  0.31941038370132446\n",
      "Warning: nan gradient found. The current loss is:  0.012358441948890686\n",
      "Warning: nan gradient found. The current loss is:  0.6416916847229004\n",
      "Warning: nan gradient found. The current loss is:  0.6184395551681519\n",
      "Warning: nan gradient found. The current loss is:  0.6734641790390015\n",
      "Warning: nan gradient found. The current loss is:  0.4542900323867798\n",
      "Warning: nan gradient found. The current loss is:  0.6939860582351685\n",
      "Warning: nan gradient found. The current loss is:  0.6502169370651245\n",
      "Warning: nan gradient found. The current loss is:  0.6830097436904907\n",
      "Warning: nan gradient found. The current loss is:  0.5428760647773743\n",
      "Warning: nan gradient found. The current loss is:  1.126543402671814\n",
      "Warning: nan gradient found. The current loss is:  0.793100893497467\n",
      "Warning: nan gradient found. The current loss is:  0.04864536598324776\n",
      "Warning: nan gradient found. The current loss is:  0.343986451625824\n",
      "Warning: nan gradient found. The current loss is:  0.12768009305000305\n",
      "Warning: nan gradient found. The current loss is:  0.6022837162017822\n",
      "Warning: nan gradient found. The current loss is:  0.448893666267395\n",
      "Warning: nan gradient found. The current loss is:  0.4304550886154175\n",
      "Warning: nan gradient found. The current loss is:  0.5836700797080994\n",
      "Warning: nan gradient found. The current loss is:  0.19116632640361786\n",
      "Warning: nan gradient found. The current loss is:  0.5239931344985962\n",
      "Warning: nan gradient found. The current loss is:  0.44145452976226807\n",
      "Warning: nan gradient found. The current loss is:  0.49417269229888916\n",
      "Warning: nan gradient found. The current loss is:  0.7325618863105774\n",
      "Warning: nan gradient found. The current loss is:  0.1215086504817009\n",
      "Warning: nan gradient found. The current loss is:  0.8767330050468445\n",
      "Warning: nan gradient found. The current loss is:  0.5693638324737549\n",
      "Warning: nan gradient found. The current loss is:  0.30488288402557373\n",
      "Warning: nan gradient found. The current loss is:  0.3906019330024719\n",
      "Warning: nan gradient found. The current loss is:  0.9165140390396118\n",
      "Warning: nan gradient found. The current loss is:  0.5981569290161133\n",
      "Warning: nan gradient found. The current loss is:  0.2920088768005371\n",
      "Warning: nan gradient found. The current loss is:  0.5009177923202515\n",
      "Current batch training loss: 0.500918  [793600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4179723858833313\n",
      "Warning: nan gradient found. The current loss is:  0.2691294848918915\n",
      "Warning: nan gradient found. The current loss is:  0.40356189012527466\n",
      "Warning: nan gradient found. The current loss is:  0.09560389071702957\n",
      "Warning: nan gradient found. The current loss is:  0.5937811136245728\n",
      "Warning: nan gradient found. The current loss is:  0.8444796800613403\n",
      "Warning: nan gradient found. The current loss is:  1.137074589729309\n",
      "Warning: nan gradient found. The current loss is:  1.2979519367218018\n",
      "Warning: nan gradient found. The current loss is:  0.4540373384952545\n",
      "Warning: nan gradient found. The current loss is:  0.39465731382369995\n",
      "Warning: nan gradient found. The current loss is:  0.7720840573310852\n",
      "Warning: nan gradient found. The current loss is:  0.5394673347473145\n",
      "Warning: nan gradient found. The current loss is:  0.6162450909614563\n",
      "Warning: nan gradient found. The current loss is:  0.18905484676361084\n",
      "Warning: nan gradient found. The current loss is:  0.7219773530960083\n",
      "Warning: nan gradient found. The current loss is:  0.8135630488395691\n",
      "Warning: nan gradient found. The current loss is:  1.034145474433899\n",
      "Warning: nan gradient found. The current loss is:  1.058851957321167\n",
      "Warning: nan gradient found. The current loss is:  0.3582439422607422\n",
      "Warning: nan gradient found. The current loss is:  0.48612189292907715\n",
      "Warning: nan gradient found. The current loss is:  0.3708288371562958\n",
      "Warning: nan gradient found. The current loss is:  2.623955249786377\n",
      "Warning: nan gradient found. The current loss is:  0.463914155960083\n",
      "Warning: nan gradient found. The current loss is:  0.6028103232383728\n",
      "Warning: nan gradient found. The current loss is:  1.23447585105896\n",
      "Warning: nan gradient found. The current loss is:  0.44945383071899414\n",
      "Warning: nan gradient found. The current loss is:  1.0990597009658813\n",
      "Warning: nan gradient found. The current loss is:  0.1210486963391304\n",
      "Warning: nan gradient found. The current loss is:  1.2132108211517334\n",
      "Warning: nan gradient found. The current loss is:  1.2175188064575195\n",
      "Warning: nan gradient found. The current loss is:  0.691180944442749\n",
      "Warning: nan gradient found. The current loss is:  0.3797219395637512\n",
      "Warning: nan gradient found. The current loss is:  1.2423925399780273\n",
      "Warning: nan gradient found. The current loss is:  0.4157465994358063\n",
      "Warning: nan gradient found. The current loss is:  0.5183242559432983\n",
      "Warning: nan gradient found. The current loss is:  0.454498827457428\n",
      "Warning: nan gradient found. The current loss is:  0.23698917031288147\n",
      "Warning: nan gradient found. The current loss is:  1.500455617904663\n",
      "Warning: nan gradient found. The current loss is:  0.6993147730827332\n",
      "Warning: nan gradient found. The current loss is:  0.4276656210422516\n",
      "Warning: nan gradient found. The current loss is:  0.3238097131252289\n",
      "Warning: nan gradient found. The current loss is:  0.7334176301956177\n",
      "Warning: nan gradient found. The current loss is:  0.1399409919977188\n",
      "Warning: nan gradient found. The current loss is:  0.8338091373443604\n",
      "Warning: nan gradient found. The current loss is:  1.5527058839797974\n",
      "Warning: nan gradient found. The current loss is:  0.8848204612731934\n",
      "Warning: nan gradient found. The current loss is:  1.9176615476608276\n",
      "Warning: nan gradient found. The current loss is:  0.5850179195404053\n",
      "Warning: nan gradient found. The current loss is:  0.5774048566818237\n",
      "Warning: nan gradient found. The current loss is:  1.1969799995422363\n",
      "Warning: nan gradient found. The current loss is:  0.6465775966644287\n",
      "Warning: nan gradient found. The current loss is:  0.4109264612197876\n",
      "Warning: nan gradient found. The current loss is:  0.5090222358703613\n",
      "Warning: nan gradient found. The current loss is:  1.2295212745666504\n",
      "Warning: nan gradient found. The current loss is:  0.7834959030151367\n",
      "Warning: nan gradient found. The current loss is:  0.6686380505561829\n",
      "Warning: nan gradient found. The current loss is:  0.7918940782546997\n",
      "Warning: nan gradient found. The current loss is:  0.7877365350723267\n",
      "Warning: nan gradient found. The current loss is:  0.1267561912536621\n",
      "Warning: nan gradient found. The current loss is:  1.300915241241455\n",
      "Warning: nan gradient found. The current loss is:  0.3960533142089844\n",
      "Warning: nan gradient found. The current loss is:  1.0389301776885986\n",
      "Warning: nan gradient found. The current loss is:  1.5302963256835938\n",
      "Warning: nan gradient found. The current loss is:  0.7843421697616577\n",
      "Warning: nan gradient found. The current loss is:  0.39510083198547363\n",
      "Warning: nan gradient found. The current loss is:  0.9313414692878723\n",
      "Warning: nan gradient found. The current loss is:  0.6668071746826172\n",
      "Warning: nan gradient found. The current loss is:  3.5034422874450684\n",
      "Warning: nan gradient found. The current loss is:  0.3355622887611389\n",
      "Warning: nan gradient found. The current loss is:  0.7247868776321411\n",
      "Warning: nan gradient found. The current loss is:  1.2960104942321777\n",
      "Warning: nan gradient found. The current loss is:  0.6095250248908997\n",
      "Warning: nan gradient found. The current loss is:  0.8406916260719299\n",
      "Warning: nan gradient found. The current loss is:  0.6653597950935364\n",
      "Warning: nan gradient found. The current loss is:  0.5715252757072449\n",
      "Warning: nan gradient found. The current loss is:  1.182782769203186\n",
      "Warning: nan gradient found. The current loss is:  0.2340296357870102\n",
      "Warning: nan gradient found. The current loss is:  0.32304614782333374\n",
      "Warning: nan gradient found. The current loss is:  1.0817605257034302\n",
      "Warning: nan gradient found. The current loss is:  0.8548779487609863\n",
      "Warning: nan gradient found. The current loss is:  0.46230989694595337\n",
      "Warning: nan gradient found. The current loss is:  0.4868858754634857\n",
      "Warning: nan gradient found. The current loss is:  0.6450916528701782\n",
      "Warning: nan gradient found. The current loss is:  0.5398401021957397\n",
      "Warning: nan gradient found. The current loss is:  1.116204857826233\n",
      "Warning: nan gradient found. The current loss is:  1.0041792392730713\n",
      "Warning: nan gradient found. The current loss is:  1.149978518486023\n",
      "Warning: nan gradient found. The current loss is:  0.7659721970558167\n",
      "Warning: nan gradient found. The current loss is:  1.283124327659607\n",
      "Warning: nan gradient found. The current loss is:  1.3002746105194092\n",
      "Warning: nan gradient found. The current loss is:  0.9781006574630737\n",
      "Warning: nan gradient found. The current loss is:  1.8630186319351196\n",
      "Warning: nan gradient found. The current loss is:  0.5157867670059204\n",
      "Warning: nan gradient found. The current loss is:  0.5123308300971985\n",
      "Warning: nan gradient found. The current loss is:  0.45812374353408813\n",
      "Warning: nan gradient found. The current loss is:  0.43318748474121094\n",
      "Warning: nan gradient found. The current loss is:  0.7940337657928467\n",
      "Warning: nan gradient found. The current loss is:  0.20155665278434753\n",
      "Warning: nan gradient found. The current loss is:  1.425339698791504\n",
      "Warning: nan gradient found. The current loss is:  0.6899681091308594\n",
      "Current batch training loss: 0.689968  [819200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6136981248855591\n",
      "Warning: nan gradient found. The current loss is:  0.6408435702323914\n",
      "Warning: nan gradient found. The current loss is:  0.5040770769119263\n",
      "Warning: nan gradient found. The current loss is:  0.8574374318122864\n",
      "Warning: nan gradient found. The current loss is:  0.8304325938224792\n",
      "Warning: nan gradient found. The current loss is:  0.7948021292686462\n",
      "Warning: nan gradient found. The current loss is:  2.8883988857269287\n",
      "Warning: nan gradient found. The current loss is:  0.7326363921165466\n",
      "Warning: nan gradient found. The current loss is:  0.5334154367446899\n",
      "Warning: nan gradient found. The current loss is:  0.5887883901596069\n",
      "Warning: nan gradient found. The current loss is:  0.9209882020950317\n",
      "Warning: nan gradient found. The current loss is:  0.6420836448669434\n",
      "Warning: nan gradient found. The current loss is:  0.9301692843437195\n",
      "Warning: nan gradient found. The current loss is:  0.4710375964641571\n",
      "Warning: nan gradient found. The current loss is:  0.587186336517334\n",
      "Warning: nan gradient found. The current loss is:  0.4319707155227661\n",
      "Warning: nan gradient found. The current loss is:  0.6160637736320496\n",
      "Warning: nan gradient found. The current loss is:  0.26326829195022583\n",
      "Warning: nan gradient found. The current loss is:  0.5443846583366394\n",
      "Warning: nan gradient found. The current loss is:  0.5076245069503784\n",
      "Warning: nan gradient found. The current loss is:  0.4049098789691925\n",
      "Warning: nan gradient found. The current loss is:  0.6304721832275391\n",
      "Warning: nan gradient found. The current loss is:  0.5134339928627014\n",
      "Warning: nan gradient found. The current loss is:  2.2564446926116943\n",
      "Warning: nan gradient found. The current loss is:  0.838631272315979\n",
      "Warning: nan gradient found. The current loss is:  0.170222207903862\n",
      "Warning: nan gradient found. The current loss is:  0.83332359790802\n",
      "Warning: nan gradient found. The current loss is:  0.27228274941444397\n",
      "Warning: nan gradient found. The current loss is:  1.0426251888275146\n",
      "Warning: nan gradient found. The current loss is:  1.5361719131469727\n",
      "Warning: nan gradient found. The current loss is:  1.6544276475906372\n",
      "Warning: nan gradient found. The current loss is:  0.5920202136039734\n",
      "Warning: nan gradient found. The current loss is:  0.7111460566520691\n",
      "Warning: nan gradient found. The current loss is:  0.5504024624824524\n",
      "Warning: nan gradient found. The current loss is:  0.9645466804504395\n",
      "Warning: nan gradient found. The current loss is:  1.007190227508545\n",
      "Warning: nan gradient found. The current loss is:  0.5665897727012634\n",
      "Warning: nan gradient found. The current loss is:  0.21404992043972015\n",
      "Warning: nan gradient found. The current loss is:  0.6157807111740112\n",
      "Warning: nan gradient found. The current loss is:  1.0563603639602661\n",
      "Warning: nan gradient found. The current loss is:  0.9147490859031677\n",
      "Warning: nan gradient found. The current loss is:  1.615568995475769\n",
      "Warning: nan gradient found. The current loss is:  0.59505295753479\n",
      "Warning: nan gradient found. The current loss is:  0.9476922750473022\n",
      "Warning: nan gradient found. The current loss is:  0.5138945579528809\n",
      "Warning: nan gradient found. The current loss is:  0.7554314136505127\n",
      "Warning: nan gradient found. The current loss is:  1.2339357137680054\n",
      "Warning: nan gradient found. The current loss is:  0.7144669890403748\n",
      "Warning: nan gradient found. The current loss is:  0.7537748217582703\n",
      "Warning: nan gradient found. The current loss is:  0.707735538482666\n",
      "Warning: nan gradient found. The current loss is:  0.8170110583305359\n",
      "Warning: nan gradient found. The current loss is:  0.49562156200408936\n",
      "Warning: nan gradient found. The current loss is:  0.47351789474487305\n",
      "Warning: nan gradient found. The current loss is:  0.7623122334480286\n",
      "Warning: nan gradient found. The current loss is:  0.5961017608642578\n",
      "Warning: nan gradient found. The current loss is:  0.7071974873542786\n",
      "Warning: nan gradient found. The current loss is:  0.7529066801071167\n",
      "Warning: nan gradient found. The current loss is:  0.34633636474609375\n",
      "Warning: nan gradient found. The current loss is:  0.6668950319290161\n",
      "Warning: nan gradient found. The current loss is:  0.25868529081344604\n",
      "Warning: nan gradient found. The current loss is:  0.32732731103897095\n",
      "Warning: nan gradient found. The current loss is:  1.1325901746749878\n",
      "Warning: nan gradient found. The current loss is:  0.5425976514816284\n",
      "Warning: nan gradient found. The current loss is:  0.6458082795143127\n",
      "Warning: nan gradient found. The current loss is:  0.5058020353317261\n",
      "Warning: nan gradient found. The current loss is:  0.8053754568099976\n",
      "Warning: nan gradient found. The current loss is:  0.6260057687759399\n",
      "Warning: nan gradient found. The current loss is:  0.5710842609405518\n",
      "Warning: nan gradient found. The current loss is:  1.2525187730789185\n",
      "Warning: nan gradient found. The current loss is:  2.1893739700317383\n",
      "Warning: nan gradient found. The current loss is:  0.49874791502952576\n",
      "Warning: nan gradient found. The current loss is:  0.3740108013153076\n",
      "Warning: nan gradient found. The current loss is:  1.0396254062652588\n",
      "Warning: nan gradient found. The current loss is:  0.7091553211212158\n",
      "Warning: nan gradient found. The current loss is:  1.2844343185424805\n",
      "Warning: nan gradient found. The current loss is:  0.5574437975883484\n",
      "Warning: nan gradient found. The current loss is:  0.5106410980224609\n",
      "Warning: nan gradient found. The current loss is:  0.24526245892047882\n",
      "Warning: nan gradient found. The current loss is:  0.615828275680542\n",
      "Warning: nan gradient found. The current loss is:  0.7770869731903076\n",
      "Warning: nan gradient found. The current loss is:  0.7229040861129761\n",
      "Warning: nan gradient found. The current loss is:  0.5952576398849487\n",
      "Warning: nan gradient found. The current loss is:  0.258463054895401\n",
      "Warning: nan gradient found. The current loss is:  1.207054615020752\n",
      "Warning: nan gradient found. The current loss is:  1.186963438987732\n",
      "Warning: nan gradient found. The current loss is:  0.7517884969711304\n",
      "Warning: nan gradient found. The current loss is:  0.48450613021850586\n",
      "Warning: nan gradient found. The current loss is:  1.5315102338790894\n",
      "Warning: nan gradient found. The current loss is:  1.2110342979431152\n",
      "Warning: nan gradient found. The current loss is:  0.534695029258728\n",
      "Warning: nan gradient found. The current loss is:  0.497332364320755\n",
      "Warning: nan gradient found. The current loss is:  1.333080530166626\n",
      "Warning: nan gradient found. The current loss is:  1.9847391843795776\n",
      "Warning: nan gradient found. The current loss is:  0.8905242681503296\n",
      "Warning: nan gradient found. The current loss is:  0.3127879202365875\n",
      "Warning: nan gradient found. The current loss is:  0.36993730068206787\n",
      "Warning: nan gradient found. The current loss is:  0.25683557987213135\n",
      "Warning: nan gradient found. The current loss is:  0.2676447033882141\n",
      "Warning: nan gradient found. The current loss is:  0.5246809720993042\n",
      "Warning: nan gradient found. The current loss is:  1.4039642810821533\n",
      "Current batch training loss: 1.403964  [844800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5634997487068176\n",
      "Warning: nan gradient found. The current loss is:  0.3480122685432434\n",
      "Warning: nan gradient found. The current loss is:  0.21314802765846252\n",
      "Warning: nan gradient found. The current loss is:  0.25649306178092957\n",
      "Warning: nan gradient found. The current loss is:  0.6203696727752686\n",
      "Warning: nan gradient found. The current loss is:  0.025333132594823837\n",
      "Warning: nan gradient found. The current loss is:  0.7800008654594421\n",
      "Warning: nan gradient found. The current loss is:  0.7068431973457336\n",
      "Warning: nan gradient found. The current loss is:  0.5762752890586853\n",
      "Warning: nan gradient found. The current loss is:  0.7918360233306885\n",
      "Warning: nan gradient found. The current loss is:  0.8317278623580933\n",
      "Warning: nan gradient found. The current loss is:  0.5229140520095825\n",
      "Warning: nan gradient found. The current loss is:  0.9995598196983337\n",
      "Warning: nan gradient found. The current loss is:  0.5406755805015564\n",
      "Warning: nan gradient found. The current loss is:  1.1038599014282227\n",
      "Warning: nan gradient found. The current loss is:  0.27618759870529175\n",
      "Warning: nan gradient found. The current loss is:  0.8340044021606445\n",
      "Warning: nan gradient found. The current loss is:  0.9914084672927856\n",
      "Warning: nan gradient found. The current loss is:  0.2591922879219055\n",
      "Warning: nan gradient found. The current loss is:  1.0098087787628174\n",
      "Warning: nan gradient found. The current loss is:  0.3265085518360138\n",
      "Warning: nan gradient found. The current loss is:  0.4596082866191864\n",
      "Warning: nan gradient found. The current loss is:  0.6274533271789551\n",
      "Warning: nan gradient found. The current loss is:  1.3668711185455322\n",
      "Warning: nan gradient found. The current loss is:  0.562720775604248\n",
      "Warning: nan gradient found. The current loss is:  0.7015040516853333\n",
      "Warning: nan gradient found. The current loss is:  0.7533079385757446\n",
      "Warning: nan gradient found. The current loss is:  0.637792706489563\n",
      "Warning: nan gradient found. The current loss is:  0.17615951597690582\n",
      "Warning: nan gradient found. The current loss is:  2.1539790630340576\n",
      "Warning: nan gradient found. The current loss is:  0.7335737943649292\n",
      "Warning: nan gradient found. The current loss is:  0.6302809715270996\n",
      "Warning: nan gradient found. The current loss is:  0.8135174512863159\n",
      "Warning: nan gradient found. The current loss is:  0.10940515995025635\n",
      "Warning: nan gradient found. The current loss is:  0.7171215415000916\n",
      "Warning: nan gradient found. The current loss is:  0.7789286375045776\n",
      "Warning: nan gradient found. The current loss is:  1.1996171474456787\n",
      "Warning: nan gradient found. The current loss is:  0.6742426156997681\n",
      "Warning: nan gradient found. The current loss is:  0.6279911994934082\n",
      "Warning: nan gradient found. The current loss is:  0.9078177213668823\n",
      "Warning: nan gradient found. The current loss is:  0.7624819278717041\n",
      "Warning: nan gradient found. The current loss is:  0.5446935892105103\n",
      "Warning: nan gradient found. The current loss is:  0.2516121566295624\n",
      "Warning: nan gradient found. The current loss is:  0.7628583312034607\n",
      "Warning: nan gradient found. The current loss is:  0.3668018579483032\n",
      "Warning: nan gradient found. The current loss is:  0.49475663900375366\n",
      "Warning: nan gradient found. The current loss is:  0.37342050671577454\n",
      "Warning: nan gradient found. The current loss is:  0.777318000793457\n",
      "Warning: nan gradient found. The current loss is:  0.06938633322715759\n",
      "Warning: nan gradient found. The current loss is:  0.6264457702636719\n",
      "Warning: nan gradient found. The current loss is:  0.38590288162231445\n",
      "Warning: nan gradient found. The current loss is:  1.2571611404418945\n",
      "Warning: nan gradient found. The current loss is:  0.5193231701850891\n",
      "Warning: nan gradient found. The current loss is:  0.982172966003418\n",
      "Warning: nan gradient found. The current loss is:  0.696406900882721\n",
      "Warning: nan gradient found. The current loss is:  0.17859405279159546\n",
      "Warning: nan gradient found. The current loss is:  0.6921789050102234\n",
      "Warning: nan gradient found. The current loss is:  1.455810546875\n",
      "Warning: nan gradient found. The current loss is:  0.5843405723571777\n",
      "Warning: nan gradient found. The current loss is:  0.6162094473838806\n",
      "Warning: nan gradient found. The current loss is:  1.462700605392456\n",
      "Warning: nan gradient found. The current loss is:  0.9294866323471069\n",
      "Warning: nan gradient found. The current loss is:  0.2698900103569031\n",
      "Warning: nan gradient found. The current loss is:  1.0664639472961426\n",
      "Warning: nan gradient found. The current loss is:  0.3596994876861572\n",
      "Warning: nan gradient found. The current loss is:  0.10336947441101074\n",
      "Warning: nan gradient found. The current loss is:  0.2855224907398224\n",
      "Warning: nan gradient found. The current loss is:  0.4617447853088379\n",
      "Warning: nan gradient found. The current loss is:  1.2429507970809937\n",
      "Warning: nan gradient found. The current loss is:  0.638120174407959\n",
      "Warning: nan gradient found. The current loss is:  1.3125510215759277\n",
      "Warning: nan gradient found. The current loss is:  0.3192301094532013\n",
      "Warning: nan gradient found. The current loss is:  0.6060262322425842\n",
      "Warning: nan gradient found. The current loss is:  0.39705485105514526\n",
      "Warning: nan gradient found. The current loss is:  1.1722118854522705\n",
      "Warning: nan gradient found. The current loss is:  0.9040119647979736\n",
      "Warning: nan gradient found. The current loss is:  0.6353874206542969\n",
      "Warning: nan gradient found. The current loss is:  1.9188131093978882\n",
      "Warning: nan gradient found. The current loss is:  1.4864838123321533\n",
      "Warning: nan gradient found. The current loss is:  1.361327886581421\n",
      "Warning: nan gradient found. The current loss is:  1.9619354009628296\n",
      "Warning: nan gradient found. The current loss is:  0.5643375515937805\n",
      "Warning: nan gradient found. The current loss is:  0.9836776852607727\n",
      "Warning: nan gradient found. The current loss is:  0.501928985118866\n",
      "Warning: nan gradient found. The current loss is:  0.6137834787368774\n",
      "Warning: nan gradient found. The current loss is:  0.848892092704773\n",
      "Warning: nan gradient found. The current loss is:  0.33535268902778625\n",
      "Warning: nan gradient found. The current loss is:  1.8632618188858032\n",
      "Warning: nan gradient found. The current loss is:  1.551997423171997\n",
      "Warning: nan gradient found. The current loss is:  0.6977267265319824\n",
      "Warning: nan gradient found. The current loss is:  0.6456625461578369\n",
      "Warning: nan gradient found. The current loss is:  0.33018651604652405\n",
      "Warning: nan gradient found. The current loss is:  0.7090554237365723\n",
      "Warning: nan gradient found. The current loss is:  0.6993441581726074\n",
      "Warning: nan gradient found. The current loss is:  2.004288673400879\n",
      "Warning: nan gradient found. The current loss is:  0.6021472215652466\n",
      "Warning: nan gradient found. The current loss is:  0.5589597821235657\n",
      "Warning: nan gradient found. The current loss is:  0.4846091568470001\n",
      "Warning: nan gradient found. The current loss is:  0.9555732011795044\n",
      "Warning: nan gradient found. The current loss is:  0.25741294026374817\n",
      "Current batch training loss: 0.257413  [870400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5483500957489014\n",
      "Warning: nan gradient found. The current loss is:  0.6306053400039673\n",
      "Warning: nan gradient found. The current loss is:  0.44794759154319763\n",
      "Warning: nan gradient found. The current loss is:  1.7436656951904297\n",
      "Warning: nan gradient found. The current loss is:  0.6848858594894409\n",
      "Warning: nan gradient found. The current loss is:  0.8114362359046936\n",
      "Warning: nan gradient found. The current loss is:  0.6695277690887451\n",
      "Warning: nan gradient found. The current loss is:  0.6569336652755737\n",
      "Warning: nan gradient found. The current loss is:  0.049077924340963364\n",
      "Warning: nan gradient found. The current loss is:  0.7499269843101501\n",
      "Warning: nan gradient found. The current loss is:  0.502269983291626\n",
      "Warning: nan gradient found. The current loss is:  1.125523567199707\n",
      "Warning: nan gradient found. The current loss is:  0.47955793142318726\n",
      "Warning: nan gradient found. The current loss is:  0.7845141887664795\n",
      "Warning: nan gradient found. The current loss is:  0.5299296975135803\n",
      "Warning: nan gradient found. The current loss is:  0.35001620650291443\n",
      "Warning: nan gradient found. The current loss is:  0.2804061472415924\n",
      "Warning: nan gradient found. The current loss is:  0.3251107633113861\n",
      "Warning: nan gradient found. The current loss is:  0.2092386931180954\n",
      "Warning: nan gradient found. The current loss is:  0.5350439548492432\n",
      "Warning: nan gradient found. The current loss is:  0.7003815174102783\n",
      "Warning: nan gradient found. The current loss is:  0.5016419291496277\n",
      "Warning: nan gradient found. The current loss is:  1.0949963331222534\n",
      "Warning: nan gradient found. The current loss is:  1.2099347114562988\n",
      "Warning: nan gradient found. The current loss is:  0.3201580047607422\n",
      "Warning: nan gradient found. The current loss is:  1.2478086948394775\n",
      "Warning: nan gradient found. The current loss is:  0.8389941453933716\n",
      "Warning: nan gradient found. The current loss is:  0.6372941732406616\n",
      "Warning: nan gradient found. The current loss is:  0.8527601957321167\n",
      "Warning: nan gradient found. The current loss is:  1.3038692474365234\n",
      "Warning: nan gradient found. The current loss is:  0.439494788646698\n",
      "Warning: nan gradient found. The current loss is:  0.7530204653739929\n",
      "Warning: nan gradient found. The current loss is:  0.6670770049095154\n",
      "Warning: nan gradient found. The current loss is:  0.8482375741004944\n",
      "Warning: nan gradient found. The current loss is:  0.5445720553398132\n",
      "Warning: nan gradient found. The current loss is:  0.7406930327415466\n",
      "Warning: nan gradient found. The current loss is:  0.9332800507545471\n",
      "Warning: nan gradient found. The current loss is:  0.28338423371315\n",
      "Warning: nan gradient found. The current loss is:  0.2622257173061371\n",
      "Warning: nan gradient found. The current loss is:  0.4453142583370209\n",
      "Warning: nan gradient found. The current loss is:  0.8359329104423523\n",
      "Warning: nan gradient found. The current loss is:  0.748811662197113\n",
      "Warning: nan gradient found. The current loss is:  1.0061826705932617\n",
      "Warning: nan gradient found. The current loss is:  0.6149808764457703\n",
      "Warning: nan gradient found. The current loss is:  0.28426653146743774\n",
      "Warning: nan gradient found. The current loss is:  1.1120002269744873\n",
      "Warning: nan gradient found. The current loss is:  0.4621589481830597\n",
      "Warning: nan gradient found. The current loss is:  0.9934367537498474\n",
      "Warning: nan gradient found. The current loss is:  1.144939661026001\n",
      "Warning: nan gradient found. The current loss is:  1.344204068183899\n",
      "Warning: nan gradient found. The current loss is:  0.11036457121372223\n",
      "Warning: nan gradient found. The current loss is:  1.8322144746780396\n",
      "Warning: nan gradient found. The current loss is:  0.512090265750885\n",
      "Warning: nan gradient found. The current loss is:  0.16892941296100616\n",
      "Warning: nan gradient found. The current loss is:  0.7360162138938904\n",
      "Warning: nan gradient found. The current loss is:  0.5665673017501831\n",
      "Warning: nan gradient found. The current loss is:  2.1119189262390137\n",
      "Warning: nan gradient found. The current loss is:  0.7437915802001953\n",
      "Warning: nan gradient found. The current loss is:  0.8364075422286987\n",
      "Warning: nan gradient found. The current loss is:  1.066726803779602\n",
      "Warning: nan gradient found. The current loss is:  0.646659255027771\n",
      "Warning: nan gradient found. The current loss is:  1.0975594520568848\n",
      "Warning: nan gradient found. The current loss is:  1.5083690881729126\n",
      "Warning: nan gradient found. The current loss is:  0.7398588061332703\n",
      "Warning: nan gradient found. The current loss is:  0.5694495439529419\n",
      "Warning: nan gradient found. The current loss is:  0.06369929015636444\n",
      "Warning: nan gradient found. The current loss is:  0.5639265179634094\n",
      "Warning: nan gradient found. The current loss is:  1.3816865682601929\n",
      "Warning: nan gradient found. The current loss is:  0.2000233680009842\n",
      "Warning: nan gradient found. The current loss is:  0.4704182744026184\n",
      "Warning: nan gradient found. The current loss is:  0.6920313835144043\n",
      "Warning: nan gradient found. The current loss is:  0.6024194955825806\n",
      "Warning: nan gradient found. The current loss is:  0.8212662935256958\n",
      "Warning: nan gradient found. The current loss is:  0.9889163970947266\n",
      "Warning: nan gradient found. The current loss is:  0.5215354561805725\n",
      "Warning: nan gradient found. The current loss is:  0.5710043907165527\n",
      "Warning: nan gradient found. The current loss is:  0.8486907482147217\n",
      "Warning: nan gradient found. The current loss is:  0.35667818784713745\n",
      "Warning: nan gradient found. The current loss is:  0.460382342338562\n",
      "Warning: nan gradient found. The current loss is:  1.08504056930542\n",
      "Warning: nan gradient found. The current loss is:  0.9887325763702393\n",
      "Warning: nan gradient found. The current loss is:  1.2709591388702393\n",
      "Warning: nan gradient found. The current loss is:  0.09197300672531128\n",
      "Warning: nan gradient found. The current loss is:  0.569576621055603\n",
      "Warning: nan gradient found. The current loss is:  0.5509411096572876\n",
      "Warning: nan gradient found. The current loss is:  1.2951754331588745\n",
      "Warning: nan gradient found. The current loss is:  0.6881914138793945\n",
      "Warning: nan gradient found. The current loss is:  0.21720007061958313\n",
      "Warning: nan gradient found. The current loss is:  0.8627848625183105\n",
      "Warning: nan gradient found. The current loss is:  0.4838208556175232\n",
      "Warning: nan gradient found. The current loss is:  0.4042820930480957\n",
      "Warning: nan gradient found. The current loss is:  0.7429767847061157\n",
      "Warning: nan gradient found. The current loss is:  2.8860554695129395\n",
      "Warning: nan gradient found. The current loss is:  0.38381460309028625\n",
      "Warning: nan gradient found. The current loss is:  0.7837080359458923\n",
      "Warning: nan gradient found. The current loss is:  0.6361989378929138\n",
      "Warning: nan gradient found. The current loss is:  0.5226721167564392\n",
      "Warning: nan gradient found. The current loss is:  0.5211877226829529\n",
      "Warning: nan gradient found. The current loss is:  0.680172860622406\n",
      "Warning: nan gradient found. The current loss is:  0.803095817565918\n",
      "Current batch training loss: 0.803096  [896000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7786589860916138\n",
      "Warning: nan gradient found. The current loss is:  1.2414261102676392\n",
      "Warning: nan gradient found. The current loss is:  0.8183159828186035\n",
      "Warning: nan gradient found. The current loss is:  0.5340218544006348\n",
      "Warning: nan gradient found. The current loss is:  0.6964944005012512\n",
      "Warning: nan gradient found. The current loss is:  0.9489116072654724\n",
      "Warning: nan gradient found. The current loss is:  0.49895721673965454\n",
      "Warning: nan gradient found. The current loss is:  1.2818498611450195\n",
      "Warning: nan gradient found. The current loss is:  0.1451098918914795\n",
      "Warning: nan gradient found. The current loss is:  0.744584321975708\n",
      "Warning: nan gradient found. The current loss is:  0.574682354927063\n",
      "Warning: nan gradient found. The current loss is:  0.2512598931789398\n",
      "Warning: nan gradient found. The current loss is:  0.3492651581764221\n",
      "Warning: nan gradient found. The current loss is:  1.1829006671905518\n",
      "Warning: nan gradient found. The current loss is:  0.7468280792236328\n",
      "Warning: nan gradient found. The current loss is:  0.26470035314559937\n",
      "Warning: nan gradient found. The current loss is:  1.2410181760787964\n",
      "Warning: nan gradient found. The current loss is:  0.4484853446483612\n",
      "Warning: nan gradient found. The current loss is:  0.41501012444496155\n",
      "Warning: nan gradient found. The current loss is:  0.871742844581604\n",
      "Warning: nan gradient found. The current loss is:  0.2512674331665039\n",
      "Warning: nan gradient found. The current loss is:  0.49118316173553467\n",
      "Warning: nan gradient found. The current loss is:  0.4969419836997986\n",
      "Warning: nan gradient found. The current loss is:  0.42127251625061035\n",
      "Warning: nan gradient found. The current loss is:  0.5777662992477417\n",
      "Warning: nan gradient found. The current loss is:  0.6937623023986816\n",
      "Warning: nan gradient found. The current loss is:  0.8039662837982178\n",
      "Warning: nan gradient found. The current loss is:  1.051073670387268\n",
      "Warning: nan gradient found. The current loss is:  1.145167350769043\n",
      "Warning: nan gradient found. The current loss is:  1.260563850402832\n",
      "Warning: nan gradient found. The current loss is:  0.47830986976623535\n",
      "Warning: nan gradient found. The current loss is:  0.3238253891468048\n",
      "Warning: nan gradient found. The current loss is:  0.5847322940826416\n",
      "Warning: nan gradient found. The current loss is:  0.98614501953125\n",
      "Warning: nan gradient found. The current loss is:  0.09149375557899475\n",
      "Warning: nan gradient found. The current loss is:  0.8203386068344116\n",
      "Warning: nan gradient found. The current loss is:  0.6698237657546997\n",
      "Warning: nan gradient found. The current loss is:  0.7938728332519531\n",
      "Warning: nan gradient found. The current loss is:  0.3016952872276306\n",
      "Warning: nan gradient found. The current loss is:  0.42846110463142395\n",
      "Warning: nan gradient found. The current loss is:  1.0718351602554321\n",
      "Warning: nan gradient found. The current loss is:  1.3306868076324463\n",
      "Warning: nan gradient found. The current loss is:  2.0352442264556885\n",
      "Warning: nan gradient found. The current loss is:  1.048107385635376\n",
      "Warning: nan gradient found. The current loss is:  0.9395089149475098\n",
      "Warning: nan gradient found. The current loss is:  1.2859809398651123\n",
      "Warning: nan gradient found. The current loss is:  0.8075749278068542\n",
      "Warning: nan gradient found. The current loss is:  0.35654300451278687\n",
      "Warning: nan gradient found. The current loss is:  0.6017142534255981\n",
      "Warning: nan gradient found. The current loss is:  0.9395723342895508\n",
      "Warning: nan gradient found. The current loss is:  3.765195369720459\n",
      "Warning: nan gradient found. The current loss is:  0.7898826599121094\n",
      "Warning: nan gradient found. The current loss is:  1.384497046470642\n",
      "Warning: nan gradient found. The current loss is:  0.270336389541626\n",
      "Warning: nan gradient found. The current loss is:  1.042901873588562\n",
      "Warning: nan gradient found. The current loss is:  0.5868004560470581\n",
      "Warning: nan gradient found. The current loss is:  0.5833348035812378\n",
      "Warning: nan gradient found. The current loss is:  2.7428526878356934\n",
      "Warning: nan gradient found. The current loss is:  1.20662522315979\n",
      "Warning: nan gradient found. The current loss is:  0.709015965461731\n",
      "Warning: nan gradient found. The current loss is:  0.48031085729599\n",
      "Warning: nan gradient found. The current loss is:  0.8979023098945618\n",
      "Warning: nan gradient found. The current loss is:  1.202775001525879\n",
      "Warning: nan gradient found. The current loss is:  0.801667332649231\n",
      "Warning: nan gradient found. The current loss is:  0.18343743681907654\n",
      "Warning: nan gradient found. The current loss is:  0.35148799419403076\n",
      "Warning: nan gradient found. The current loss is:  0.49079060554504395\n",
      "Warning: nan gradient found. The current loss is:  0.3002072274684906\n",
      "Warning: nan gradient found. The current loss is:  0.054734937846660614\n",
      "Warning: nan gradient found. The current loss is:  1.2795612812042236\n",
      "Warning: nan gradient found. The current loss is:  0.6735384464263916\n",
      "Warning: nan gradient found. The current loss is:  0.2706981599330902\n",
      "Warning: nan gradient found. The current loss is:  0.26262709498405457\n",
      "Warning: nan gradient found. The current loss is:  0.6042926907539368\n",
      "Warning: nan gradient found. The current loss is:  0.13217619061470032\n",
      "Warning: nan gradient found. The current loss is:  0.40936776995658875\n",
      "Warning: nan gradient found. The current loss is:  0.9991751909255981\n",
      "Warning: nan gradient found. The current loss is:  0.5344380140304565\n",
      "Warning: nan gradient found. The current loss is:  0.5727678537368774\n",
      "Warning: nan gradient found. The current loss is:  0.4862768054008484\n",
      "Warning: nan gradient found. The current loss is:  0.5452426671981812\n",
      "Warning: nan gradient found. The current loss is:  0.5570639371871948\n",
      "Warning: nan gradient found. The current loss is:  0.9085601568222046\n",
      "Warning: nan gradient found. The current loss is:  0.5765713453292847\n",
      "Warning: nan gradient found. The current loss is:  0.28398382663726807\n",
      "Warning: nan gradient found. The current loss is:  0.5932843685150146\n",
      "Warning: nan gradient found. The current loss is:  1.1336005926132202\n",
      "Warning: nan gradient found. The current loss is:  0.37306544184684753\n",
      "Warning: nan gradient found. The current loss is:  0.3218578100204468\n",
      "Warning: nan gradient found. The current loss is:  0.8064570426940918\n",
      "Warning: nan gradient found. The current loss is:  0.4928446114063263\n",
      "Warning: nan gradient found. The current loss is:  0.679776668548584\n",
      "Warning: nan gradient found. The current loss is:  0.7955784797668457\n",
      "Warning: nan gradient found. The current loss is:  1.6090058088302612\n",
      "Warning: nan gradient found. The current loss is:  0.3625970482826233\n",
      "Warning: nan gradient found. The current loss is:  0.4598039388656616\n",
      "Warning: nan gradient found. The current loss is:  0.48940253257751465\n",
      "Warning: nan gradient found. The current loss is:  0.494419127702713\n",
      "Warning: nan gradient found. The current loss is:  0.49701857566833496\n",
      "Warning: nan gradient found. The current loss is:  0.6404985785484314\n",
      "Current batch training loss: 0.640499  [921600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6161566972732544\n",
      "Warning: nan gradient found. The current loss is:  0.12719756364822388\n",
      "Warning: nan gradient found. The current loss is:  0.5739867091178894\n",
      "Warning: nan gradient found. The current loss is:  0.8340097665786743\n",
      "Warning: nan gradient found. The current loss is:  0.40540921688079834\n",
      "Warning: nan gradient found. The current loss is:  0.5091620683670044\n",
      "Warning: nan gradient found. The current loss is:  0.5018496513366699\n",
      "Warning: nan gradient found. The current loss is:  0.5378726124763489\n",
      "Warning: nan gradient found. The current loss is:  0.24439340829849243\n",
      "Warning: nan gradient found. The current loss is:  0.66530442237854\n",
      "Warning: nan gradient found. The current loss is:  1.2696139812469482\n",
      "Warning: nan gradient found. The current loss is:  0.7637708187103271\n",
      "Warning: nan gradient found. The current loss is:  1.3379186391830444\n",
      "Warning: nan gradient found. The current loss is:  0.7827882170677185\n",
      "Warning: nan gradient found. The current loss is:  0.4512976408004761\n",
      "Warning: nan gradient found. The current loss is:  1.0315635204315186\n",
      "Warning: nan gradient found. The current loss is:  0.3109298348426819\n",
      "Warning: nan gradient found. The current loss is:  0.7241193652153015\n",
      "Warning: nan gradient found. The current loss is:  1.2652395963668823\n",
      "Warning: nan gradient found. The current loss is:  0.49513375759124756\n",
      "Warning: nan gradient found. The current loss is:  0.6165768504142761\n",
      "Warning: nan gradient found. The current loss is:  0.4626697599887848\n",
      "Warning: nan gradient found. The current loss is:  0.49132856726646423\n",
      "Warning: nan gradient found. The current loss is:  0.45341336727142334\n",
      "Warning: nan gradient found. The current loss is:  0.8858819603919983\n",
      "Warning: nan gradient found. The current loss is:  1.3085482120513916\n",
      "Warning: nan gradient found. The current loss is:  0.366629958152771\n",
      "Warning: nan gradient found. The current loss is:  0.5578524470329285\n",
      "Warning: nan gradient found. The current loss is:  0.5389531850814819\n",
      "Warning: nan gradient found. The current loss is:  1.0240709781646729\n",
      "Warning: nan gradient found. The current loss is:  0.32388925552368164\n",
      "Warning: nan gradient found. The current loss is:  0.5889933109283447\n",
      "Warning: nan gradient found. The current loss is:  -0.04374474287033081\n",
      "Warning: nan gradient found. The current loss is:  0.13942773640155792\n",
      "Warning: nan gradient found. The current loss is:  0.9347923994064331\n",
      "Warning: nan gradient found. The current loss is:  -0.019493982195854187\n",
      "Warning: nan gradient found. The current loss is:  0.6943227648735046\n",
      "Warning: nan gradient found. The current loss is:  0.8631773591041565\n",
      "Warning: nan gradient found. The current loss is:  0.3940362334251404\n",
      "Warning: nan gradient found. The current loss is:  1.3514227867126465\n",
      "Warning: nan gradient found. The current loss is:  1.1759483814239502\n",
      "Warning: nan gradient found. The current loss is:  0.49231064319610596\n",
      "Warning: nan gradient found. The current loss is:  1.000650405883789\n",
      "Warning: nan gradient found. The current loss is:  0.4486786127090454\n",
      "Warning: nan gradient found. The current loss is:  0.5775594711303711\n",
      "Warning: nan gradient found. The current loss is:  0.7768442630767822\n",
      "Warning: nan gradient found. The current loss is:  0.6575294733047485\n",
      "Warning: nan gradient found. The current loss is:  0.35135015845298767\n",
      "Warning: nan gradient found. The current loss is:  1.3255903720855713\n",
      "Warning: nan gradient found. The current loss is:  0.8753484487533569\n",
      "Warning: nan gradient found. The current loss is:  1.3032225370407104\n",
      "Warning: nan gradient found. The current loss is:  0.6644322872161865\n",
      "Warning: nan gradient found. The current loss is:  2.8925607204437256\n",
      "Warning: nan gradient found. The current loss is:  0.5097241997718811\n",
      "Warning: nan gradient found. The current loss is:  0.7061509490013123\n",
      "Warning: nan gradient found. The current loss is:  3.3178205490112305\n",
      "Warning: nan gradient found. The current loss is:  0.4894807040691376\n",
      "Warning: nan gradient found. The current loss is:  0.514376699924469\n",
      "Warning: nan gradient found. The current loss is:  0.7433878183364868\n",
      "Warning: nan gradient found. The current loss is:  0.6627600193023682\n",
      "Warning: nan gradient found. The current loss is:  0.7286373972892761\n",
      "Warning: nan gradient found. The current loss is:  0.4042464792728424\n",
      "Warning: nan gradient found. The current loss is:  0.471225380897522\n",
      "Warning: nan gradient found. The current loss is:  0.9349286556243896\n",
      "Warning: nan gradient found. The current loss is:  0.5789997577667236\n",
      "Warning: nan gradient found. The current loss is:  0.254520058631897\n",
      "Warning: nan gradient found. The current loss is:  2.601405143737793\n",
      "Warning: nan gradient found. The current loss is:  0.7893224954605103\n",
      "Warning: nan gradient found. The current loss is:  0.8541121482849121\n",
      "Warning: nan gradient found. The current loss is:  0.31496021151542664\n",
      "Warning: nan gradient found. The current loss is:  0.44121527671813965\n",
      "Warning: nan gradient found. The current loss is:  1.7708109617233276\n",
      "Warning: nan gradient found. The current loss is:  0.6586040258407593\n",
      "Warning: nan gradient found. The current loss is:  0.6885722875595093\n",
      "Warning: nan gradient found. The current loss is:  0.38584601879119873\n",
      "Warning: nan gradient found. The current loss is:  1.086904525756836\n",
      "Warning: nan gradient found. The current loss is:  0.9608544707298279\n",
      "Warning: nan gradient found. The current loss is:  0.5774257183074951\n",
      "Warning: nan gradient found. The current loss is:  0.5588415861129761\n",
      "Warning: nan gradient found. The current loss is:  1.141005516052246\n",
      "Warning: nan gradient found. The current loss is:  0.3750421404838562\n",
      "Warning: nan gradient found. The current loss is:  0.525329053401947\n",
      "Warning: nan gradient found. The current loss is:  1.0459004640579224\n",
      "Warning: nan gradient found. The current loss is:  0.5645402669906616\n",
      "Warning: nan gradient found. The current loss is:  0.7576850652694702\n",
      "Warning: nan gradient found. The current loss is:  1.4727516174316406\n",
      "Warning: nan gradient found. The current loss is:  0.2509794533252716\n",
      "Warning: nan gradient found. The current loss is:  0.813164472579956\n",
      "Warning: nan gradient found. The current loss is:  0.6238605976104736\n",
      "Warning: nan gradient found. The current loss is:  0.32521504163742065\n",
      "Warning: nan gradient found. The current loss is:  1.1931023597717285\n",
      "Warning: nan gradient found. The current loss is:  0.6417217254638672\n",
      "Warning: nan gradient found. The current loss is:  0.6633539795875549\n",
      "Warning: nan gradient found. The current loss is:  0.39913737773895264\n",
      "Warning: nan gradient found. The current loss is:  0.5599793195724487\n",
      "Warning: nan gradient found. The current loss is:  0.4013199210166931\n",
      "Warning: nan gradient found. The current loss is:  0.4744316339492798\n",
      "Warning: nan gradient found. The current loss is:  0.49430975317955017\n",
      "Warning: nan gradient found. The current loss is:  0.40494388341903687\n",
      "Warning: nan gradient found. The current loss is:  0.7703092098236084\n",
      "Current batch training loss: 0.770309  [947200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.23863130807876587\n",
      "Warning: nan gradient found. The current loss is:  0.8543481826782227\n",
      "Warning: nan gradient found. The current loss is:  0.4496314525604248\n",
      "Warning: nan gradient found. The current loss is:  0.099729523062706\n",
      "Warning: nan gradient found. The current loss is:  0.2523883879184723\n",
      "Warning: nan gradient found. The current loss is:  0.5577760934829712\n",
      "Warning: nan gradient found. The current loss is:  0.7100366950035095\n",
      "Warning: nan gradient found. The current loss is:  1.0510432720184326\n",
      "Warning: nan gradient found. The current loss is:  0.2606440484523773\n",
      "Warning: nan gradient found. The current loss is:  0.6055493354797363\n",
      "Warning: nan gradient found. The current loss is:  1.5290827751159668\n",
      "Warning: nan gradient found. The current loss is:  0.435398668050766\n",
      "Warning: nan gradient found. The current loss is:  0.4583791494369507\n",
      "Warning: nan gradient found. The current loss is:  0.4645141661167145\n",
      "Warning: nan gradient found. The current loss is:  0.7043075561523438\n",
      "Warning: nan gradient found. The current loss is:  1.244573712348938\n",
      "Warning: nan gradient found. The current loss is:  0.5305314064025879\n",
      "Warning: nan gradient found. The current loss is:  1.8094890117645264\n",
      "Warning: nan gradient found. The current loss is:  0.7999393939971924\n",
      "Warning: nan gradient found. The current loss is:  0.7056877613067627\n",
      "Warning: nan gradient found. The current loss is:  0.7753974199295044\n",
      "Warning: nan gradient found. The current loss is:  0.3555452823638916\n",
      "Warning: nan gradient found. The current loss is:  0.8944162726402283\n",
      "Warning: nan gradient found. The current loss is:  0.14960318803787231\n",
      "Warning: nan gradient found. The current loss is:  0.7842932939529419\n",
      "Warning: nan gradient found. The current loss is:  0.5661393404006958\n",
      "Warning: nan gradient found. The current loss is:  0.6492743492126465\n",
      "Warning: nan gradient found. The current loss is:  0.2143832892179489\n",
      "Warning: nan gradient found. The current loss is:  0.7457389831542969\n",
      "Warning: nan gradient found. The current loss is:  0.792855978012085\n",
      "Warning: nan gradient found. The current loss is:  0.2545347213745117\n",
      "Warning: nan gradient found. The current loss is:  0.2546358108520508\n",
      "Warning: nan gradient found. The current loss is:  0.3586389124393463\n",
      "Warning: nan gradient found. The current loss is:  0.5868240594863892\n",
      "Warning: nan gradient found. The current loss is:  0.865176796913147\n",
      "Warning: nan gradient found. The current loss is:  0.11444962024688721\n",
      "Warning: nan gradient found. The current loss is:  0.5226255655288696\n",
      "Warning: nan gradient found. The current loss is:  0.6231722235679626\n",
      "Warning: nan gradient found. The current loss is:  0.49814242124557495\n",
      "Warning: nan gradient found. The current loss is:  0.6897207498550415\n",
      "Warning: nan gradient found. The current loss is:  0.5331552624702454\n",
      "Warning: nan gradient found. The current loss is:  0.16286516189575195\n",
      "Warning: nan gradient found. The current loss is:  0.4046310782432556\n",
      "Warning: nan gradient found. The current loss is:  0.7203286290168762\n",
      "Warning: nan gradient found. The current loss is:  0.41234615445137024\n",
      "Warning: nan gradient found. The current loss is:  0.9516164064407349\n",
      "Warning: nan gradient found. The current loss is:  0.19709214568138123\n",
      "Warning: nan gradient found. The current loss is:  0.7145333290100098\n",
      "Warning: nan gradient found. The current loss is:  1.5080702304840088\n",
      "Warning: nan gradient found. The current loss is:  0.9258565902709961\n",
      "Warning: nan gradient found. The current loss is:  0.8430465459823608\n",
      "Warning: nan gradient found. The current loss is:  0.195401132106781\n",
      "Warning: nan gradient found. The current loss is:  0.6089903116226196\n",
      "Warning: nan gradient found. The current loss is:  0.5832335352897644\n",
      "Warning: nan gradient found. The current loss is:  0.33589139580726624\n",
      "Warning: nan gradient found. The current loss is:  0.37938398122787476\n",
      "Warning: nan gradient found. The current loss is:  0.6095196008682251\n",
      "Warning: nan gradient found. The current loss is:  0.8485107421875\n",
      "Warning: nan gradient found. The current loss is:  0.44574981927871704\n",
      "Warning: nan gradient found. The current loss is:  0.4251357913017273\n",
      "Warning: nan gradient found. The current loss is:  0.36734890937805176\n",
      "Warning: nan gradient found. The current loss is:  1.1767868995666504\n",
      "Warning: nan gradient found. The current loss is:  0.3652278780937195\n",
      "Warning: nan gradient found. The current loss is:  0.5170570611953735\n",
      "Warning: nan gradient found. The current loss is:  0.5938852429389954\n",
      "Warning: nan gradient found. The current loss is:  0.40308600664138794\n",
      "Warning: nan gradient found. The current loss is:  0.28134939074516296\n",
      "Warning: nan gradient found. The current loss is:  1.467111349105835\n",
      "Warning: nan gradient found. The current loss is:  0.43817266821861267\n",
      "Warning: nan gradient found. The current loss is:  0.3604802191257477\n",
      "Warning: nan gradient found. The current loss is:  0.5613542199134827\n",
      "Warning: nan gradient found. The current loss is:  0.4879680573940277\n",
      "Warning: nan gradient found. The current loss is:  1.1292940378189087\n",
      "Warning: nan gradient found. The current loss is:  1.0379477739334106\n",
      "Warning: nan gradient found. The current loss is:  1.1107497215270996\n",
      "Warning: nan gradient found. The current loss is:  1.319412112236023\n",
      "Warning: nan gradient found. The current loss is:  1.3448569774627686\n",
      "Warning: nan gradient found. The current loss is:  0.9184980988502502\n",
      "Warning: nan gradient found. The current loss is:  0.6754782199859619\n",
      "Warning: nan gradient found. The current loss is:  1.1318737268447876\n",
      "Warning: nan gradient found. The current loss is:  0.20048528909683228\n",
      "Warning: nan gradient found. The current loss is:  1.8389078378677368\n",
      "Warning: nan gradient found. The current loss is:  0.816013753414154\n",
      "Warning: nan gradient found. The current loss is:  0.5394177436828613\n",
      "Warning: nan gradient found. The current loss is:  0.43678349256515503\n",
      "Warning: nan gradient found. The current loss is:  0.9280643463134766\n",
      "Warning: nan gradient found. The current loss is:  0.4618763327598572\n",
      "Warning: nan gradient found. The current loss is:  1.110416293144226\n",
      "Warning: nan gradient found. The current loss is:  0.9719632863998413\n",
      "Warning: nan gradient found. The current loss is:  0.4356684684753418\n",
      "Warning: nan gradient found. The current loss is:  0.9125041961669922\n",
      "Warning: nan gradient found. The current loss is:  0.5884807109832764\n",
      "Warning: nan gradient found. The current loss is:  0.6049684286117554\n",
      "Warning: nan gradient found. The current loss is:  0.6633241176605225\n",
      "Warning: nan gradient found. The current loss is:  1.201147198677063\n",
      "Warning: nan gradient found. The current loss is:  0.2350098192691803\n",
      "Warning: nan gradient found. The current loss is:  0.8990138173103333\n",
      "Warning: nan gradient found. The current loss is:  0.21477110683918\n",
      "Warning: nan gradient found. The current loss is:  0.8941918015480042\n",
      "Warning: nan gradient found. The current loss is:  1.6483534574508667\n",
      "Current batch training loss: 1.648353  [972800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5935274362564087\n",
      "Warning: nan gradient found. The current loss is:  1.1336439847946167\n",
      "Warning: nan gradient found. The current loss is:  1.0088825225830078\n",
      "Warning: nan gradient found. The current loss is:  0.21687856316566467\n",
      "Warning: nan gradient found. The current loss is:  0.39593589305877686\n",
      "Warning: nan gradient found. The current loss is:  0.4384680688381195\n",
      "Warning: nan gradient found. The current loss is:  1.1222797632217407\n",
      "Warning: nan gradient found. The current loss is:  0.8541694283485413\n",
      "Warning: nan gradient found. The current loss is:  0.520025372505188\n",
      "Warning: nan gradient found. The current loss is:  0.29867562651634216\n",
      "Warning: nan gradient found. The current loss is:  0.9202538728713989\n",
      "Warning: nan gradient found. The current loss is:  0.6200821995735168\n",
      "Warning: nan gradient found. The current loss is:  0.7096558809280396\n",
      "Warning: nan gradient found. The current loss is:  0.6063326001167297\n",
      "Warning: nan gradient found. The current loss is:  0.8097803592681885\n",
      "Warning: nan gradient found. The current loss is:  1.5183727741241455\n",
      "Warning: nan gradient found. The current loss is:  1.179398775100708\n",
      "Warning: nan gradient found. The current loss is:  0.5923277139663696\n",
      "Warning: nan gradient found. The current loss is:  0.277523934841156\n",
      "Warning: nan gradient found. The current loss is:  0.80544114112854\n",
      "Warning: nan gradient found. The current loss is:  1.0013511180877686\n",
      "Warning: nan gradient found. The current loss is:  1.07741117477417\n",
      "Warning: nan gradient found. The current loss is:  0.7054951190948486\n",
      "Warning: nan gradient found. The current loss is:  0.8232958912849426\n",
      "Warning: nan gradient found. The current loss is:  0.3190274238586426\n",
      "Warning: nan gradient found. The current loss is:  0.22236183285713196\n",
      "Warning: nan gradient found. The current loss is:  0.44461584091186523\n",
      "Warning: nan gradient found. The current loss is:  0.816150426864624\n",
      "Warning: nan gradient found. The current loss is:  0.4893409013748169\n",
      "Warning: nan gradient found. The current loss is:  0.4924355745315552\n",
      "Warning: nan gradient found. The current loss is:  0.3924753665924072\n",
      "Warning: nan gradient found. The current loss is:  0.325630784034729\n",
      "Warning: nan gradient found. The current loss is:  0.03899708762764931\n",
      "Warning: nan gradient found. The current loss is:  1.242677927017212\n",
      "Warning: nan gradient found. The current loss is:  1.1124393939971924\n",
      "Warning: nan gradient found. The current loss is:  0.7423778772354126\n",
      "Warning: nan gradient found. The current loss is:  0.012820139527320862\n",
      "Warning: nan gradient found. The current loss is:  0.5120061635971069\n",
      "Warning: nan gradient found. The current loss is:  0.7673647403717041\n",
      "Warning: nan gradient found. The current loss is:  0.5050067901611328\n",
      "Warning: nan gradient found. The current loss is:  0.3896316885948181\n",
      "Warning: nan gradient found. The current loss is:  1.3807300329208374\n",
      "Warning: nan gradient found. The current loss is:  0.8285819292068481\n",
      "Warning: nan gradient found. The current loss is:  1.7648342847824097\n",
      "Warning: nan gradient found. The current loss is:  0.8250412344932556\n",
      "Warning: nan gradient found. The current loss is:  0.3532557487487793\n",
      "Warning: nan gradient found. The current loss is:  0.6994721293449402\n",
      "Warning: nan gradient found. The current loss is:  0.14751416444778442\n",
      "Warning: nan gradient found. The current loss is:  0.38984793424606323\n",
      "Warning: nan gradient found. The current loss is:  0.7328131794929504\n",
      "Warning: nan gradient found. The current loss is:  0.765303373336792\n",
      "Warning: nan gradient found. The current loss is:  1.3966158628463745\n",
      "Warning: nan gradient found. The current loss is:  0.7701371908187866\n",
      "Warning: nan gradient found. The current loss is:  0.11147617548704147\n",
      "Warning: nan gradient found. The current loss is:  0.4770738482475281\n",
      "Warning: nan gradient found. The current loss is:  0.9899758696556091\n",
      "Warning: nan gradient found. The current loss is:  0.56015545129776\n",
      "Warning: nan gradient found. The current loss is:  0.9627919793128967\n",
      "Warning: nan gradient found. The current loss is:  1.1050653457641602\n",
      "Warning: nan gradient found. The current loss is:  0.3868343234062195\n",
      "Warning: nan gradient found. The current loss is:  0.45136478543281555\n",
      "Warning: nan gradient found. The current loss is:  0.44118067622184753\n",
      "Warning: nan gradient found. The current loss is:  0.7063019871711731\n",
      "Warning: nan gradient found. The current loss is:  0.4182640016078949\n",
      "Warning: nan gradient found. The current loss is:  0.7785457968711853\n",
      "Warning: nan gradient found. The current loss is:  0.8561210632324219\n",
      "Warning: nan gradient found. The current loss is:  2.3072025775909424\n",
      "Warning: nan gradient found. The current loss is:  0.46504271030426025\n",
      "Warning: nan gradient found. The current loss is:  1.1797624826431274\n",
      "Warning: nan gradient found. The current loss is:  0.5180021524429321\n",
      "Warning: nan gradient found. The current loss is:  0.2316027730703354\n",
      "Warning: nan gradient found. The current loss is:  1.3251394033432007\n",
      "Warning: nan gradient found. The current loss is:  0.6290686130523682\n",
      "Warning: nan gradient found. The current loss is:  0.4450063407421112\n",
      "Warning: nan gradient found. The current loss is:  0.5226501226425171\n",
      "Warning: nan gradient found. The current loss is:  0.5147413015365601\n",
      "Warning: nan gradient found. The current loss is:  0.39016565680503845\n",
      "Warning: nan gradient found. The current loss is:  1.005561113357544\n",
      "Warning: nan gradient found. The current loss is:  0.08651215583086014\n",
      "Warning: nan gradient found. The current loss is:  0.5165867805480957\n",
      "Warning: nan gradient found. The current loss is:  0.2849751114845276\n",
      "Warning: nan gradient found. The current loss is:  0.4508724510669708\n",
      "Warning: nan gradient found. The current loss is:  0.1733156144618988\n",
      "Warning: nan gradient found. The current loss is:  0.4414368271827698\n",
      "Warning: nan gradient found. The current loss is:  0.5263408422470093\n",
      "Warning: nan gradient found. The current loss is:  0.061738885939121246\n",
      "Warning: nan gradient found. The current loss is:  0.8317989706993103\n",
      "Warning: nan gradient found. The current loss is:  0.6900479793548584\n",
      "Warning: nan gradient found. The current loss is:  0.44673624634742737\n",
      "Warning: nan gradient found. The current loss is:  0.24498909711837769\n",
      "Warning: nan gradient found. The current loss is:  0.6508172750473022\n",
      "Warning: nan gradient found. The current loss is:  0.8577917814254761\n",
      "Warning: nan gradient found. The current loss is:  0.756659209728241\n",
      "Warning: nan gradient found. The current loss is:  0.49181023240089417\n",
      "Warning: nan gradient found. The current loss is:  0.17741724848747253\n",
      "Warning: nan gradient found. The current loss is:  0.4530843496322632\n",
      "Warning: nan gradient found. The current loss is:  0.7207400798797607\n",
      "Warning: nan gradient found. The current loss is:  0.8834108114242554\n",
      "Warning: nan gradient found. The current loss is:  0.44941267371177673\n",
      "Warning: nan gradient found. The current loss is:  0.19358155131340027\n",
      "Current batch training loss: 0.193582  [998400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.230355978012085\n",
      "Warning: nan gradient found. The current loss is:  0.8629846572875977\n",
      "Warning: nan gradient found. The current loss is:  0.35910165309906006\n",
      "Warning: nan gradient found. The current loss is:  0.2207963913679123\n",
      "Warning: nan gradient found. The current loss is:  0.4459361732006073\n",
      "Warning: nan gradient found. The current loss is:  1.0613617897033691\n",
      "Warning: nan gradient found. The current loss is:  1.7221887111663818\n",
      "Warning: nan gradient found. The current loss is:  1.1332335472106934\n",
      "Warning: nan gradient found. The current loss is:  0.40609291195869446\n",
      "Warning: nan gradient found. The current loss is:  0.4141311049461365\n",
      "Warning: nan gradient found. The current loss is:  0.1890454888343811\n",
      "Warning: nan gradient found. The current loss is:  1.5177931785583496\n",
      "Warning: nan gradient found. The current loss is:  0.10935120284557343\n",
      "Warning: nan gradient found. The current loss is:  0.589034914970398\n",
      "Warning: nan gradient found. The current loss is:  0.5277906656265259\n",
      "Warning: nan gradient found. The current loss is:  0.42202872037887573\n",
      "Warning: nan gradient found. The current loss is:  0.8060572743415833\n",
      "Warning: nan gradient found. The current loss is:  1.0145853757858276\n",
      "Warning: nan gradient found. The current loss is:  1.2298688888549805\n",
      "Warning: nan gradient found. The current loss is:  0.21879130601882935\n",
      "Warning: nan gradient found. The current loss is:  0.8441672921180725\n",
      "Warning: nan gradient found. The current loss is:  0.9531504511833191\n",
      "Warning: nan gradient found. The current loss is:  0.27118611335754395\n",
      "Warning: nan gradient found. The current loss is:  0.724926233291626\n",
      "Warning: nan gradient found. The current loss is:  0.676084578037262\n",
      "Warning: nan gradient found. The current loss is:  0.6926015615463257\n",
      "Warning: nan gradient found. The current loss is:  0.5216489434242249\n",
      "Warning: nan gradient found. The current loss is:  0.9776300191879272\n",
      "Warning: nan gradient found. The current loss is:  0.3240091800689697\n",
      "Warning: nan gradient found. The current loss is:  0.3034026622772217\n",
      "Warning: nan gradient found. The current loss is:  1.4555739164352417\n",
      "Warning: nan gradient found. The current loss is:  0.9159259796142578\n",
      "Warning: nan gradient found. The current loss is:  0.579332709312439\n",
      "Warning: nan gradient found. The current loss is:  0.2844109833240509\n",
      "Warning: nan gradient found. The current loss is:  0.5929892659187317\n",
      "Warning: nan gradient found. The current loss is:  0.6651755571365356\n",
      "Warning: nan gradient found. The current loss is:  0.2771710753440857\n",
      "Warning: nan gradient found. The current loss is:  0.40529870986938477\n",
      "Warning: nan gradient found. The current loss is:  2.2019617557525635\n",
      "Warning: nan gradient found. The current loss is:  1.130981683731079\n",
      "Warning: nan gradient found. The current loss is:  0.6335048675537109\n",
      "Warning: nan gradient found. The current loss is:  0.6806111335754395\n",
      "Warning: nan gradient found. The current loss is:  0.36148539185523987\n",
      "Warning: nan gradient found. The current loss is:  0.27528977394104004\n",
      "Warning: nan gradient found. The current loss is:  0.697574257850647\n",
      "Warning: nan gradient found. The current loss is:  0.218446284532547\n",
      "Warning: nan gradient found. The current loss is:  0.6667356491088867\n",
      "Warning: nan gradient found. The current loss is:  0.8944258093833923\n",
      "Warning: nan gradient found. The current loss is:  1.0644407272338867\n",
      "Warning: nan gradient found. The current loss is:  0.5367804169654846\n",
      "Warning: nan gradient found. The current loss is:  0.5865744352340698\n",
      "Warning: nan gradient found. The current loss is:  0.7173083424568176\n",
      "Warning: nan gradient found. The current loss is:  0.3698085844516754\n",
      "Warning: nan gradient found. The current loss is:  1.3511748313903809\n",
      "Warning: nan gradient found. The current loss is:  1.2966713905334473\n",
      "Warning: nan gradient found. The current loss is:  0.628555417060852\n",
      "Warning: nan gradient found. The current loss is:  0.8400284051895142\n",
      "Warning: nan gradient found. The current loss is:  0.6600909233093262\n",
      "Warning: nan gradient found. The current loss is:  0.46606767177581787\n",
      "Warning: nan gradient found. The current loss is:  0.3269721567630768\n",
      "Warning: nan gradient found. The current loss is:  1.0092467069625854\n",
      "Warning: nan gradient found. The current loss is:  0.18966753780841827\n",
      "Warning: nan gradient found. The current loss is:  0.5806607007980347\n",
      "Warning: nan gradient found. The current loss is:  0.39195117354393005\n",
      "Warning: nan gradient found. The current loss is:  0.9392819404602051\n",
      "Warning: nan gradient found. The current loss is:  0.639631986618042\n",
      "Warning: nan gradient found. The current loss is:  0.06692643463611603\n",
      "Warning: nan gradient found. The current loss is:  0.6730612516403198\n",
      "Warning: nan gradient found. The current loss is:  0.4970402717590332\n",
      "Warning: nan gradient found. The current loss is:  0.4960328936576843\n",
      "Warning: nan gradient found. The current loss is:  0.3540635108947754\n",
      "Warning: nan gradient found. The current loss is:  0.6479582786560059\n",
      "Warning: nan gradient found. The current loss is:  0.2494402676820755\n",
      "Warning: nan gradient found. The current loss is:  0.391358345746994\n",
      "Warning: nan gradient found. The current loss is:  1.344806432723999\n",
      "Warning: nan gradient found. The current loss is:  1.4890953302383423\n",
      "Warning: nan gradient found. The current loss is:  0.7986912131309509\n",
      "Warning: nan gradient found. The current loss is:  0.37364768981933594\n",
      "Warning: nan gradient found. The current loss is:  0.7547892928123474\n",
      "Warning: nan gradient found. The current loss is:  2.4422006607055664\n",
      "Warning: nan gradient found. The current loss is:  1.33882474899292\n",
      "Warning: nan gradient found. The current loss is:  0.25316980481147766\n",
      "Warning: nan gradient found. The current loss is:  0.6511873006820679\n",
      "Warning: nan gradient found. The current loss is:  0.7376166582107544\n",
      "Warning: nan gradient found. The current loss is:  0.7478070259094238\n",
      "Warning: nan gradient found. The current loss is:  0.7385765314102173\n",
      "Warning: nan gradient found. The current loss is:  0.21808309853076935\n",
      "Warning: nan gradient found. The current loss is:  1.0412318706512451\n",
      "Warning: nan gradient found. The current loss is:  0.3855588436126709\n",
      "Warning: nan gradient found. The current loss is:  0.466927707195282\n",
      "Warning: nan gradient found. The current loss is:  0.854163408279419\n",
      "Warning: nan gradient found. The current loss is:  0.9346475601196289\n",
      "Warning: nan gradient found. The current loss is:  0.4983491897583008\n",
      "Warning: nan gradient found. The current loss is:  0.8508726358413696\n",
      "Warning: nan gradient found. The current loss is:  0.8972659111022949\n",
      "Warning: nan gradient found. The current loss is:  0.709266185760498\n",
      "Warning: nan gradient found. The current loss is:  0.39636343717575073\n",
      "Warning: nan gradient found. The current loss is:  0.9635033011436462\n",
      "Warning: nan gradient found. The current loss is:  0.9743489027023315\n",
      "Warning: nan gradient found. The current loss is:  0.798183798789978\n",
      "Current batch training loss: 0.798184  [1024000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.2890815734863281\n",
      "Warning: nan gradient found. The current loss is:  0.38358503580093384\n",
      "Warning: nan gradient found. The current loss is:  0.773639440536499\n",
      "Warning: nan gradient found. The current loss is:  0.6241872310638428\n",
      "Warning: nan gradient found. The current loss is:  0.8240545988082886\n",
      "Warning: nan gradient found. The current loss is:  0.4742303788661957\n",
      "Warning: nan gradient found. The current loss is:  0.9514275193214417\n",
      "Warning: nan gradient found. The current loss is:  1.5576202869415283\n",
      "Warning: nan gradient found. The current loss is:  1.0807113647460938\n",
      "Warning: nan gradient found. The current loss is:  1.270580530166626\n",
      "Warning: nan gradient found. The current loss is:  1.5446405410766602\n",
      "Warning: nan gradient found. The current loss is:  -0.13353610038757324\n",
      "Warning: nan gradient found. The current loss is:  0.26548606157302856\n",
      "Warning: nan gradient found. The current loss is:  0.302822083234787\n",
      "Warning: nan gradient found. The current loss is:  0.4305112957954407\n",
      "Warning: nan gradient found. The current loss is:  0.45052027702331543\n",
      "Warning: nan gradient found. The current loss is:  0.9689056873321533\n",
      "Warning: nan gradient found. The current loss is:  0.32696637511253357\n",
      "Warning: nan gradient found. The current loss is:  0.8555408716201782\n",
      "Warning: nan gradient found. The current loss is:  0.6604278087615967\n",
      "Warning: nan gradient found. The current loss is:  0.876801609992981\n",
      "Warning: nan gradient found. The current loss is:  0.990720808506012\n",
      "Warning: nan gradient found. The current loss is:  0.9004421234130859\n",
      "Warning: nan gradient found. The current loss is:  0.502863347530365\n",
      "Warning: nan gradient found. The current loss is:  0.658180832862854\n",
      "Warning: nan gradient found. The current loss is:  0.29098963737487793\n",
      "Warning: nan gradient found. The current loss is:  0.4935089945793152\n",
      "Warning: nan gradient found. The current loss is:  0.8399494886398315\n",
      "Warning: nan gradient found. The current loss is:  0.18018831312656403\n",
      "Warning: nan gradient found. The current loss is:  0.6385799646377563\n",
      "Warning: nan gradient found. The current loss is:  0.607376217842102\n",
      "Warning: nan gradient found. The current loss is:  0.21382656693458557\n",
      "Warning: nan gradient found. The current loss is:  0.21060363948345184\n",
      "Warning: nan gradient found. The current loss is:  0.6142189502716064\n",
      "Warning: nan gradient found. The current loss is:  0.7076555490493774\n",
      "Warning: nan gradient found. The current loss is:  0.3794305920600891\n",
      "Warning: nan gradient found. The current loss is:  0.47303879261016846\n",
      "Warning: nan gradient found. The current loss is:  0.38272643089294434\n",
      "Warning: nan gradient found. The current loss is:  0.5581647753715515\n",
      "Warning: nan gradient found. The current loss is:  1.3041965961456299\n",
      "Warning: nan gradient found. The current loss is:  0.47160008549690247\n",
      "Warning: nan gradient found. The current loss is:  0.2713406980037689\n",
      "Warning: nan gradient found. The current loss is:  0.663500189781189\n",
      "Warning: nan gradient found. The current loss is:  0.6736061573028564\n",
      "Warning: nan gradient found. The current loss is:  0.544491171836853\n",
      "Warning: nan gradient found. The current loss is:  0.8111598491668701\n",
      "Warning: nan gradient found. The current loss is:  0.16891656816005707\n",
      "Warning: nan gradient found. The current loss is:  1.5304110050201416\n",
      "Warning: nan gradient found. The current loss is:  0.6384134292602539\n",
      "Warning: nan gradient found. The current loss is:  0.3931715488433838\n",
      "Warning: nan gradient found. The current loss is:  0.8421634435653687\n",
      "Warning: nan gradient found. The current loss is:  1.2898133993148804\n",
      "Warning: nan gradient found. The current loss is:  0.6274864077568054\n",
      "Warning: nan gradient found. The current loss is:  0.9073299169540405\n",
      "Warning: nan gradient found. The current loss is:  0.41702574491500854\n",
      "Warning: nan gradient found. The current loss is:  0.3266868591308594\n",
      "Warning: nan gradient found. The current loss is:  0.32480645179748535\n",
      "Warning: nan gradient found. The current loss is:  0.4956400394439697\n",
      "Warning: nan gradient found. The current loss is:  0.6798774003982544\n",
      "Warning: nan gradient found. The current loss is:  0.37364310026168823\n",
      "Warning: nan gradient found. The current loss is:  0.7507929801940918\n",
      "Warning: nan gradient found. The current loss is:  1.0652412176132202\n",
      "Warning: nan gradient found. The current loss is:  0.7175875902175903\n",
      "Warning: nan gradient found. The current loss is:  0.91240394115448\n",
      "Warning: nan gradient found. The current loss is:  1.1521655321121216\n",
      "Warning: nan gradient found. The current loss is:  0.5922374725341797\n",
      "Warning: nan gradient found. The current loss is:  0.8756685256958008\n",
      "Warning: nan gradient found. The current loss is:  0.5609169006347656\n",
      "Warning: nan gradient found. The current loss is:  0.5882176160812378\n",
      "Warning: nan gradient found. The current loss is:  1.2552560567855835\n",
      "Warning: nan gradient found. The current loss is:  0.9600950479507446\n",
      "Warning: nan gradient found. The current loss is:  0.7582041025161743\n",
      "Warning: nan gradient found. The current loss is:  0.06150829792022705\n",
      "Warning: nan gradient found. The current loss is:  0.7543802261352539\n",
      "Warning: nan gradient found. The current loss is:  0.4685880243778229\n",
      "Warning: nan gradient found. The current loss is:  0.6690188646316528\n",
      "Warning: nan gradient found. The current loss is:  0.2309628576040268\n",
      "Warning: nan gradient found. The current loss is:  0.4178510904312134\n",
      "Warning: nan gradient found. The current loss is:  0.9396405816078186\n",
      "Warning: nan gradient found. The current loss is:  1.6202746629714966\n",
      "Warning: nan gradient found. The current loss is:  0.4335545599460602\n",
      "Warning: nan gradient found. The current loss is:  0.23848338425159454\n",
      "Warning: nan gradient found. The current loss is:  0.6290504336357117\n",
      "Warning: nan gradient found. The current loss is:  1.2631291151046753\n",
      "Warning: nan gradient found. The current loss is:  0.6040054559707642\n",
      "Warning: nan gradient found. The current loss is:  0.7846987247467041\n",
      "Warning: nan gradient found. The current loss is:  0.9544123411178589\n",
      "Warning: nan gradient found. The current loss is:  0.41204941272735596\n",
      "Warning: nan gradient found. The current loss is:  0.2770181894302368\n",
      "Warning: nan gradient found. The current loss is:  0.694812536239624\n",
      "Warning: nan gradient found. The current loss is:  0.25656387209892273\n",
      "Warning: nan gradient found. The current loss is:  0.8985868692398071\n",
      "Warning: nan gradient found. The current loss is:  0.5917263031005859\n",
      "Warning: nan gradient found. The current loss is:  0.9459844827651978\n",
      "Warning: nan gradient found. The current loss is:  0.7053835391998291\n",
      "Warning: nan gradient found. The current loss is:  0.8580732941627502\n",
      "Warning: nan gradient found. The current loss is:  0.9514361619949341\n",
      "Warning: nan gradient found. The current loss is:  0.6843901872634888\n",
      "Warning: nan gradient found. The current loss is:  0.37429749965667725\n",
      "Warning: nan gradient found. The current loss is:  0.5994244813919067\n",
      "Current batch training loss: 0.599424  [1049600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.1735926866531372\n",
      "Warning: nan gradient found. The current loss is:  1.0476359128952026\n",
      "Warning: nan gradient found. The current loss is:  0.06344674527645111\n",
      "Warning: nan gradient found. The current loss is:  1.1029982566833496\n",
      "Warning: nan gradient found. The current loss is:  0.7523254156112671\n",
      "Warning: nan gradient found. The current loss is:  0.3846432566642761\n",
      "Warning: nan gradient found. The current loss is:  0.7590762972831726\n",
      "Warning: nan gradient found. The current loss is:  0.24653276801109314\n",
      "Warning: nan gradient found. The current loss is:  1.6758151054382324\n",
      "Warning: nan gradient found. The current loss is:  0.48259347677230835\n",
      "Warning: nan gradient found. The current loss is:  0.939037561416626\n",
      "Warning: nan gradient found. The current loss is:  1.4433228969573975\n",
      "Warning: nan gradient found. The current loss is:  0.35996532440185547\n",
      "Warning: nan gradient found. The current loss is:  0.41092348098754883\n",
      "Warning: nan gradient found. The current loss is:  0.4777400493621826\n",
      "Warning: nan gradient found. The current loss is:  0.5465831756591797\n",
      "Warning: nan gradient found. The current loss is:  0.22928833961486816\n",
      "Warning: nan gradient found. The current loss is:  0.7398990392684937\n",
      "Warning: nan gradient found. The current loss is:  0.21231797337532043\n",
      "Warning: nan gradient found. The current loss is:  0.36807793378829956\n",
      "Warning: nan gradient found. The current loss is:  0.7072097659111023\n",
      "Warning: nan gradient found. The current loss is:  0.9431324005126953\n",
      "Warning: nan gradient found. The current loss is:  0.848710298538208\n",
      "Warning: nan gradient found. The current loss is:  0.28481242060661316\n",
      "Warning: nan gradient found. The current loss is:  0.628240704536438\n",
      "Warning: nan gradient found. The current loss is:  0.5309373736381531\n",
      "Warning: nan gradient found. The current loss is:  0.2938392758369446\n",
      "Warning: nan gradient found. The current loss is:  1.1218937635421753\n",
      "Warning: nan gradient found. The current loss is:  0.07832998782396317\n",
      "Warning: nan gradient found. The current loss is:  0.4629320204257965\n",
      "Warning: nan gradient found. The current loss is:  1.7998543977737427\n",
      "Warning: nan gradient found. The current loss is:  0.22905945777893066\n",
      "Warning: nan gradient found. The current loss is:  0.4929982125759125\n",
      "Warning: nan gradient found. The current loss is:  0.4746354818344116\n",
      "Warning: nan gradient found. The current loss is:  0.5587325096130371\n",
      "Warning: nan gradient found. The current loss is:  1.1366217136383057\n",
      "Warning: nan gradient found. The current loss is:  0.8196916580200195\n",
      "Warning: nan gradient found. The current loss is:  0.6343052387237549\n",
      "Warning: nan gradient found. The current loss is:  1.488529920578003\n",
      "Warning: nan gradient found. The current loss is:  1.284139633178711\n",
      "Warning: nan gradient found. The current loss is:  1.4502573013305664\n",
      "Warning: nan gradient found. The current loss is:  0.7761971354484558\n",
      "Warning: nan gradient found. The current loss is:  0.43589529395103455\n",
      "Warning: nan gradient found. The current loss is:  0.3934195339679718\n",
      "Warning: nan gradient found. The current loss is:  0.6163588762283325\n",
      "Warning: nan gradient found. The current loss is:  1.1248053312301636\n",
      "Warning: nan gradient found. The current loss is:  0.5019392371177673\n",
      "Warning: nan gradient found. The current loss is:  0.5074414014816284\n",
      "Warning: nan gradient found. The current loss is:  0.7695488333702087\n",
      "Warning: nan gradient found. The current loss is:  0.8418706059455872\n",
      "Warning: nan gradient found. The current loss is:  0.5696830153465271\n",
      "Warning: nan gradient found. The current loss is:  0.8204785585403442\n",
      "Warning: nan gradient found. The current loss is:  2.1146910190582275\n",
      "Warning: nan gradient found. The current loss is:  0.7370739579200745\n",
      "Warning: nan gradient found. The current loss is:  1.38555109500885\n",
      "Warning: nan gradient found. The current loss is:  1.2068123817443848\n",
      "Warning: nan gradient found. The current loss is:  0.03258563578128815\n",
      "Warning: nan gradient found. The current loss is:  0.7378770112991333\n",
      "Warning: nan gradient found. The current loss is:  0.5056807994842529\n",
      "Warning: nan gradient found. The current loss is:  0.09869518876075745\n",
      "Warning: nan gradient found. The current loss is:  1.0369549989700317\n",
      "Warning: nan gradient found. The current loss is:  0.8255184292793274\n",
      "Warning: nan gradient found. The current loss is:  1.3041625022888184\n",
      "Warning: nan gradient found. The current loss is:  0.6574859619140625\n",
      "Warning: nan gradient found. The current loss is:  0.6095627546310425\n",
      "Warning: nan gradient found. The current loss is:  0.7082385420799255\n",
      "Warning: nan gradient found. The current loss is:  1.0800906419754028\n",
      "Warning: nan gradient found. The current loss is:  0.6350036263465881\n",
      "Warning: nan gradient found. The current loss is:  1.1044496297836304\n",
      "Warning: nan gradient found. The current loss is:  0.5794093012809753\n",
      "Warning: nan gradient found. The current loss is:  0.38572537899017334\n",
      "Warning: nan gradient found. The current loss is:  0.9702091813087463\n",
      "Warning: nan gradient found. The current loss is:  2.8429605960845947\n",
      "Warning: nan gradient found. The current loss is:  0.260822594165802\n",
      "Warning: nan gradient found. The current loss is:  0.7489162683486938\n",
      "Warning: nan gradient found. The current loss is:  0.12622225284576416\n",
      "Warning: nan gradient found. The current loss is:  0.4176468849182129\n",
      "Warning: nan gradient found. The current loss is:  0.7241962552070618\n",
      "Warning: nan gradient found. The current loss is:  0.9148960113525391\n",
      "Warning: nan gradient found. The current loss is:  0.32518118619918823\n",
      "Warning: nan gradient found. The current loss is:  0.4204425811767578\n",
      "Warning: nan gradient found. The current loss is:  0.29366177320480347\n",
      "Warning: nan gradient found. The current loss is:  0.9889588356018066\n",
      "Warning: nan gradient found. The current loss is:  0.12119945883750916\n",
      "Warning: nan gradient found. The current loss is:  0.7965725064277649\n",
      "Warning: nan gradient found. The current loss is:  2.4319348335266113\n",
      "Warning: nan gradient found. The current loss is:  1.1701205968856812\n",
      "Warning: nan gradient found. The current loss is:  1.3332126140594482\n",
      "Warning: nan gradient found. The current loss is:  0.3078848421573639\n",
      "Warning: nan gradient found. The current loss is:  1.1216309070587158\n",
      "Warning: nan gradient found. The current loss is:  0.394035279750824\n",
      "Warning: nan gradient found. The current loss is:  0.6490306258201599\n",
      "Warning: nan gradient found. The current loss is:  1.4765894412994385\n",
      "Warning: nan gradient found. The current loss is:  0.4038796126842499\n",
      "Warning: nan gradient found. The current loss is:  0.5075246691703796\n",
      "Warning: nan gradient found. The current loss is:  0.6599626541137695\n",
      "Warning: nan gradient found. The current loss is:  0.5561724305152893\n",
      "Warning: nan gradient found. The current loss is:  0.5297943353652954\n",
      "Warning: nan gradient found. The current loss is:  0.3905888497829437\n",
      "Warning: nan gradient found. The current loss is:  0.9082589149475098\n",
      "Current batch training loss: 0.908259  [1075200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2656693160533905\n",
      "Warning: nan gradient found. The current loss is:  0.6773697137832642\n",
      "Warning: nan gradient found. The current loss is:  0.42709657549858093\n",
      "Warning: nan gradient found. The current loss is:  0.5339053869247437\n",
      "Warning: nan gradient found. The current loss is:  0.16595029830932617\n",
      "Warning: nan gradient found. The current loss is:  0.8329179286956787\n",
      "Warning: nan gradient found. The current loss is:  0.5403957962989807\n",
      "Warning: nan gradient found. The current loss is:  1.1977274417877197\n",
      "Warning: nan gradient found. The current loss is:  0.9762681722640991\n",
      "Warning: nan gradient found. The current loss is:  0.5839381217956543\n",
      "Warning: nan gradient found. The current loss is:  0.3366062045097351\n",
      "Warning: nan gradient found. The current loss is:  0.6785022020339966\n",
      "Warning: nan gradient found. The current loss is:  0.6215208768844604\n",
      "Warning: nan gradient found. The current loss is:  0.6212955713272095\n",
      "Warning: nan gradient found. The current loss is:  0.6817913055419922\n",
      "Warning: nan gradient found. The current loss is:  2.7033026218414307\n",
      "Warning: nan gradient found. The current loss is:  0.3030911386013031\n",
      "Warning: nan gradient found. The current loss is:  1.1623470783233643\n",
      "Warning: nan gradient found. The current loss is:  0.34303492307662964\n",
      "Warning: nan gradient found. The current loss is:  0.904681921005249\n",
      "Warning: nan gradient found. The current loss is:  1.5448298454284668\n",
      "Warning: nan gradient found. The current loss is:  0.2929230034351349\n",
      "Warning: nan gradient found. The current loss is:  2.179159641265869\n",
      "Warning: nan gradient found. The current loss is:  0.45061278343200684\n",
      "Warning: nan gradient found. The current loss is:  0.6775403618812561\n",
      "Warning: nan gradient found. The current loss is:  0.7000510096549988\n",
      "Warning: nan gradient found. The current loss is:  1.8251681327819824\n",
      "Warning: nan gradient found. The current loss is:  0.697424590587616\n",
      "Warning: nan gradient found. The current loss is:  0.39834943413734436\n",
      "Warning: nan gradient found. The current loss is:  0.5304132699966431\n",
      "Warning: nan gradient found. The current loss is:  0.5416650772094727\n",
      "Warning: nan gradient found. The current loss is:  0.656624972820282\n",
      "Warning: nan gradient found. The current loss is:  0.7964179515838623\n",
      "Warning: nan gradient found. The current loss is:  0.9268267750740051\n",
      "Warning: nan gradient found. The current loss is:  0.5256774425506592\n",
      "Warning: nan gradient found. The current loss is:  0.692611038684845\n",
      "Warning: nan gradient found. The current loss is:  1.2714078426361084\n",
      "Warning: nan gradient found. The current loss is:  1.909207820892334\n",
      "Warning: nan gradient found. The current loss is:  0.3878212869167328\n",
      "Warning: nan gradient found. The current loss is:  0.37638866901397705\n",
      "Warning: nan gradient found. The current loss is:  0.8953427076339722\n",
      "Warning: nan gradient found. The current loss is:  0.38459116220474243\n",
      "Warning: nan gradient found. The current loss is:  1.1770434379577637\n",
      "Warning: nan gradient found. The current loss is:  0.43635451793670654\n",
      "Warning: nan gradient found. The current loss is:  0.4623151421546936\n",
      "Warning: nan gradient found. The current loss is:  0.3981403708457947\n",
      "Warning: nan gradient found. The current loss is:  0.5289145112037659\n",
      "Warning: nan gradient found. The current loss is:  0.07888513803482056\n",
      "Warning: nan gradient found. The current loss is:  0.6098134517669678\n",
      "Warning: nan gradient found. The current loss is:  0.7948176860809326\n",
      "Warning: nan gradient found. The current loss is:  0.2956799566745758\n",
      "Warning: nan gradient found. The current loss is:  0.3149156868457794\n",
      "Warning: nan gradient found. The current loss is:  0.8603877425193787\n",
      "Warning: nan gradient found. The current loss is:  0.47355735301971436\n",
      "Warning: nan gradient found. The current loss is:  1.3392455577850342\n",
      "Warning: nan gradient found. The current loss is:  0.3598337471485138\n",
      "Warning: nan gradient found. The current loss is:  0.5700697898864746\n",
      "Warning: nan gradient found. The current loss is:  0.625350832939148\n",
      "Warning: nan gradient found. The current loss is:  0.27580225467681885\n",
      "Warning: nan gradient found. The current loss is:  0.7855502367019653\n",
      "Warning: nan gradient found. The current loss is:  1.150146722793579\n",
      "Warning: nan gradient found. The current loss is:  0.16314539313316345\n",
      "Warning: nan gradient found. The current loss is:  0.5406814813613892\n",
      "Warning: nan gradient found. The current loss is:  0.45298856496810913\n",
      "Warning: nan gradient found. The current loss is:  0.6965687274932861\n",
      "Warning: nan gradient found. The current loss is:  0.5400835275650024\n",
      "Warning: nan gradient found. The current loss is:  0.7809504270553589\n",
      "Warning: nan gradient found. The current loss is:  0.6741340756416321\n",
      "Warning: nan gradient found. The current loss is:  1.4232511520385742\n",
      "Warning: nan gradient found. The current loss is:  0.4485410749912262\n",
      "Warning: nan gradient found. The current loss is:  0.5989418029785156\n",
      "Warning: nan gradient found. The current loss is:  0.2123469114303589\n",
      "Warning: nan gradient found. The current loss is:  0.9515873193740845\n",
      "Warning: nan gradient found. The current loss is:  0.6414117813110352\n",
      "Warning: nan gradient found. The current loss is:  0.1878030002117157\n",
      "Warning: nan gradient found. The current loss is:  0.7755476236343384\n",
      "Warning: nan gradient found. The current loss is:  0.7508292198181152\n",
      "Warning: nan gradient found. The current loss is:  1.0237712860107422\n",
      "Warning: nan gradient found. The current loss is:  0.19828800857067108\n",
      "Warning: nan gradient found. The current loss is:  0.7197825908660889\n",
      "Warning: nan gradient found. The current loss is:  0.3324858844280243\n",
      "Warning: nan gradient found. The current loss is:  1.1557385921478271\n",
      "Warning: nan gradient found. The current loss is:  0.11865995824337006\n",
      "Warning: nan gradient found. The current loss is:  0.34489157795906067\n",
      "Warning: nan gradient found. The current loss is:  1.1064091920852661\n",
      "Warning: nan gradient found. The current loss is:  0.8216780424118042\n",
      "Warning: nan gradient found. The current loss is:  0.5693787932395935\n",
      "Warning: nan gradient found. The current loss is:  0.7014232873916626\n",
      "Warning: nan gradient found. The current loss is:  0.7203598022460938\n",
      "Warning: nan gradient found. The current loss is:  0.4266716241836548\n",
      "Warning: nan gradient found. The current loss is:  1.1875441074371338\n",
      "Warning: nan gradient found. The current loss is:  1.3113808631896973\n",
      "Warning: nan gradient found. The current loss is:  0.7941033840179443\n",
      "Warning: nan gradient found. The current loss is:  0.37265217304229736\n",
      "Warning: nan gradient found. The current loss is:  1.0780267715454102\n",
      "Warning: nan gradient found. The current loss is:  0.13085851073265076\n",
      "Warning: nan gradient found. The current loss is:  1.7717763185501099\n",
      "Warning: nan gradient found. The current loss is:  0.24252042174339294\n",
      "Warning: nan gradient found. The current loss is:  0.47605207562446594\n",
      "Warning: nan gradient found. The current loss is:  1.0223724842071533\n",
      "Current batch training loss: 1.022372  [1100800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6842555403709412\n",
      "Warning: nan gradient found. The current loss is:  0.8820102214813232\n",
      "Warning: nan gradient found. The current loss is:  0.3756359815597534\n",
      "Warning: nan gradient found. The current loss is:  0.575678825378418\n",
      "Warning: nan gradient found. The current loss is:  1.1129037141799927\n",
      "Warning: nan gradient found. The current loss is:  -0.04744237661361694\n",
      "Warning: nan gradient found. The current loss is:  0.4933767318725586\n",
      "Warning: nan gradient found. The current loss is:  0.5863306522369385\n",
      "Warning: nan gradient found. The current loss is:  0.4890027940273285\n",
      "Warning: nan gradient found. The current loss is:  0.8529579639434814\n",
      "Warning: nan gradient found. The current loss is:  0.7548133730888367\n",
      "Warning: nan gradient found. The current loss is:  0.7647950649261475\n",
      "Warning: nan gradient found. The current loss is:  0.8159587979316711\n",
      "Warning: nan gradient found. The current loss is:  0.0463690459728241\n",
      "Warning: nan gradient found. The current loss is:  0.7214503884315491\n",
      "Warning: nan gradient found. The current loss is:  2.1804416179656982\n",
      "Warning: nan gradient found. The current loss is:  0.2397465854883194\n",
      "Warning: nan gradient found. The current loss is:  2.093418598175049\n",
      "Warning: nan gradient found. The current loss is:  0.7011421918869019\n",
      "Warning: nan gradient found. The current loss is:  0.513789176940918\n",
      "Warning: nan gradient found. The current loss is:  0.503411054611206\n",
      "Warning: nan gradient found. The current loss is:  1.3551254272460938\n",
      "Warning: nan gradient found. The current loss is:  0.4628523886203766\n",
      "Warning: nan gradient found. The current loss is:  0.5756354928016663\n",
      "Warning: nan gradient found. The current loss is:  0.7766627073287964\n",
      "Warning: nan gradient found. The current loss is:  0.38405412435531616\n",
      "Warning: nan gradient found. The current loss is:  0.5672802925109863\n",
      "Warning: nan gradient found. The current loss is:  1.0765835046768188\n",
      "Warning: nan gradient found. The current loss is:  1.9641472101211548\n",
      "Warning: nan gradient found. The current loss is:  0.36088263988494873\n",
      "Warning: nan gradient found. The current loss is:  0.6643150448799133\n",
      "Warning: nan gradient found. The current loss is:  0.32040852308273315\n",
      "Warning: nan gradient found. The current loss is:  0.9719955921173096\n",
      "Warning: nan gradient found. The current loss is:  0.8136578798294067\n",
      "Warning: nan gradient found. The current loss is:  0.6443358659744263\n",
      "Warning: nan gradient found. The current loss is:  0.802681028842926\n",
      "Warning: nan gradient found. The current loss is:  1.1779637336730957\n",
      "Warning: nan gradient found. The current loss is:  0.5753630995750427\n",
      "Warning: nan gradient found. The current loss is:  0.6961649656295776\n",
      "Warning: nan gradient found. The current loss is:  1.1291699409484863\n",
      "Warning: nan gradient found. The current loss is:  0.3720048666000366\n",
      "Warning: nan gradient found. The current loss is:  1.0011498928070068\n",
      "Warning: nan gradient found. The current loss is:  0.47070467472076416\n",
      "Warning: nan gradient found. The current loss is:  0.5413900017738342\n",
      "Warning: nan gradient found. The current loss is:  0.7889955639839172\n",
      "Warning: nan gradient found. The current loss is:  0.5418007373809814\n",
      "Warning: nan gradient found. The current loss is:  0.206009641289711\n",
      "Warning: nan gradient found. The current loss is:  0.6004176139831543\n",
      "Warning: nan gradient found. The current loss is:  1.0087798833847046\n",
      "Warning: nan gradient found. The current loss is:  0.5924277305603027\n",
      "Warning: nan gradient found. The current loss is:  0.8043853640556335\n",
      "Warning: nan gradient found. The current loss is:  0.7079488635063171\n",
      "Warning: nan gradient found. The current loss is:  1.5782017707824707\n",
      "Warning: nan gradient found. The current loss is:  1.1041456460952759\n",
      "Warning: nan gradient found. The current loss is:  1.1624823808670044\n",
      "Warning: nan gradient found. The current loss is:  -0.011687934398651123\n",
      "Warning: nan gradient found. The current loss is:  0.7023564577102661\n",
      "Warning: nan gradient found. The current loss is:  0.7690479159355164\n",
      "Warning: nan gradient found. The current loss is:  0.6782597303390503\n",
      "Warning: nan gradient found. The current loss is:  0.565300464630127\n",
      "Warning: nan gradient found. The current loss is:  0.6127598285675049\n",
      "Warning: nan gradient found. The current loss is:  0.3782700002193451\n",
      "Warning: nan gradient found. The current loss is:  0.5162228941917419\n",
      "Warning: nan gradient found. The current loss is:  1.1388121843338013\n",
      "Warning: nan gradient found. The current loss is:  0.8725736141204834\n",
      "Warning: nan gradient found. The current loss is:  0.42187589406967163\n",
      "Warning: nan gradient found. The current loss is:  0.5836389660835266\n",
      "Warning: nan gradient found. The current loss is:  0.555689811706543\n",
      "Warning: nan gradient found. The current loss is:  0.41944658756256104\n",
      "Warning: nan gradient found. The current loss is:  0.11176317930221558\n",
      "Warning: nan gradient found. The current loss is:  0.4387131631374359\n",
      "Warning: nan gradient found. The current loss is:  2.329832077026367\n",
      "Warning: nan gradient found. The current loss is:  1.0500051975250244\n",
      "Warning: nan gradient found. The current loss is:  0.2825242578983307\n",
      "Warning: nan gradient found. The current loss is:  0.9227911233901978\n",
      "Warning: nan gradient found. The current loss is:  0.7047368288040161\n",
      "Warning: nan gradient found. The current loss is:  0.4171029329299927\n",
      "Warning: nan gradient found. The current loss is:  0.4353923201560974\n",
      "Warning: nan gradient found. The current loss is:  0.2374352514743805\n",
      "Warning: nan gradient found. The current loss is:  0.45073556900024414\n",
      "Warning: nan gradient found. The current loss is:  0.631293773651123\n",
      "Warning: nan gradient found. The current loss is:  3.274907350540161\n",
      "Warning: nan gradient found. The current loss is:  0.6167235374450684\n",
      "Warning: nan gradient found. The current loss is:  0.4287893772125244\n",
      "Warning: nan gradient found. The current loss is:  1.0956473350524902\n",
      "Warning: nan gradient found. The current loss is:  0.19124159216880798\n",
      "Warning: nan gradient found. The current loss is:  0.31751537322998047\n",
      "Warning: nan gradient found. The current loss is:  0.4831603169441223\n",
      "Warning: nan gradient found. The current loss is:  0.8868619203567505\n",
      "Warning: nan gradient found. The current loss is:  0.8583486676216125\n",
      "Warning: nan gradient found. The current loss is:  0.40185627341270447\n",
      "Warning: nan gradient found. The current loss is:  0.5276080369949341\n",
      "Warning: nan gradient found. The current loss is:  0.38905084133148193\n",
      "Warning: nan gradient found. The current loss is:  1.4981300830841064\n",
      "Warning: nan gradient found. The current loss is:  1.1618337631225586\n",
      "Warning: nan gradient found. The current loss is:  0.8678467273712158\n",
      "Warning: nan gradient found. The current loss is:  0.3270670771598816\n",
      "Warning: nan gradient found. The current loss is:  0.6882895231246948\n",
      "Warning: nan gradient found. The current loss is:  1.307848572731018\n",
      "Warning: nan gradient found. The current loss is:  1.5680875778198242\n",
      "Current batch training loss: 1.568088  [1126400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.24077057838439941\n",
      "Warning: nan gradient found. The current loss is:  0.5887969136238098\n",
      "Warning: nan gradient found. The current loss is:  0.6645166277885437\n",
      "Warning: nan gradient found. The current loss is:  1.911630630493164\n",
      "Warning: nan gradient found. The current loss is:  0.4494914710521698\n",
      "Warning: nan gradient found. The current loss is:  0.3011070489883423\n",
      "Warning: nan gradient found. The current loss is:  0.8679611682891846\n",
      "Warning: nan gradient found. The current loss is:  0.40784305334091187\n",
      "Warning: nan gradient found. The current loss is:  0.9313033223152161\n",
      "Warning: nan gradient found. The current loss is:  0.24598737061023712\n",
      "Warning: nan gradient found. The current loss is:  0.7105705738067627\n",
      "Warning: nan gradient found. The current loss is:  0.9349437355995178\n",
      "Warning: nan gradient found. The current loss is:  0.827846884727478\n",
      "Warning: nan gradient found. The current loss is:  0.8863186836242676\n",
      "Warning: nan gradient found. The current loss is:  0.5714020729064941\n",
      "Warning: nan gradient found. The current loss is:  0.3741714656352997\n",
      "Warning: nan gradient found. The current loss is:  1.0215768814086914\n",
      "Warning: nan gradient found. The current loss is:  0.5228849649429321\n",
      "Warning: nan gradient found. The current loss is:  0.9344055652618408\n",
      "Warning: nan gradient found. The current loss is:  1.2326152324676514\n",
      "Warning: nan gradient found. The current loss is:  0.6088767051696777\n",
      "Warning: nan gradient found. The current loss is:  0.33689379692077637\n",
      "Warning: nan gradient found. The current loss is:  0.5209495425224304\n",
      "Warning: nan gradient found. The current loss is:  0.4843602180480957\n",
      "Warning: nan gradient found. The current loss is:  0.5155644416809082\n",
      "Warning: nan gradient found. The current loss is:  0.8720660209655762\n",
      "Warning: nan gradient found. The current loss is:  1.2724727392196655\n",
      "Warning: nan gradient found. The current loss is:  0.9271459579467773\n",
      "Warning: nan gradient found. The current loss is:  0.4824189841747284\n",
      "Warning: nan gradient found. The current loss is:  0.09217120707035065\n",
      "Warning: nan gradient found. The current loss is:  0.20066462457180023\n",
      "Warning: nan gradient found. The current loss is:  1.2890268564224243\n",
      "Warning: nan gradient found. The current loss is:  0.5176501870155334\n",
      "Warning: nan gradient found. The current loss is:  0.7667617201805115\n",
      "Warning: nan gradient found. The current loss is:  0.4717984199523926\n",
      "Warning: nan gradient found. The current loss is:  0.816643476486206\n",
      "Warning: nan gradient found. The current loss is:  0.434816837310791\n",
      "Warning: nan gradient found. The current loss is:  0.17930427193641663\n",
      "Warning: nan gradient found. The current loss is:  1.0072101354599\n",
      "Warning: nan gradient found. The current loss is:  0.761841356754303\n",
      "Warning: nan gradient found. The current loss is:  0.36598068475723267\n",
      "Warning: nan gradient found. The current loss is:  0.6588441729545593\n",
      "Warning: nan gradient found. The current loss is:  0.4007421135902405\n",
      "Warning: nan gradient found. The current loss is:  0.5687943696975708\n",
      "Warning: nan gradient found. The current loss is:  1.1132464408874512\n",
      "Warning: nan gradient found. The current loss is:  0.7797304391860962\n",
      "Warning: nan gradient found. The current loss is:  0.768530547618866\n",
      "Warning: nan gradient found. The current loss is:  1.0089783668518066\n",
      "Warning: nan gradient found. The current loss is:  0.22152715921401978\n",
      "Warning: nan gradient found. The current loss is:  0.6745611429214478\n",
      "Warning: nan gradient found. The current loss is:  0.7720247507095337\n",
      "Warning: nan gradient found. The current loss is:  0.6460193991661072\n",
      "Warning: nan gradient found. The current loss is:  0.48782894015312195\n",
      "Warning: nan gradient found. The current loss is:  1.2497550249099731\n",
      "Warning: nan gradient found. The current loss is:  0.8833445310592651\n",
      "Warning: nan gradient found. The current loss is:  0.8101710677146912\n",
      "Warning: nan gradient found. The current loss is:  1.0222082138061523\n",
      "Warning: nan gradient found. The current loss is:  0.6517422199249268\n",
      "Warning: nan gradient found. The current loss is:  0.19306066632270813\n",
      "Warning: nan gradient found. The current loss is:  0.19009503722190857\n",
      "Warning: nan gradient found. The current loss is:  0.3320636749267578\n",
      "Warning: nan gradient found. The current loss is:  0.7526537179946899\n",
      "Warning: nan gradient found. The current loss is:  0.2532547116279602\n",
      "Warning: nan gradient found. The current loss is:  0.4606369137763977\n",
      "Warning: nan gradient found. The current loss is:  0.3337002694606781\n",
      "Warning: nan gradient found. The current loss is:  2.1977474689483643\n",
      "Warning: nan gradient found. The current loss is:  0.5244269371032715\n",
      "Warning: nan gradient found. The current loss is:  0.3349039852619171\n",
      "Warning: nan gradient found. The current loss is:  0.7856599688529968\n",
      "Warning: nan gradient found. The current loss is:  0.6722754240036011\n",
      "Warning: nan gradient found. The current loss is:  0.4686354398727417\n",
      "Warning: nan gradient found. The current loss is:  0.6172989010810852\n",
      "Warning: nan gradient found. The current loss is:  0.5741980671882629\n",
      "Warning: nan gradient found. The current loss is:  0.7456750869750977\n",
      "Warning: nan gradient found. The current loss is:  0.8377359509468079\n",
      "Warning: nan gradient found. The current loss is:  0.4480500817298889\n",
      "Warning: nan gradient found. The current loss is:  0.5146892666816711\n",
      "Warning: nan gradient found. The current loss is:  0.45476090908050537\n",
      "Warning: nan gradient found. The current loss is:  0.8234091997146606\n",
      "Warning: nan gradient found. The current loss is:  0.5761438012123108\n",
      "Warning: nan gradient found. The current loss is:  1.0952272415161133\n",
      "Warning: nan gradient found. The current loss is:  0.5267452001571655\n",
      "Warning: nan gradient found. The current loss is:  0.44873732328414917\n",
      "Warning: nan gradient found. The current loss is:  0.46717965602874756\n",
      "Warning: nan gradient found. The current loss is:  0.8786555528640747\n",
      "Warning: nan gradient found. The current loss is:  0.3898787498474121\n",
      "Warning: nan gradient found. The current loss is:  0.37574249505996704\n",
      "Warning: nan gradient found. The current loss is:  0.4691559970378876\n",
      "Warning: nan gradient found. The current loss is:  1.3265142440795898\n",
      "Warning: nan gradient found. The current loss is:  0.5191961526870728\n",
      "Warning: nan gradient found. The current loss is:  0.25799307227134705\n",
      "Warning: nan gradient found. The current loss is:  0.8012562394142151\n",
      "Warning: nan gradient found. The current loss is:  0.655314028263092\n",
      "Warning: nan gradient found. The current loss is:  0.6096348166465759\n",
      "Warning: nan gradient found. The current loss is:  0.2765422761440277\n",
      "Warning: nan gradient found. The current loss is:  1.0229791402816772\n",
      "Warning: nan gradient found. The current loss is:  0.005988948047161102\n",
      "Warning: nan gradient found. The current loss is:  1.0964008569717407\n",
      "Warning: nan gradient found. The current loss is:  0.4302345812320709\n",
      "Warning: nan gradient found. The current loss is:  0.2952349781990051\n",
      "Current batch training loss: 0.295235  [1152000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.36299121379852295\n",
      "Warning: nan gradient found. The current loss is:  0.6910221576690674\n",
      "Warning: nan gradient found. The current loss is:  0.1427910327911377\n",
      "Warning: nan gradient found. The current loss is:  0.3721461892127991\n",
      "Warning: nan gradient found. The current loss is:  0.555780827999115\n",
      "Warning: nan gradient found. The current loss is:  0.35322967171669006\n",
      "Warning: nan gradient found. The current loss is:  0.9473326206207275\n",
      "Warning: nan gradient found. The current loss is:  0.6456239819526672\n",
      "Warning: nan gradient found. The current loss is:  0.6068180799484253\n",
      "Warning: nan gradient found. The current loss is:  0.4499773681163788\n",
      "Warning: nan gradient found. The current loss is:  0.7989938855171204\n",
      "Warning: nan gradient found. The current loss is:  0.6601598858833313\n",
      "Warning: nan gradient found. The current loss is:  1.0430740118026733\n",
      "Warning: nan gradient found. The current loss is:  0.9154284000396729\n",
      "Warning: nan gradient found. The current loss is:  1.4911308288574219\n",
      "Warning: nan gradient found. The current loss is:  0.6116083860397339\n",
      "Warning: nan gradient found. The current loss is:  0.5609077215194702\n",
      "Warning: nan gradient found. The current loss is:  0.5936762094497681\n",
      "Warning: nan gradient found. The current loss is:  1.0834314823150635\n",
      "Warning: nan gradient found. The current loss is:  0.4426092207431793\n",
      "Warning: nan gradient found. The current loss is:  1.1576499938964844\n",
      "Warning: nan gradient found. The current loss is:  0.9177064895629883\n",
      "Warning: nan gradient found. The current loss is:  0.9851858615875244\n",
      "Warning: nan gradient found. The current loss is:  1.640918493270874\n",
      "Warning: nan gradient found. The current loss is:  0.18272851407527924\n",
      "Warning: nan gradient found. The current loss is:  0.4861654043197632\n",
      "Warning: nan gradient found. The current loss is:  1.0487161874771118\n",
      "Warning: nan gradient found. The current loss is:  0.496069073677063\n",
      "Warning: nan gradient found. The current loss is:  0.4418032169342041\n",
      "Warning: nan gradient found. The current loss is:  0.7270268797874451\n",
      "Warning: nan gradient found. The current loss is:  0.2580963969230652\n",
      "Warning: nan gradient found. The current loss is:  0.504838228225708\n",
      "Warning: nan gradient found. The current loss is:  1.39890456199646\n",
      "Warning: nan gradient found. The current loss is:  0.9054069519042969\n",
      "Warning: nan gradient found. The current loss is:  0.35167524218559265\n",
      "Warning: nan gradient found. The current loss is:  0.5665233135223389\n",
      "Warning: nan gradient found. The current loss is:  0.7276585102081299\n",
      "Warning: nan gradient found. The current loss is:  2.103691816329956\n",
      "Warning: nan gradient found. The current loss is:  0.806599497795105\n",
      "Warning: nan gradient found. The current loss is:  2.1576030254364014\n",
      "Warning: nan gradient found. The current loss is:  1.00974702835083\n",
      "Warning: nan gradient found. The current loss is:  0.7213015556335449\n",
      "Warning: nan gradient found. The current loss is:  0.2971923351287842\n",
      "Warning: nan gradient found. The current loss is:  0.7648398876190186\n",
      "Warning: nan gradient found. The current loss is:  0.607907772064209\n",
      "Warning: nan gradient found. The current loss is:  1.6316893100738525\n",
      "Warning: nan gradient found. The current loss is:  1.1236424446105957\n",
      "Warning: nan gradient found. The current loss is:  0.6963157653808594\n",
      "Warning: nan gradient found. The current loss is:  0.9006649851799011\n",
      "Warning: nan gradient found. The current loss is:  0.2393098771572113\n",
      "Warning: nan gradient found. The current loss is:  0.9845582246780396\n",
      "Warning: nan gradient found. The current loss is:  0.8856693506240845\n",
      "Warning: nan gradient found. The current loss is:  0.6841113567352295\n",
      "Warning: nan gradient found. The current loss is:  0.24049773812294006\n",
      "Warning: nan gradient found. The current loss is:  0.5479950904846191\n",
      "Warning: nan gradient found. The current loss is:  1.0853418111801147\n",
      "Warning: nan gradient found. The current loss is:  1.1074743270874023\n",
      "Warning: nan gradient found. The current loss is:  0.05145861580967903\n",
      "Warning: nan gradient found. The current loss is:  0.1914437860250473\n",
      "Warning: nan gradient found. The current loss is:  0.40458792448043823\n",
      "Warning: nan gradient found. The current loss is:  0.4158006012439728\n",
      "Warning: nan gradient found. The current loss is:  0.6914252638816833\n",
      "Warning: nan gradient found. The current loss is:  0.21776333451271057\n",
      "Warning: nan gradient found. The current loss is:  0.5216280221939087\n",
      "Warning: nan gradient found. The current loss is:  0.3894043564796448\n",
      "Warning: nan gradient found. The current loss is:  0.394351601600647\n",
      "Warning: nan gradient found. The current loss is:  0.5388402342796326\n",
      "Warning: nan gradient found. The current loss is:  0.8252490162849426\n",
      "Warning: nan gradient found. The current loss is:  1.1018779277801514\n",
      "Warning: nan gradient found. The current loss is:  0.2460465431213379\n",
      "Warning: nan gradient found. The current loss is:  0.7145311832427979\n",
      "Warning: nan gradient found. The current loss is:  0.2822262942790985\n",
      "Warning: nan gradient found. The current loss is:  0.8259990215301514\n",
      "Warning: nan gradient found. The current loss is:  0.5986167788505554\n",
      "Warning: nan gradient found. The current loss is:  1.1170291900634766\n",
      "Warning: nan gradient found. The current loss is:  0.5130294561386108\n",
      "Warning: nan gradient found. The current loss is:  0.6211736798286438\n",
      "Warning: nan gradient found. The current loss is:  1.6251931190490723\n",
      "Warning: nan gradient found. The current loss is:  1.1617333889007568\n",
      "Warning: nan gradient found. The current loss is:  0.16534866392612457\n",
      "Warning: nan gradient found. The current loss is:  0.5719337463378906\n",
      "Warning: nan gradient found. The current loss is:  0.8243557810783386\n",
      "Warning: nan gradient found. The current loss is:  1.1306854486465454\n",
      "Warning: nan gradient found. The current loss is:  0.3054884076118469\n",
      "Warning: nan gradient found. The current loss is:  1.5364887714385986\n",
      "Warning: nan gradient found. The current loss is:  1.0708980560302734\n",
      "Warning: nan gradient found. The current loss is:  0.38818156719207764\n",
      "Warning: nan gradient found. The current loss is:  0.8348784446716309\n",
      "Warning: nan gradient found. The current loss is:  0.8643811941146851\n",
      "Warning: nan gradient found. The current loss is:  1.1969244480133057\n",
      "Warning: nan gradient found. The current loss is:  0.610406756401062\n",
      "Warning: nan gradient found. The current loss is:  0.2455437034368515\n",
      "Warning: nan gradient found. The current loss is:  0.7091599106788635\n",
      "Warning: nan gradient found. The current loss is:  0.593116044998169\n",
      "Warning: nan gradient found. The current loss is:  0.3232634365558624\n",
      "Warning: nan gradient found. The current loss is:  0.3116166889667511\n",
      "Warning: nan gradient found. The current loss is:  0.79474937915802\n",
      "Warning: nan gradient found. The current loss is:  1.1465883255004883\n",
      "Warning: nan gradient found. The current loss is:  0.29512274265289307\n",
      "Warning: nan gradient found. The current loss is:  1.8533978462219238\n",
      "Current batch training loss: 1.853398  [1177600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  2.3347816467285156\n",
      "Warning: nan gradient found. The current loss is:  0.5369127988815308\n",
      "Warning: nan gradient found. The current loss is:  0.7220072150230408\n",
      "Warning: nan gradient found. The current loss is:  0.5059654712677002\n",
      "Warning: nan gradient found. The current loss is:  1.1356197595596313\n",
      "Warning: nan gradient found. The current loss is:  0.5796791911125183\n",
      "Warning: nan gradient found. The current loss is:  0.32434898614883423\n",
      "Warning: nan gradient found. The current loss is:  0.965775728225708\n",
      "Warning: nan gradient found. The current loss is:  0.08240637183189392\n",
      "Warning: nan gradient found. The current loss is:  1.122514009475708\n",
      "Warning: nan gradient found. The current loss is:  0.7592937350273132\n",
      "Warning: nan gradient found. The current loss is:  0.35849529504776\n",
      "Warning: nan gradient found. The current loss is:  0.864891529083252\n",
      "Warning: nan gradient found. The current loss is:  1.0034425258636475\n",
      "Warning: nan gradient found. The current loss is:  0.3154449462890625\n",
      "Warning: nan gradient found. The current loss is:  0.5614412426948547\n",
      "Warning: nan gradient found. The current loss is:  1.0964293479919434\n",
      "Warning: nan gradient found. The current loss is:  0.1408059000968933\n",
      "Warning: nan gradient found. The current loss is:  1.478027582168579\n",
      "Warning: nan gradient found. The current loss is:  0.10980699211359024\n",
      "Warning: nan gradient found. The current loss is:  0.9327740669250488\n",
      "Warning: nan gradient found. The current loss is:  1.7815591096878052\n",
      "Warning: nan gradient found. The current loss is:  0.8666079044342041\n",
      "Warning: nan gradient found. The current loss is:  0.44320279359817505\n",
      "Warning: nan gradient found. The current loss is:  0.2936132252216339\n",
      "Warning: nan gradient found. The current loss is:  0.30024081468582153\n",
      "Warning: nan gradient found. The current loss is:  0.27433890104293823\n",
      "Warning: nan gradient found. The current loss is:  0.7909870743751526\n",
      "Warning: nan gradient found. The current loss is:  0.7926225066184998\n",
      "Warning: nan gradient found. The current loss is:  0.6377902626991272\n",
      "Warning: nan gradient found. The current loss is:  1.1301794052124023\n",
      "Warning: nan gradient found. The current loss is:  1.9009461402893066\n",
      "Warning: nan gradient found. The current loss is:  0.4857787787914276\n",
      "Warning: nan gradient found. The current loss is:  0.39455991983413696\n",
      "Warning: nan gradient found. The current loss is:  0.4875289499759674\n",
      "Warning: nan gradient found. The current loss is:  0.412631630897522\n",
      "Warning: nan gradient found. The current loss is:  1.4659662246704102\n",
      "Warning: nan gradient found. The current loss is:  0.643725574016571\n",
      "Warning: nan gradient found. The current loss is:  0.89408278465271\n",
      "Warning: nan gradient found. The current loss is:  0.5435065627098083\n",
      "Warning: nan gradient found. The current loss is:  0.6251137256622314\n",
      "Warning: nan gradient found. The current loss is:  0.5426244735717773\n",
      "Warning: nan gradient found. The current loss is:  0.39345598220825195\n",
      "Warning: nan gradient found. The current loss is:  0.8047440052032471\n",
      "Warning: nan gradient found. The current loss is:  1.0699633359909058\n",
      "Warning: nan gradient found. The current loss is:  0.7667959928512573\n",
      "Warning: nan gradient found. The current loss is:  0.9559509754180908\n",
      "Warning: nan gradient found. The current loss is:  0.6874648332595825\n",
      "Warning: nan gradient found. The current loss is:  0.36371779441833496\n",
      "Warning: nan gradient found. The current loss is:  2.1202378273010254\n",
      "Warning: nan gradient found. The current loss is:  1.5571173429489136\n",
      "Warning: nan gradient found. The current loss is:  0.4325065314769745\n",
      "Warning: nan gradient found. The current loss is:  0.11314260959625244\n",
      "Warning: nan gradient found. The current loss is:  0.6330046653747559\n",
      "Warning: nan gradient found. The current loss is:  0.5002977848052979\n",
      "Warning: nan gradient found. The current loss is:  0.3936367630958557\n",
      "Warning: nan gradient found. The current loss is:  0.28443676233291626\n",
      "Warning: nan gradient found. The current loss is:  0.17281416058540344\n",
      "Warning: nan gradient found. The current loss is:  0.46739381551742554\n",
      "Warning: nan gradient found. The current loss is:  0.36467689275741577\n",
      "Warning: nan gradient found. The current loss is:  0.9204514026641846\n",
      "Warning: nan gradient found. The current loss is:  1.0503673553466797\n",
      "Warning: nan gradient found. The current loss is:  1.1297005414962769\n",
      "Warning: nan gradient found. The current loss is:  0.9599108695983887\n",
      "Warning: nan gradient found. The current loss is:  0.5385472178459167\n",
      "Warning: nan gradient found. The current loss is:  1.248970627784729\n",
      "Warning: nan gradient found. The current loss is:  1.1088621616363525\n",
      "Warning: nan gradient found. The current loss is:  0.6936085224151611\n",
      "Warning: nan gradient found. The current loss is:  0.450528621673584\n",
      "Warning: nan gradient found. The current loss is:  0.5693261623382568\n",
      "Warning: nan gradient found. The current loss is:  1.160220980644226\n",
      "Warning: nan gradient found. The current loss is:  0.7468977570533752\n",
      "Warning: nan gradient found. The current loss is:  1.1448272466659546\n",
      "Warning: nan gradient found. The current loss is:  0.5385842323303223\n",
      "Warning: nan gradient found. The current loss is:  0.8164914846420288\n",
      "Warning: nan gradient found. The current loss is:  0.928015410900116\n",
      "Warning: nan gradient found. The current loss is:  0.9100958108901978\n",
      "Warning: nan gradient found. The current loss is:  1.1862478256225586\n",
      "Warning: nan gradient found. The current loss is:  0.9285382032394409\n",
      "Warning: nan gradient found. The current loss is:  0.4868427515029907\n",
      "Warning: nan gradient found. The current loss is:  0.6786609888076782\n",
      "Warning: nan gradient found. The current loss is:  0.5479947328567505\n",
      "Warning: nan gradient found. The current loss is:  2.292933702468872\n",
      "Warning: nan gradient found. The current loss is:  0.5332893133163452\n",
      "Warning: nan gradient found. The current loss is:  0.5443551540374756\n",
      "Warning: nan gradient found. The current loss is:  0.37436792254447937\n",
      "Warning: nan gradient found. The current loss is:  0.5253368020057678\n",
      "Warning: nan gradient found. The current loss is:  0.3661481440067291\n",
      "Warning: nan gradient found. The current loss is:  1.6691781282424927\n",
      "Warning: nan gradient found. The current loss is:  0.9036617875099182\n",
      "Warning: nan gradient found. The current loss is:  0.3582260012626648\n",
      "Warning: nan gradient found. The current loss is:  0.34512239694595337\n",
      "Warning: nan gradient found. The current loss is:  1.2411959171295166\n",
      "Warning: nan gradient found. The current loss is:  0.6804760098457336\n",
      "Warning: nan gradient found. The current loss is:  0.6292264461517334\n",
      "Warning: nan gradient found. The current loss is:  1.0781075954437256\n",
      "Warning: nan gradient found. The current loss is:  0.19955435395240784\n",
      "Warning: nan gradient found. The current loss is:  1.0238085985183716\n",
      "Warning: nan gradient found. The current loss is:  0.532320499420166\n",
      "Warning: nan gradient found. The current loss is:  0.6712226867675781\n",
      "Current batch training loss: 0.671223  [1203200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.39207059144973755\n",
      "Warning: nan gradient found. The current loss is:  0.26052582263946533\n",
      "Warning: nan gradient found. The current loss is:  0.5231877565383911\n",
      "Warning: nan gradient found. The current loss is:  1.1342591047286987\n",
      "Warning: nan gradient found. The current loss is:  1.001461386680603\n",
      "Warning: nan gradient found. The current loss is:  0.6706849932670593\n",
      "Warning: nan gradient found. The current loss is:  0.14801406860351562\n",
      "Warning: nan gradient found. The current loss is:  0.7996132373809814\n",
      "Warning: nan gradient found. The current loss is:  0.20349231362342834\n",
      "Warning: nan gradient found. The current loss is:  0.7163958549499512\n",
      "Warning: nan gradient found. The current loss is:  0.44645971059799194\n",
      "Warning: nan gradient found. The current loss is:  0.36986637115478516\n",
      "Warning: nan gradient found. The current loss is:  0.4070560932159424\n",
      "Warning: nan gradient found. The current loss is:  0.5866997241973877\n",
      "Warning: nan gradient found. The current loss is:  0.42269667983055115\n",
      "Warning: nan gradient found. The current loss is:  1.0027055740356445\n",
      "Warning: nan gradient found. The current loss is:  0.6706206798553467\n",
      "Warning: nan gradient found. The current loss is:  0.7430515289306641\n",
      "Warning: nan gradient found. The current loss is:  0.5643136501312256\n",
      "Warning: nan gradient found. The current loss is:  0.45537346601486206\n",
      "Warning: nan gradient found. The current loss is:  0.4339521527290344\n",
      "Warning: nan gradient found. The current loss is:  2.0224227905273438\n",
      "Warning: nan gradient found. The current loss is:  0.9380096197128296\n",
      "Warning: nan gradient found. The current loss is:  1.5383801460266113\n",
      "Warning: nan gradient found. The current loss is:  0.49460697174072266\n",
      "Warning: nan gradient found. The current loss is:  2.7321395874023438\n",
      "Warning: nan gradient found. The current loss is:  0.23992203176021576\n",
      "Warning: nan gradient found. The current loss is:  2.038107395172119\n",
      "Warning: nan gradient found. The current loss is:  0.675394594669342\n",
      "Warning: nan gradient found. The current loss is:  0.33346590399742126\n",
      "Warning: nan gradient found. The current loss is:  0.6390393376350403\n",
      "Warning: nan gradient found. The current loss is:  0.760757565498352\n",
      "Warning: nan gradient found. The current loss is:  0.6652714014053345\n",
      "Warning: nan gradient found. The current loss is:  3.4519224166870117\n",
      "Warning: nan gradient found. The current loss is:  1.284548044204712\n",
      "Warning: nan gradient found. The current loss is:  0.017931416630744934\n",
      "Warning: nan gradient found. The current loss is:  0.9018549919128418\n",
      "Warning: nan gradient found. The current loss is:  0.059781514108181\n",
      "Warning: nan gradient found. The current loss is:  1.3460652828216553\n",
      "Warning: nan gradient found. The current loss is:  1.6852147579193115\n",
      "Warning: nan gradient found. The current loss is:  0.6920910477638245\n",
      "Warning: nan gradient found. The current loss is:  0.5061465501785278\n",
      "Warning: nan gradient found. The current loss is:  1.0574294328689575\n",
      "Warning: nan gradient found. The current loss is:  0.8864492177963257\n",
      "Warning: nan gradient found. The current loss is:  0.3558918237686157\n",
      "Warning: nan gradient found. The current loss is:  0.5789363980293274\n",
      "Warning: nan gradient found. The current loss is:  0.7392685413360596\n",
      "Warning: nan gradient found. The current loss is:  0.7997072339057922\n",
      "Warning: nan gradient found. The current loss is:  0.5189183354377747\n",
      "Warning: nan gradient found. The current loss is:  0.19494006037712097\n",
      "Warning: nan gradient found. The current loss is:  1.0378515720367432\n",
      "Warning: nan gradient found. The current loss is:  0.6690982580184937\n",
      "Warning: nan gradient found. The current loss is:  1.2338385581970215\n",
      "Warning: nan gradient found. The current loss is:  0.3992524743080139\n",
      "Warning: nan gradient found. The current loss is:  0.5451902151107788\n",
      "Warning: nan gradient found. The current loss is:  0.24750743806362152\n",
      "Warning: nan gradient found. The current loss is:  0.17783726751804352\n",
      "Warning: nan gradient found. The current loss is:  0.20966815948486328\n",
      "Warning: nan gradient found. The current loss is:  0.7019386291503906\n",
      "Warning: nan gradient found. The current loss is:  0.9748678207397461\n",
      "Warning: nan gradient found. The current loss is:  0.8780477643013\n",
      "Warning: nan gradient found. The current loss is:  0.2412719428539276\n",
      "Warning: nan gradient found. The current loss is:  0.3767518997192383\n",
      "Warning: nan gradient found. The current loss is:  0.1768769919872284\n",
      "Warning: nan gradient found. The current loss is:  0.3335176706314087\n",
      "Warning: nan gradient found. The current loss is:  0.10307922959327698\n",
      "Warning: nan gradient found. The current loss is:  0.701090931892395\n",
      "Warning: nan gradient found. The current loss is:  0.8904035091400146\n",
      "Warning: nan gradient found. The current loss is:  0.42616987228393555\n",
      "Warning: nan gradient found. The current loss is:  0.5447680354118347\n",
      "Warning: nan gradient found. The current loss is:  0.5283974409103394\n",
      "Warning: nan gradient found. The current loss is:  1.2155221700668335\n",
      "Warning: nan gradient found. The current loss is:  0.6453078389167786\n",
      "Warning: nan gradient found. The current loss is:  0.29755210876464844\n",
      "Warning: nan gradient found. The current loss is:  0.011529266834259033\n",
      "Warning: nan gradient found. The current loss is:  0.4164472222328186\n",
      "Warning: nan gradient found. The current loss is:  0.8677425384521484\n",
      "Warning: nan gradient found. The current loss is:  0.13230806589126587\n",
      "Warning: nan gradient found. The current loss is:  0.3778914213180542\n",
      "Warning: nan gradient found. The current loss is:  0.15201632678508759\n",
      "Warning: nan gradient found. The current loss is:  0.49093228578567505\n",
      "Warning: nan gradient found. The current loss is:  2.6746883392333984\n",
      "Warning: nan gradient found. The current loss is:  0.5823058485984802\n",
      "Warning: nan gradient found. The current loss is:  0.1923120766878128\n",
      "Warning: nan gradient found. The current loss is:  0.757188618183136\n",
      "Warning: nan gradient found. The current loss is:  0.5385124683380127\n",
      "Warning: nan gradient found. The current loss is:  0.39777129888534546\n",
      "Warning: nan gradient found. The current loss is:  0.2976720333099365\n",
      "Warning: nan gradient found. The current loss is:  0.2287101298570633\n",
      "Warning: nan gradient found. The current loss is:  0.8229312896728516\n",
      "Warning: nan gradient found. The current loss is:  0.29724055528640747\n",
      "Warning: nan gradient found. The current loss is:  0.2015174925327301\n",
      "Warning: nan gradient found. The current loss is:  0.5638058185577393\n",
      "Warning: nan gradient found. The current loss is:  0.6301296949386597\n",
      "Warning: nan gradient found. The current loss is:  1.1156606674194336\n",
      "Warning: nan gradient found. The current loss is:  1.0711138248443604\n",
      "Warning: nan gradient found. The current loss is:  0.81544429063797\n",
      "Warning: nan gradient found. The current loss is:  1.1706130504608154\n",
      "Warning: nan gradient found. The current loss is:  0.3066636621952057\n",
      "Warning: nan gradient found. The current loss is:  1.520853042602539\n",
      "Current batch training loss: 1.520853  [1228800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8480492830276489\n",
      "Warning: nan gradient found. The current loss is:  0.4220649302005768\n",
      "Warning: nan gradient found. The current loss is:  0.336006760597229\n",
      "Warning: nan gradient found. The current loss is:  1.2905583381652832\n",
      "Warning: nan gradient found. The current loss is:  0.6619771122932434\n",
      "Warning: nan gradient found. The current loss is:  0.6696641445159912\n",
      "Warning: nan gradient found. The current loss is:  0.2272571325302124\n",
      "Warning: nan gradient found. The current loss is:  0.22673144936561584\n",
      "Warning: nan gradient found. The current loss is:  0.19173595309257507\n",
      "Warning: nan gradient found. The current loss is:  0.32130566239356995\n",
      "Warning: nan gradient found. The current loss is:  1.3704100847244263\n",
      "Warning: nan gradient found. The current loss is:  1.2117555141448975\n",
      "Warning: nan gradient found. The current loss is:  0.6124396324157715\n",
      "Warning: nan gradient found. The current loss is:  0.61298668384552\n",
      "Warning: nan gradient found. The current loss is:  0.9711278080940247\n",
      "Warning: nan gradient found. The current loss is:  0.6053169965744019\n",
      "Warning: nan gradient found. The current loss is:  0.9171010255813599\n",
      "Warning: nan gradient found. The current loss is:  1.192284107208252\n",
      "Warning: nan gradient found. The current loss is:  0.041077107191085815\n",
      "Warning: nan gradient found. The current loss is:  1.487518548965454\n",
      "Warning: nan gradient found. The current loss is:  0.49709683656692505\n",
      "Warning: nan gradient found. The current loss is:  0.4789878726005554\n",
      "Warning: nan gradient found. The current loss is:  0.4436812400817871\n",
      "Warning: nan gradient found. The current loss is:  0.4807383418083191\n",
      "Warning: nan gradient found. The current loss is:  0.603100597858429\n",
      "Warning: nan gradient found. The current loss is:  0.2059708684682846\n",
      "Warning: nan gradient found. The current loss is:  0.7068746089935303\n",
      "Warning: nan gradient found. The current loss is:  0.4805532395839691\n",
      "Warning: nan gradient found. The current loss is:  0.8445159196853638\n",
      "Warning: nan gradient found. The current loss is:  0.4528557360172272\n",
      "Warning: nan gradient found. The current loss is:  0.6978853344917297\n",
      "Warning: nan gradient found. The current loss is:  0.9566096067428589\n",
      "Warning: nan gradient found. The current loss is:  0.8497779369354248\n",
      "Warning: nan gradient found. The current loss is:  0.1307058334350586\n",
      "Warning: nan gradient found. The current loss is:  0.7900514006614685\n",
      "Warning: nan gradient found. The current loss is:  0.33843785524368286\n",
      "Warning: nan gradient found. The current loss is:  0.3051447868347168\n",
      "Warning: nan gradient found. The current loss is:  0.7780426740646362\n",
      "Warning: nan gradient found. The current loss is:  0.45751649141311646\n",
      "Warning: nan gradient found. The current loss is:  1.3623489141464233\n",
      "Warning: nan gradient found. The current loss is:  0.47865068912506104\n",
      "Warning: nan gradient found. The current loss is:  0.41000768542289734\n",
      "Warning: nan gradient found. The current loss is:  1.47628915309906\n",
      "Warning: nan gradient found. The current loss is:  0.993287980556488\n",
      "Warning: nan gradient found. The current loss is:  0.43172264099121094\n",
      "Warning: nan gradient found. The current loss is:  0.22703677415847778\n",
      "Warning: nan gradient found. The current loss is:  0.6134488582611084\n",
      "Warning: nan gradient found. The current loss is:  0.491704523563385\n",
      "Warning: nan gradient found. The current loss is:  -0.010876327753067017\n",
      "Warning: nan gradient found. The current loss is:  1.4227795600891113\n",
      "Warning: nan gradient found. The current loss is:  0.5926777124404907\n",
      "Warning: nan gradient found. The current loss is:  0.5168071389198303\n",
      "Warning: nan gradient found. The current loss is:  0.6225855350494385\n",
      "Warning: nan gradient found. The current loss is:  0.5682415962219238\n",
      "Warning: nan gradient found. The current loss is:  1.0639997720718384\n",
      "Warning: nan gradient found. The current loss is:  0.4322130084037781\n",
      "Warning: nan gradient found. The current loss is:  0.9382207989692688\n",
      "Warning: nan gradient found. The current loss is:  0.60133957862854\n",
      "Warning: nan gradient found. The current loss is:  0.9882916212081909\n",
      "Warning: nan gradient found. The current loss is:  0.7864803671836853\n",
      "Warning: nan gradient found. The current loss is:  0.2786864936351776\n",
      "Warning: nan gradient found. The current loss is:  0.20221693813800812\n",
      "Warning: nan gradient found. The current loss is:  0.6075912714004517\n",
      "Warning: nan gradient found. The current loss is:  0.5718569755554199\n",
      "Warning: nan gradient found. The current loss is:  0.8248459100723267\n",
      "Warning: nan gradient found. The current loss is:  0.9849429130554199\n",
      "Warning: nan gradient found. The current loss is:  0.1776905208826065\n",
      "Warning: nan gradient found. The current loss is:  1.0510413646697998\n",
      "Warning: nan gradient found. The current loss is:  0.6698293685913086\n",
      "Warning: nan gradient found. The current loss is:  1.6398911476135254\n",
      "Warning: nan gradient found. The current loss is:  0.41341906785964966\n",
      "Warning: nan gradient found. The current loss is:  0.5399112105369568\n",
      "Warning: nan gradient found. The current loss is:  0.935241162776947\n",
      "Warning: nan gradient found. The current loss is:  0.09883680939674377\n",
      "Warning: nan gradient found. The current loss is:  0.292312890291214\n",
      "Warning: nan gradient found. The current loss is:  0.1794455349445343\n",
      "Warning: nan gradient found. The current loss is:  0.3793836832046509\n",
      "Warning: nan gradient found. The current loss is:  0.46302035450935364\n",
      "Warning: nan gradient found. The current loss is:  0.5789881348609924\n",
      "Warning: nan gradient found. The current loss is:  0.3129558265209198\n",
      "Warning: nan gradient found. The current loss is:  0.923220157623291\n",
      "Warning: nan gradient found. The current loss is:  0.9437170028686523\n",
      "Warning: nan gradient found. The current loss is:  0.49886012077331543\n",
      "Warning: nan gradient found. The current loss is:  0.862591028213501\n",
      "Warning: nan gradient found. The current loss is:  1.1728885173797607\n",
      "Warning: nan gradient found. The current loss is:  1.103529691696167\n",
      "Warning: nan gradient found. The current loss is:  1.131738305091858\n",
      "Warning: nan gradient found. The current loss is:  0.54399174451828\n",
      "Warning: nan gradient found. The current loss is:  0.7688226699829102\n",
      "Warning: nan gradient found. The current loss is:  0.7230530381202698\n",
      "Warning: nan gradient found. The current loss is:  0.3079858124256134\n",
      "Warning: nan gradient found. The current loss is:  1.4481174945831299\n",
      "Warning: nan gradient found. The current loss is:  0.642079770565033\n",
      "Warning: nan gradient found. The current loss is:  1.0196739435195923\n",
      "Warning: nan gradient found. The current loss is:  0.5803178548812866\n",
      "Warning: nan gradient found. The current loss is:  0.46568578481674194\n",
      "Warning: nan gradient found. The current loss is:  0.21967917680740356\n",
      "Warning: nan gradient found. The current loss is:  1.6748543977737427\n",
      "Warning: nan gradient found. The current loss is:  0.5324604511260986\n",
      "Warning: nan gradient found. The current loss is:  0.8151569366455078\n",
      "Current batch training loss: 0.815157  [1254400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.0128673315048218\n",
      "Warning: nan gradient found. The current loss is:  1.0522925853729248\n",
      "Warning: nan gradient found. The current loss is:  0.5794411897659302\n",
      "Warning: nan gradient found. The current loss is:  1.207176923751831\n",
      "Warning: nan gradient found. The current loss is:  0.5873085856437683\n",
      "Warning: nan gradient found. The current loss is:  0.7892389297485352\n",
      "Warning: nan gradient found. The current loss is:  0.7646083235740662\n",
      "Warning: nan gradient found. The current loss is:  0.9386512041091919\n",
      "Warning: nan gradient found. The current loss is:  0.6461821794509888\n",
      "Warning: nan gradient found. The current loss is:  1.8910099267959595\n",
      "Warning: nan gradient found. The current loss is:  1.1909579038619995\n",
      "Warning: nan gradient found. The current loss is:  0.416561484336853\n",
      "Warning: nan gradient found. The current loss is:  0.23571638762950897\n",
      "Warning: nan gradient found. The current loss is:  0.21834827959537506\n",
      "Warning: nan gradient found. The current loss is:  0.06961148977279663\n",
      "Warning: nan gradient found. The current loss is:  0.619171142578125\n",
      "Warning: nan gradient found. The current loss is:  0.6413326263427734\n",
      "Warning: nan gradient found. The current loss is:  0.36361128091812134\n",
      "Warning: nan gradient found. The current loss is:  0.5354273319244385\n",
      "Warning: nan gradient found. The current loss is:  0.532173752784729\n",
      "Warning: nan gradient found. The current loss is:  0.641737699508667\n",
      "Warning: nan gradient found. The current loss is:  0.13048803806304932\n",
      "Warning: nan gradient found. The current loss is:  -0.09716527163982391\n",
      "Warning: nan gradient found. The current loss is:  0.4587072730064392\n",
      "Warning: nan gradient found. The current loss is:  0.5970708131790161\n",
      "Warning: nan gradient found. The current loss is:  0.5345526337623596\n",
      "Warning: nan gradient found. The current loss is:  1.5782952308654785\n",
      "Warning: nan gradient found. The current loss is:  0.6797584891319275\n",
      "Warning: nan gradient found. The current loss is:  0.9525156021118164\n",
      "Warning: nan gradient found. The current loss is:  0.41050225496292114\n",
      "Warning: nan gradient found. The current loss is:  0.8540165424346924\n",
      "Warning: nan gradient found. The current loss is:  0.6409038305282593\n",
      "Warning: nan gradient found. The current loss is:  0.8049277067184448\n",
      "Warning: nan gradient found. The current loss is:  1.2059119939804077\n",
      "Warning: nan gradient found. The current loss is:  0.5149989128112793\n",
      "Warning: nan gradient found. The current loss is:  0.16112473607063293\n",
      "Warning: nan gradient found. The current loss is:  0.29169073700904846\n",
      "Warning: nan gradient found. The current loss is:  0.20483887195587158\n",
      "Warning: nan gradient found. The current loss is:  0.4421848952770233\n",
      "Warning: nan gradient found. The current loss is:  0.5856859683990479\n",
      "Warning: nan gradient found. The current loss is:  1.313735008239746\n",
      "Warning: nan gradient found. The current loss is:  0.4402541518211365\n",
      "Warning: nan gradient found. The current loss is:  1.883690357208252\n",
      "Warning: nan gradient found. The current loss is:  0.3056721091270447\n",
      "Warning: nan gradient found. The current loss is:  0.8669856786727905\n",
      "Warning: nan gradient found. The current loss is:  0.44054025411605835\n",
      "Warning: nan gradient found. The current loss is:  0.932023286819458\n",
      "Warning: nan gradient found. The current loss is:  0.1583343744277954\n",
      "Warning: nan gradient found. The current loss is:  0.6367897391319275\n",
      "Warning: nan gradient found. The current loss is:  0.3233259320259094\n",
      "Warning: nan gradient found. The current loss is:  1.1389954090118408\n",
      "Warning: nan gradient found. The current loss is:  0.8172198534011841\n",
      "Warning: nan gradient found. The current loss is:  1.1243209838867188\n",
      "Warning: nan gradient found. The current loss is:  1.4279696941375732\n",
      "Warning: nan gradient found. The current loss is:  2.971182107925415\n",
      "Warning: nan gradient found. The current loss is:  0.4786209166049957\n",
      "Warning: nan gradient found. The current loss is:  0.7853289246559143\n",
      "Warning: nan gradient found. The current loss is:  1.2441409826278687\n",
      "Warning: nan gradient found. The current loss is:  0.9743785858154297\n",
      "Warning: nan gradient found. The current loss is:  0.7962930202484131\n",
      "Warning: nan gradient found. The current loss is:  0.17360979318618774\n",
      "Warning: nan gradient found. The current loss is:  0.3382227420806885\n",
      "Warning: nan gradient found. The current loss is:  2.797853469848633\n",
      "Warning: nan gradient found. The current loss is:  1.5237553119659424\n",
      "Warning: nan gradient found. The current loss is:  1.8791954517364502\n",
      "Warning: nan gradient found. The current loss is:  0.8720983266830444\n",
      "Warning: nan gradient found. The current loss is:  0.5607494711875916\n",
      "Warning: nan gradient found. The current loss is:  1.838212013244629\n",
      "Warning: nan gradient found. The current loss is:  0.5961596965789795\n",
      "Warning: nan gradient found. The current loss is:  0.48763805627822876\n",
      "Warning: nan gradient found. The current loss is:  0.23032498359680176\n",
      "Warning: nan gradient found. The current loss is:  1.5302423238754272\n",
      "Warning: nan gradient found. The current loss is:  0.6472449898719788\n",
      "Warning: nan gradient found. The current loss is:  0.47986918687820435\n",
      "Warning: nan gradient found. The current loss is:  0.2584676742553711\n",
      "Warning: nan gradient found. The current loss is:  0.25286921858787537\n",
      "Warning: nan gradient found. The current loss is:  1.5465435981750488\n",
      "Warning: nan gradient found. The current loss is:  0.7546573877334595\n",
      "Warning: nan gradient found. The current loss is:  0.626079797744751\n",
      "Warning: nan gradient found. The current loss is:  1.4513673782348633\n",
      "Warning: nan gradient found. The current loss is:  0.6242330074310303\n",
      "Warning: nan gradient found. The current loss is:  0.5218489170074463\n",
      "Warning: nan gradient found. The current loss is:  0.5135450959205627\n",
      "Warning: nan gradient found. The current loss is:  0.6659752130508423\n",
      "Warning: nan gradient found. The current loss is:  0.4252540171146393\n",
      "Warning: nan gradient found. The current loss is:  0.438678503036499\n",
      "Warning: nan gradient found. The current loss is:  0.6690469980239868\n",
      "Warning: nan gradient found. The current loss is:  0.06887838244438171\n",
      "Warning: nan gradient found. The current loss is:  0.5645791292190552\n",
      "Warning: nan gradient found. The current loss is:  0.3520435690879822\n",
      "Warning: nan gradient found. The current loss is:  0.17609521746635437\n",
      "Warning: nan gradient found. The current loss is:  0.2133145034313202\n",
      "Warning: nan gradient found. The current loss is:  0.3770607113838196\n",
      "Warning: nan gradient found. The current loss is:  0.7840543985366821\n",
      "Warning: nan gradient found. The current loss is:  0.37883460521698\n",
      "Warning: nan gradient found. The current loss is:  0.6536036729812622\n",
      "Warning: nan gradient found. The current loss is:  0.5961072444915771\n",
      "Warning: nan gradient found. The current loss is:  0.30966413021087646\n",
      "Warning: nan gradient found. The current loss is:  0.3704783022403717\n",
      "Warning: nan gradient found. The current loss is:  0.7558966875076294\n",
      "Current batch training loss: 0.755897  [1280000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.1447513103485107\n",
      "Warning: nan gradient found. The current loss is:  0.7864625453948975\n",
      "Warning: nan gradient found. The current loss is:  0.3980952203273773\n",
      "Warning: nan gradient found. The current loss is:  0.24030014872550964\n",
      "Warning: nan gradient found. The current loss is:  0.916079044342041\n",
      "Warning: nan gradient found. The current loss is:  0.7985180616378784\n",
      "Warning: nan gradient found. The current loss is:  0.4210609793663025\n",
      "Warning: nan gradient found. The current loss is:  1.0327224731445312\n",
      "Warning: nan gradient found. The current loss is:  0.9693676233291626\n",
      "Warning: nan gradient found. The current loss is:  0.5891268253326416\n",
      "Warning: nan gradient found. The current loss is:  0.5839212536811829\n",
      "Warning: nan gradient found. The current loss is:  0.6567258834838867\n",
      "Warning: nan gradient found. The current loss is:  0.3082408308982849\n",
      "Warning: nan gradient found. The current loss is:  0.5216189026832581\n",
      "Warning: nan gradient found. The current loss is:  0.5277725458145142\n",
      "Warning: nan gradient found. The current loss is:  0.6143325567245483\n",
      "Warning: nan gradient found. The current loss is:  0.9721188545227051\n",
      "Warning: nan gradient found. The current loss is:  0.41675618290901184\n",
      "Warning: nan gradient found. The current loss is:  0.4624370038509369\n",
      "Warning: nan gradient found. The current loss is:  1.097474455833435\n",
      "Warning: nan gradient found. The current loss is:  0.6167911291122437\n",
      "Warning: nan gradient found. The current loss is:  0.4750795364379883\n",
      "Warning: nan gradient found. The current loss is:  0.707822322845459\n",
      "Warning: nan gradient found. The current loss is:  0.36771160364151\n",
      "Warning: nan gradient found. The current loss is:  0.45319998264312744\n",
      "Warning: nan gradient found. The current loss is:  0.45647236704826355\n",
      "Warning: nan gradient found. The current loss is:  2.5009212493896484\n",
      "Warning: nan gradient found. The current loss is:  1.1820811033248901\n",
      "Warning: nan gradient found. The current loss is:  0.8083414435386658\n",
      "Warning: nan gradient found. The current loss is:  1.678681492805481\n",
      "Warning: nan gradient found. The current loss is:  0.6839854717254639\n",
      "Warning: nan gradient found. The current loss is:  0.5385107398033142\n",
      "Warning: nan gradient found. The current loss is:  0.5415149927139282\n",
      "Warning: nan gradient found. The current loss is:  0.5358781218528748\n",
      "Warning: nan gradient found. The current loss is:  0.3240496516227722\n",
      "Warning: nan gradient found. The current loss is:  1.2058963775634766\n",
      "Warning: nan gradient found. The current loss is:  0.2932601869106293\n",
      "Warning: nan gradient found. The current loss is:  0.17140784859657288\n",
      "Warning: nan gradient found. The current loss is:  0.7376034259796143\n",
      "Warning: nan gradient found. The current loss is:  0.8894995450973511\n",
      "Warning: nan gradient found. The current loss is:  0.1886337399482727\n",
      "Warning: nan gradient found. The current loss is:  0.761762797832489\n",
      "Warning: nan gradient found. The current loss is:  0.5753622055053711\n",
      "Warning: nan gradient found. The current loss is:  1.136256456375122\n",
      "Warning: nan gradient found. The current loss is:  0.6257628798484802\n",
      "Warning: nan gradient found. The current loss is:  0.18473821878433228\n",
      "Warning: nan gradient found. The current loss is:  0.5012482404708862\n",
      "Warning: nan gradient found. The current loss is:  0.14827176928520203\n",
      "Warning: nan gradient found. The current loss is:  0.567942202091217\n",
      "Warning: nan gradient found. The current loss is:  0.3706986904144287\n",
      "Warning: nan gradient found. The current loss is:  0.7479129433631897\n",
      "Warning: nan gradient found. The current loss is:  0.29906079173088074\n",
      "Warning: nan gradient found. The current loss is:  0.9775060415267944\n",
      "Warning: nan gradient found. The current loss is:  0.48117971420288086\n",
      "Warning: nan gradient found. The current loss is:  0.872328519821167\n",
      "Warning: nan gradient found. The current loss is:  0.989042341709137\n",
      "Warning: nan gradient found. The current loss is:  0.4110989272594452\n",
      "Warning: nan gradient found. The current loss is:  0.33444440364837646\n",
      "Warning: nan gradient found. The current loss is:  1.0242542028427124\n",
      "Warning: nan gradient found. The current loss is:  0.7002885341644287\n",
      "Warning: nan gradient found. The current loss is:  0.4487873315811157\n",
      "Warning: nan gradient found. The current loss is:  1.5163938999176025\n",
      "Warning: nan gradient found. The current loss is:  0.31272903084754944\n",
      "Warning: nan gradient found. The current loss is:  0.6505807042121887\n",
      "Warning: nan gradient found. The current loss is:  0.7937461137771606\n",
      "Warning: nan gradient found. The current loss is:  1.2675788402557373\n",
      "Warning: nan gradient found. The current loss is:  0.8422409296035767\n",
      "Warning: nan gradient found. The current loss is:  0.3263300955295563\n",
      "Warning: nan gradient found. The current loss is:  0.7780396938323975\n",
      "Warning: nan gradient found. The current loss is:  0.7001426815986633\n",
      "Warning: nan gradient found. The current loss is:  0.5323902368545532\n",
      "Warning: nan gradient found. The current loss is:  0.3095565140247345\n",
      "Warning: nan gradient found. The current loss is:  1.0116225481033325\n",
      "Warning: nan gradient found. The current loss is:  2.60589337348938\n",
      "Warning: nan gradient found. The current loss is:  0.7369779348373413\n",
      "Warning: nan gradient found. The current loss is:  0.4141605794429779\n",
      "Warning: nan gradient found. The current loss is:  0.24393410980701447\n",
      "Warning: nan gradient found. The current loss is:  0.5005531311035156\n",
      "Warning: nan gradient found. The current loss is:  0.12542235851287842\n",
      "Warning: nan gradient found. The current loss is:  0.5419448018074036\n",
      "Warning: nan gradient found. The current loss is:  0.4690892994403839\n",
      "Warning: nan gradient found. The current loss is:  0.3855612874031067\n",
      "Warning: nan gradient found. The current loss is:  0.5550875663757324\n",
      "Warning: nan gradient found. The current loss is:  0.2378711998462677\n",
      "Warning: nan gradient found. The current loss is:  0.4076117277145386\n",
      "Warning: nan gradient found. The current loss is:  1.6369061470031738\n",
      "Warning: nan gradient found. The current loss is:  0.2398070991039276\n",
      "Warning: nan gradient found. The current loss is:  2.0993173122406006\n",
      "Warning: nan gradient found. The current loss is:  0.3375331461429596\n",
      "Warning: nan gradient found. The current loss is:  1.4281179904937744\n",
      "Warning: nan gradient found. The current loss is:  0.6049988269805908\n",
      "Warning: nan gradient found. The current loss is:  0.6240187883377075\n",
      "Warning: nan gradient found. The current loss is:  0.7135413885116577\n",
      "Warning: nan gradient found. The current loss is:  0.38715827465057373\n",
      "Warning: nan gradient found. The current loss is:  0.6860306262969971\n",
      "Warning: nan gradient found. The current loss is:  0.7965507507324219\n",
      "Warning: nan gradient found. The current loss is:  1.5920355319976807\n",
      "Warning: nan gradient found. The current loss is:  0.5789868831634521\n",
      "Warning: nan gradient found. The current loss is:  1.2893673181533813\n",
      "Warning: nan gradient found. The current loss is:  0.08913485705852509\n",
      "Current batch training loss: 0.089135  [1305600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5364997982978821\n",
      "Warning: nan gradient found. The current loss is:  0.6256355047225952\n",
      "Warning: nan gradient found. The current loss is:  0.5386615991592407\n",
      "Warning: nan gradient found. The current loss is:  0.9378014802932739\n",
      "Warning: nan gradient found. The current loss is:  2.2051258087158203\n",
      "Warning: nan gradient found. The current loss is:  0.27582573890686035\n",
      "Warning: nan gradient found. The current loss is:  0.7121545076370239\n",
      "Warning: nan gradient found. The current loss is:  0.7624416351318359\n",
      "Warning: nan gradient found. The current loss is:  0.019703909754753113\n",
      "Warning: nan gradient found. The current loss is:  0.6554932594299316\n",
      "Warning: nan gradient found. The current loss is:  0.947136402130127\n",
      "Warning: nan gradient found. The current loss is:  0.698296308517456\n",
      "Warning: nan gradient found. The current loss is:  0.38062983751296997\n",
      "Warning: nan gradient found. The current loss is:  0.0305977463722229\n",
      "Warning: nan gradient found. The current loss is:  0.7801856994628906\n",
      "Warning: nan gradient found. The current loss is:  1.7777107954025269\n",
      "Warning: nan gradient found. The current loss is:  0.1699504256248474\n",
      "Warning: nan gradient found. The current loss is:  0.6838425397872925\n",
      "Warning: nan gradient found. The current loss is:  1.388509750366211\n",
      "Warning: nan gradient found. The current loss is:  0.6613349914550781\n",
      "Warning: nan gradient found. The current loss is:  0.3373410999774933\n",
      "Warning: nan gradient found. The current loss is:  0.5195473432540894\n",
      "Warning: nan gradient found. The current loss is:  0.33710432052612305\n",
      "Warning: nan gradient found. The current loss is:  0.4573279321193695\n",
      "Warning: nan gradient found. The current loss is:  0.4709257483482361\n",
      "Warning: nan gradient found. The current loss is:  0.858366847038269\n",
      "Warning: nan gradient found. The current loss is:  0.417662650346756\n",
      "Warning: nan gradient found. The current loss is:  0.4717346429824829\n",
      "Warning: nan gradient found. The current loss is:  0.9682192206382751\n",
      "Warning: nan gradient found. The current loss is:  0.6708309054374695\n",
      "Warning: nan gradient found. The current loss is:  0.8647924661636353\n",
      "Warning: nan gradient found. The current loss is:  0.46347135305404663\n",
      "Warning: nan gradient found. The current loss is:  0.4843963086605072\n",
      "Warning: nan gradient found. The current loss is:  0.38002172112464905\n",
      "Warning: nan gradient found. The current loss is:  0.5166020393371582\n",
      "Warning: nan gradient found. The current loss is:  0.22875076532363892\n",
      "Warning: nan gradient found. The current loss is:  1.1372555494308472\n",
      "Warning: nan gradient found. The current loss is:  0.4080921411514282\n",
      "Warning: nan gradient found. The current loss is:  0.2786090075969696\n",
      "Warning: nan gradient found. The current loss is:  0.42074447870254517\n",
      "Warning: nan gradient found. The current loss is:  1.346245288848877\n",
      "Warning: nan gradient found. The current loss is:  0.7307331562042236\n",
      "Warning: nan gradient found. The current loss is:  1.1515765190124512\n",
      "Warning: nan gradient found. The current loss is:  1.492606282234192\n",
      "Warning: nan gradient found. The current loss is:  0.6099343299865723\n",
      "Warning: nan gradient found. The current loss is:  0.19739499688148499\n",
      "Warning: nan gradient found. The current loss is:  0.10498802363872528\n",
      "Warning: nan gradient found. The current loss is:  0.3249051868915558\n",
      "Warning: nan gradient found. The current loss is:  1.5306869745254517\n",
      "Warning: nan gradient found. The current loss is:  0.20770227909088135\n",
      "Warning: nan gradient found. The current loss is:  0.2825946509838104\n",
      "Warning: nan gradient found. The current loss is:  1.034052848815918\n",
      "Warning: nan gradient found. The current loss is:  1.6541191339492798\n",
      "Warning: nan gradient found. The current loss is:  0.4092405438423157\n",
      "Warning: nan gradient found. The current loss is:  0.5032726526260376\n",
      "Warning: nan gradient found. The current loss is:  0.6587732434272766\n",
      "Warning: nan gradient found. The current loss is:  1.7153191566467285\n",
      "Warning: nan gradient found. The current loss is:  0.4586884379386902\n",
      "Warning: nan gradient found. The current loss is:  1.2108814716339111\n",
      "Warning: nan gradient found. The current loss is:  0.7322230935096741\n",
      "Warning: nan gradient found. The current loss is:  0.6988184452056885\n",
      "Warning: nan gradient found. The current loss is:  0.41455143690109253\n",
      "Warning: nan gradient found. The current loss is:  1.380124807357788\n",
      "Warning: nan gradient found. The current loss is:  0.8464847803115845\n",
      "Warning: nan gradient found. The current loss is:  0.7325936555862427\n",
      "Warning: nan gradient found. The current loss is:  0.6654450297355652\n",
      "Warning: nan gradient found. The current loss is:  0.3202211856842041\n",
      "Warning: nan gradient found. The current loss is:  0.49400460720062256\n",
      "Warning: nan gradient found. The current loss is:  0.20059645175933838\n",
      "Warning: nan gradient found. The current loss is:  1.0493654012680054\n",
      "Warning: nan gradient found. The current loss is:  0.6169906854629517\n",
      "Warning: nan gradient found. The current loss is:  0.97611403465271\n",
      "Warning: nan gradient found. The current loss is:  0.8091757297515869\n",
      "Warning: nan gradient found. The current loss is:  0.7264964580535889\n",
      "Warning: nan gradient found. The current loss is:  0.518012285232544\n",
      "Warning: nan gradient found. The current loss is:  0.6473847031593323\n",
      "Warning: nan gradient found. The current loss is:  1.1937408447265625\n",
      "Warning: nan gradient found. The current loss is:  0.677871823310852\n",
      "Warning: nan gradient found. The current loss is:  0.5158504247665405\n",
      "Warning: nan gradient found. The current loss is:  0.9806814789772034\n",
      "Warning: nan gradient found. The current loss is:  0.3642861843109131\n",
      "Warning: nan gradient found. The current loss is:  0.7159168720245361\n",
      "Warning: nan gradient found. The current loss is:  0.4805911183357239\n",
      "Warning: nan gradient found. The current loss is:  0.7121660709381104\n",
      "Warning: nan gradient found. The current loss is:  0.5735259056091309\n",
      "Warning: nan gradient found. The current loss is:  0.7410948872566223\n",
      "Warning: nan gradient found. The current loss is:  0.8683440685272217\n",
      "Warning: nan gradient found. The current loss is:  1.1384023427963257\n",
      "Warning: nan gradient found. The current loss is:  0.4772820472717285\n",
      "Warning: nan gradient found. The current loss is:  0.5793972611427307\n",
      "Warning: nan gradient found. The current loss is:  0.8355208039283752\n",
      "Warning: nan gradient found. The current loss is:  0.22997403144836426\n",
      "Warning: nan gradient found. The current loss is:  0.9247385263442993\n",
      "Warning: nan gradient found. The current loss is:  0.6177118420600891\n",
      "Warning: nan gradient found. The current loss is:  2.297882080078125\n",
      "Warning: nan gradient found. The current loss is:  0.4626719057559967\n",
      "Warning: nan gradient found. The current loss is:  0.9058737754821777\n",
      "Warning: nan gradient found. The current loss is:  0.4821479916572571\n",
      "Warning: nan gradient found. The current loss is:  0.7759712934494019\n",
      "Warning: nan gradient found. The current loss is:  0.776238203048706\n",
      "Current batch training loss: 0.776238  [1331200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.3693568706512451\n",
      "Warning: nan gradient found. The current loss is:  0.43281880021095276\n",
      "Warning: nan gradient found. The current loss is:  0.4271887540817261\n",
      "Warning: nan gradient found. The current loss is:  0.9153452515602112\n",
      "Warning: nan gradient found. The current loss is:  1.8778188228607178\n",
      "Warning: nan gradient found. The current loss is:  0.7349632978439331\n",
      "Warning: nan gradient found. The current loss is:  0.7945639491081238\n",
      "Warning: nan gradient found. The current loss is:  -0.09119845926761627\n",
      "Warning: nan gradient found. The current loss is:  0.28272387385368347\n",
      "Warning: nan gradient found. The current loss is:  0.9751825332641602\n",
      "Warning: nan gradient found. The current loss is:  0.8308145999908447\n",
      "Warning: nan gradient found. The current loss is:  0.4427197575569153\n",
      "Warning: nan gradient found. The current loss is:  0.4371353089809418\n",
      "Warning: nan gradient found. The current loss is:  0.3448137044906616\n",
      "Warning: nan gradient found. The current loss is:  0.17617635428905487\n",
      "Warning: nan gradient found. The current loss is:  0.5675225257873535\n",
      "Warning: nan gradient found. The current loss is:  0.880823016166687\n",
      "Warning: nan gradient found. The current loss is:  0.7182761430740356\n",
      "Warning: nan gradient found. The current loss is:  0.9075359106063843\n",
      "Warning: nan gradient found. The current loss is:  1.61605703830719\n",
      "Warning: nan gradient found. The current loss is:  0.39928704500198364\n",
      "Warning: nan gradient found. The current loss is:  0.5951755046844482\n",
      "Warning: nan gradient found. The current loss is:  0.7374707460403442\n",
      "Warning: nan gradient found. The current loss is:  0.5778288245201111\n",
      "Warning: nan gradient found. The current loss is:  1.4444434642791748\n",
      "Warning: nan gradient found. The current loss is:  1.4089566469192505\n",
      "Warning: nan gradient found. The current loss is:  1.321141004562378\n",
      "Warning: nan gradient found. The current loss is:  0.7298128604888916\n",
      "Warning: nan gradient found. The current loss is:  0.5061871409416199\n",
      "Warning: nan gradient found. The current loss is:  0.6080063581466675\n",
      "Warning: nan gradient found. The current loss is:  1.0567634105682373\n",
      "Warning: nan gradient found. The current loss is:  0.22155331075191498\n",
      "Warning: nan gradient found. The current loss is:  0.529052734375\n",
      "Warning: nan gradient found. The current loss is:  0.680023193359375\n",
      "Warning: nan gradient found. The current loss is:  0.4079163074493408\n",
      "Warning: nan gradient found. The current loss is:  0.7978038787841797\n",
      "Warning: nan gradient found. The current loss is:  0.9841020107269287\n",
      "Warning: nan gradient found. The current loss is:  0.5916688442230225\n",
      "Warning: nan gradient found. The current loss is:  0.12828661501407623\n",
      "Warning: nan gradient found. The current loss is:  0.17372289299964905\n",
      "Warning: nan gradient found. The current loss is:  0.43767404556274414\n",
      "Warning: nan gradient found. The current loss is:  0.6353574991226196\n",
      "Warning: nan gradient found. The current loss is:  1.0386967658996582\n",
      "Warning: nan gradient found. The current loss is:  0.5687869787216187\n",
      "Warning: nan gradient found. The current loss is:  0.8325560688972473\n",
      "Warning: nan gradient found. The current loss is:  0.3730105757713318\n",
      "Warning: nan gradient found. The current loss is:  0.4098750352859497\n",
      "Warning: nan gradient found. The current loss is:  0.7763004302978516\n",
      "Warning: nan gradient found. The current loss is:  0.28043296933174133\n",
      "Warning: nan gradient found. The current loss is:  0.6565109491348267\n",
      "Warning: nan gradient found. The current loss is:  0.8091869354248047\n",
      "Warning: nan gradient found. The current loss is:  0.9800853729248047\n",
      "Warning: nan gradient found. The current loss is:  0.9048422574996948\n",
      "Warning: nan gradient found. The current loss is:  0.8350817561149597\n",
      "Warning: nan gradient found. The current loss is:  0.20856505632400513\n",
      "Warning: nan gradient found. The current loss is:  0.7309508323669434\n",
      "Warning: nan gradient found. The current loss is:  0.19422189891338348\n",
      "Warning: nan gradient found. The current loss is:  0.28770509362220764\n",
      "Warning: nan gradient found. The current loss is:  1.2338876724243164\n",
      "Warning: nan gradient found. The current loss is:  0.875464916229248\n",
      "Warning: nan gradient found. The current loss is:  0.971727192401886\n",
      "Warning: nan gradient found. The current loss is:  0.605509877204895\n",
      "Warning: nan gradient found. The current loss is:  1.250074863433838\n",
      "Warning: nan gradient found. The current loss is:  0.39977675676345825\n",
      "Warning: nan gradient found. The current loss is:  1.6989632844924927\n",
      "Warning: nan gradient found. The current loss is:  0.6696093082427979\n",
      "Warning: nan gradient found. The current loss is:  0.7159662246704102\n",
      "Warning: nan gradient found. The current loss is:  0.3084562420845032\n",
      "Warning: nan gradient found. The current loss is:  0.7621971964836121\n",
      "Warning: nan gradient found. The current loss is:  0.2258325219154358\n",
      "Warning: nan gradient found. The current loss is:  0.4354652166366577\n",
      "Warning: nan gradient found. The current loss is:  0.5542955994606018\n",
      "Warning: nan gradient found. The current loss is:  0.6443536281585693\n",
      "Warning: nan gradient found. The current loss is:  0.5469502210617065\n",
      "Warning: nan gradient found. The current loss is:  0.4460659623146057\n",
      "Warning: nan gradient found. The current loss is:  0.3000238835811615\n",
      "Warning: nan gradient found. The current loss is:  0.6334713697433472\n",
      "Warning: nan gradient found. The current loss is:  0.8752809166908264\n",
      "Warning: nan gradient found. The current loss is:  0.3419524133205414\n",
      "Warning: nan gradient found. The current loss is:  1.1767005920410156\n",
      "Warning: nan gradient found. The current loss is:  0.321848064661026\n",
      "Warning: nan gradient found. The current loss is:  0.47630468010902405\n",
      "Warning: nan gradient found. The current loss is:  0.2881832718849182\n",
      "Warning: nan gradient found. The current loss is:  0.5830526351928711\n",
      "Warning: nan gradient found. The current loss is:  0.32848310470581055\n",
      "Warning: nan gradient found. The current loss is:  0.9731228351593018\n",
      "Warning: nan gradient found. The current loss is:  0.4966505169868469\n",
      "Warning: nan gradient found. The current loss is:  0.7172648906707764\n",
      "Warning: nan gradient found. The current loss is:  0.9004825949668884\n",
      "Warning: nan gradient found. The current loss is:  1.6331746578216553\n",
      "Warning: nan gradient found. The current loss is:  0.5857259631156921\n",
      "Warning: nan gradient found. The current loss is:  0.7411791086196899\n",
      "Warning: nan gradient found. The current loss is:  0.8847616910934448\n",
      "Warning: nan gradient found. The current loss is:  0.9694365859031677\n",
      "Warning: nan gradient found. The current loss is:  1.5216922760009766\n",
      "Warning: nan gradient found. The current loss is:  0.6587743163108826\n",
      "Warning: nan gradient found. The current loss is:  0.2389199137687683\n",
      "Warning: nan gradient found. The current loss is:  1.027536153793335\n",
      "Warning: nan gradient found. The current loss is:  0.5715130567550659\n",
      "Warning: nan gradient found. The current loss is:  0.6448721885681152\n",
      "Current batch training loss: 0.644872  [1356800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8392221927642822\n",
      "Warning: nan gradient found. The current loss is:  1.2901663780212402\n",
      "Warning: nan gradient found. The current loss is:  0.3346192240715027\n",
      "Warning: nan gradient found. The current loss is:  0.8226868510246277\n",
      "Warning: nan gradient found. The current loss is:  0.5894213318824768\n",
      "Warning: nan gradient found. The current loss is:  0.5010548830032349\n",
      "Warning: nan gradient found. The current loss is:  0.580655574798584\n",
      "Warning: nan gradient found. The current loss is:  0.9179279208183289\n",
      "Warning: nan gradient found. The current loss is:  0.4795273244380951\n",
      "Warning: nan gradient found. The current loss is:  0.38439062237739563\n",
      "Warning: nan gradient found. The current loss is:  0.7372466325759888\n",
      "Warning: nan gradient found. The current loss is:  0.6264366507530212\n",
      "Warning: nan gradient found. The current loss is:  1.1708176136016846\n",
      "Warning: nan gradient found. The current loss is:  0.5856322646141052\n",
      "Warning: nan gradient found. The current loss is:  1.008028268814087\n",
      "Warning: nan gradient found. The current loss is:  0.5173121690750122\n",
      "Warning: nan gradient found. The current loss is:  0.4989004135131836\n",
      "Warning: nan gradient found. The current loss is:  0.9626051783561707\n",
      "Warning: nan gradient found. The current loss is:  0.5410057902336121\n",
      "Warning: nan gradient found. The current loss is:  0.5101885795593262\n",
      "Warning: nan gradient found. The current loss is:  0.9511129856109619\n",
      "Warning: nan gradient found. The current loss is:  0.6667003631591797\n",
      "Warning: nan gradient found. The current loss is:  0.3669034540653229\n",
      "Warning: nan gradient found. The current loss is:  0.28255265951156616\n",
      "Warning: nan gradient found. The current loss is:  0.9434984922409058\n",
      "Warning: nan gradient found. The current loss is:  0.7381627559661865\n",
      "Warning: nan gradient found. The current loss is:  0.9215102195739746\n",
      "Warning: nan gradient found. The current loss is:  1.7130064964294434\n",
      "Warning: nan gradient found. The current loss is:  0.4298248291015625\n",
      "Warning: nan gradient found. The current loss is:  0.9457438588142395\n",
      "Warning: nan gradient found. The current loss is:  0.7756020426750183\n",
      "Warning: nan gradient found. The current loss is:  1.1047401428222656\n",
      "Warning: nan gradient found. The current loss is:  0.6467560529708862\n",
      "Warning: nan gradient found. The current loss is:  0.18501153588294983\n",
      "Warning: nan gradient found. The current loss is:  0.09269052743911743\n",
      "Warning: nan gradient found. The current loss is:  0.26151302456855774\n",
      "Warning: nan gradient found. The current loss is:  0.3109573721885681\n",
      "Warning: nan gradient found. The current loss is:  0.7430845499038696\n",
      "Warning: nan gradient found. The current loss is:  1.0535497665405273\n",
      "Warning: nan gradient found. The current loss is:  0.0705900490283966\n",
      "Warning: nan gradient found. The current loss is:  0.8806288242340088\n",
      "Warning: nan gradient found. The current loss is:  0.5833451151847839\n",
      "Warning: nan gradient found. The current loss is:  0.4509594440460205\n",
      "Warning: nan gradient found. The current loss is:  0.5303642749786377\n",
      "Warning: nan gradient found. The current loss is:  0.39587223529815674\n",
      "Warning: nan gradient found. The current loss is:  0.5737303495407104\n",
      "Warning: nan gradient found. The current loss is:  0.14710092544555664\n",
      "Warning: nan gradient found. The current loss is:  1.2486939430236816\n",
      "Warning: nan gradient found. The current loss is:  0.8805756568908691\n",
      "Warning: nan gradient found. The current loss is:  0.6483138203620911\n",
      "Warning: nan gradient found. The current loss is:  0.5417262315750122\n",
      "Warning: nan gradient found. The current loss is:  1.2357127666473389\n",
      "Warning: nan gradient found. The current loss is:  0.9783940315246582\n",
      "Warning: nan gradient found. The current loss is:  0.600914716720581\n",
      "Warning: nan gradient found. The current loss is:  0.9187387228012085\n",
      "Warning: nan gradient found. The current loss is:  0.33412936329841614\n",
      "Warning: nan gradient found. The current loss is:  0.30367785692214966\n",
      "Warning: nan gradient found. The current loss is:  0.4413096308708191\n",
      "Warning: nan gradient found. The current loss is:  2.182382345199585\n",
      "Warning: nan gradient found. The current loss is:  0.5358794927597046\n",
      "Warning: nan gradient found. The current loss is:  0.5134031772613525\n",
      "Warning: nan gradient found. The current loss is:  1.8851678371429443\n",
      "Warning: nan gradient found. The current loss is:  0.7631491422653198\n",
      "Warning: nan gradient found. The current loss is:  0.6650229096412659\n",
      "Warning: nan gradient found. The current loss is:  0.25185367465019226\n",
      "Warning: nan gradient found. The current loss is:  0.8108775615692139\n",
      "Warning: nan gradient found. The current loss is:  0.3740442991256714\n",
      "Warning: nan gradient found. The current loss is:  0.7405037879943848\n",
      "Warning: nan gradient found. The current loss is:  0.26960378885269165\n",
      "Warning: nan gradient found. The current loss is:  0.823799729347229\n",
      "Warning: nan gradient found. The current loss is:  0.6811768412590027\n",
      "Warning: nan gradient found. The current loss is:  0.6359177827835083\n",
      "Warning: nan gradient found. The current loss is:  0.6938015222549438\n",
      "Warning: nan gradient found. The current loss is:  0.17281591892242432\n",
      "Warning: nan gradient found. The current loss is:  0.80683434009552\n",
      "Warning: nan gradient found. The current loss is:  0.5473116040229797\n",
      "Warning: nan gradient found. The current loss is:  0.47250479459762573\n",
      "Warning: nan gradient found. The current loss is:  1.0081512928009033\n",
      "Warning: nan gradient found. The current loss is:  0.9685434103012085\n",
      "Warning: nan gradient found. The current loss is:  0.5110529065132141\n",
      "Warning: nan gradient found. The current loss is:  0.5404624938964844\n",
      "Warning: nan gradient found. The current loss is:  0.9144684672355652\n",
      "Warning: nan gradient found. The current loss is:  0.5903588533401489\n",
      "Warning: nan gradient found. The current loss is:  0.516888439655304\n",
      "Warning: nan gradient found. The current loss is:  0.7026107311248779\n",
      "Warning: nan gradient found. The current loss is:  1.0346660614013672\n",
      "Warning: nan gradient found. The current loss is:  0.03229416906833649\n",
      "Warning: nan gradient found. The current loss is:  0.618930459022522\n",
      "Warning: nan gradient found. The current loss is:  0.9781762361526489\n",
      "Warning: nan gradient found. The current loss is:  0.5778405070304871\n",
      "Warning: nan gradient found. The current loss is:  0.5353617668151855\n",
      "Warning: nan gradient found. The current loss is:  0.5542351007461548\n",
      "Warning: nan gradient found. The current loss is:  0.539044201374054\n",
      "Warning: nan gradient found. The current loss is:  0.47738116979599\n",
      "Warning: nan gradient found. The current loss is:  1.031845211982727\n",
      "Warning: nan gradient found. The current loss is:  0.9289107918739319\n",
      "Warning: nan gradient found. The current loss is:  0.47501686215400696\n",
      "Warning: nan gradient found. The current loss is:  1.2677512168884277\n",
      "Warning: nan gradient found. The current loss is:  0.9090621471405029\n",
      "Warning: nan gradient found. The current loss is:  0.5791244506835938\n",
      "Current batch training loss: 0.579124  [1382400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5868955850601196\n",
      "Warning: nan gradient found. The current loss is:  0.013358231633901596\n",
      "Warning: nan gradient found. The current loss is:  1.2384036779403687\n",
      "Warning: nan gradient found. The current loss is:  1.0432677268981934\n",
      "Warning: nan gradient found. The current loss is:  1.16777765750885\n",
      "Warning: nan gradient found. The current loss is:  0.50225430727005\n",
      "Warning: nan gradient found. The current loss is:  0.44837284088134766\n",
      "Warning: nan gradient found. The current loss is:  0.3603644371032715\n",
      "Warning: nan gradient found. The current loss is:  1.1616361141204834\n",
      "Warning: nan gradient found. The current loss is:  0.8563880920410156\n",
      "Warning: nan gradient found. The current loss is:  0.7760050296783447\n",
      "Warning: nan gradient found. The current loss is:  0.6124017238616943\n",
      "Warning: nan gradient found. The current loss is:  0.28169119358062744\n",
      "Warning: nan gradient found. The current loss is:  0.9284480810165405\n",
      "Warning: nan gradient found. The current loss is:  0.42614296078681946\n",
      "Warning: nan gradient found. The current loss is:  0.3787621855735779\n",
      "Warning: nan gradient found. The current loss is:  0.7085585594177246\n",
      "Warning: nan gradient found. The current loss is:  0.9141296744346619\n",
      "Warning: nan gradient found. The current loss is:  0.7508661150932312\n",
      "Warning: nan gradient found. The current loss is:  1.093925952911377\n",
      "Warning: nan gradient found. The current loss is:  1.0357388257980347\n",
      "Warning: nan gradient found. The current loss is:  1.2297043800354004\n",
      "Warning: nan gradient found. The current loss is:  0.4814949333667755\n",
      "Warning: nan gradient found. The current loss is:  0.5116418600082397\n",
      "Warning: nan gradient found. The current loss is:  0.7038190364837646\n",
      "Warning: nan gradient found. The current loss is:  0.4757288694381714\n",
      "Warning: nan gradient found. The current loss is:  0.4485973119735718\n",
      "Warning: nan gradient found. The current loss is:  0.37842726707458496\n",
      "Warning: nan gradient found. The current loss is:  0.6585532426834106\n",
      "Warning: nan gradient found. The current loss is:  1.4560918807983398\n",
      "Warning: nan gradient found. The current loss is:  1.6652421951293945\n",
      "Warning: nan gradient found. The current loss is:  0.6919261813163757\n",
      "Warning: nan gradient found. The current loss is:  1.1768100261688232\n",
      "Warning: nan gradient found. The current loss is:  0.18837007880210876\n",
      "Warning: nan gradient found. The current loss is:  0.871885359287262\n",
      "Warning: nan gradient found. The current loss is:  0.6235328912734985\n",
      "Warning: nan gradient found. The current loss is:  1.129340648651123\n",
      "Warning: nan gradient found. The current loss is:  0.01169801875948906\n",
      "Warning: nan gradient found. The current loss is:  0.2765018343925476\n",
      "Warning: nan gradient found. The current loss is:  0.2865786850452423\n",
      "Warning: nan gradient found. The current loss is:  0.48939377069473267\n",
      "Warning: nan gradient found. The current loss is:  0.6962111592292786\n",
      "Warning: nan gradient found. The current loss is:  0.8280047178268433\n",
      "Warning: nan gradient found. The current loss is:  0.2029510885477066\n",
      "Warning: nan gradient found. The current loss is:  0.8386969566345215\n",
      "Warning: nan gradient found. The current loss is:  0.8153014779090881\n",
      "Warning: nan gradient found. The current loss is:  0.6655154228210449\n",
      "Warning: nan gradient found. The current loss is:  1.4209402799606323\n",
      "Warning: nan gradient found. The current loss is:  0.9150863885879517\n",
      "Warning: nan gradient found. The current loss is:  0.6812937259674072\n",
      "Warning: nan gradient found. The current loss is:  0.5899897813796997\n",
      "Warning: nan gradient found. The current loss is:  1.0423181056976318\n",
      "Warning: nan gradient found. The current loss is:  0.46757370233535767\n",
      "Warning: nan gradient found. The current loss is:  0.14791378378868103\n",
      "Warning: nan gradient found. The current loss is:  0.2408708930015564\n",
      "Warning: nan gradient found. The current loss is:  0.7956784963607788\n",
      "Warning: nan gradient found. The current loss is:  0.9868461489677429\n",
      "Warning: nan gradient found. The current loss is:  0.8339731693267822\n",
      "Warning: nan gradient found. The current loss is:  0.18167461454868317\n",
      "Warning: nan gradient found. The current loss is:  0.9377468824386597\n",
      "Warning: nan gradient found. The current loss is:  -0.09965254366397858\n",
      "Warning: nan gradient found. The current loss is:  1.1759650707244873\n",
      "Warning: nan gradient found. The current loss is:  0.5657489895820618\n",
      "Warning: nan gradient found. The current loss is:  1.087449550628662\n",
      "Warning: nan gradient found. The current loss is:  1.3007533550262451\n",
      "Warning: nan gradient found. The current loss is:  1.0657271146774292\n",
      "Warning: nan gradient found. The current loss is:  0.43842148780822754\n",
      "Warning: nan gradient found. The current loss is:  0.49292412400245667\n",
      "Warning: nan gradient found. The current loss is:  1.0525048971176147\n",
      "Warning: nan gradient found. The current loss is:  0.8761811256408691\n",
      "Warning: nan gradient found. The current loss is:  0.5695146322250366\n",
      "Warning: nan gradient found. The current loss is:  0.05068186670541763\n",
      "Warning: nan gradient found. The current loss is:  0.9047274589538574\n",
      "Warning: nan gradient found. The current loss is:  0.5805063843727112\n",
      "Warning: nan gradient found. The current loss is:  0.565272331237793\n",
      "Warning: nan gradient found. The current loss is:  0.36739110946655273\n",
      "Warning: nan gradient found. The current loss is:  0.23462820053100586\n",
      "Warning: nan gradient found. The current loss is:  0.7245198488235474\n",
      "Warning: nan gradient found. The current loss is:  0.6238773465156555\n",
      "Warning: nan gradient found. The current loss is:  0.5074700117111206\n",
      "Warning: nan gradient found. The current loss is:  0.3561250567436218\n",
      "Warning: nan gradient found. The current loss is:  1.3038347959518433\n",
      "Warning: nan gradient found. The current loss is:  0.5948968529701233\n",
      "Warning: nan gradient found. The current loss is:  1.6899863481521606\n",
      "Warning: nan gradient found. The current loss is:  0.24953874945640564\n",
      "Warning: nan gradient found. The current loss is:  0.5418146848678589\n",
      "Warning: nan gradient found. The current loss is:  0.8751773834228516\n",
      "Warning: nan gradient found. The current loss is:  0.9341409206390381\n",
      "Warning: nan gradient found. The current loss is:  0.8357057571411133\n",
      "Warning: nan gradient found. The current loss is:  0.5253369808197021\n",
      "Warning: nan gradient found. The current loss is:  0.9111278057098389\n",
      "Warning: nan gradient found. The current loss is:  0.40884625911712646\n",
      "Warning: nan gradient found. The current loss is:  0.20758682489395142\n",
      "Warning: nan gradient found. The current loss is:  0.6286941766738892\n",
      "Warning: nan gradient found. The current loss is:  0.889410674571991\n",
      "Warning: nan gradient found. The current loss is:  0.6278682351112366\n",
      "Warning: nan gradient found. The current loss is:  0.3232743740081787\n",
      "Warning: nan gradient found. The current loss is:  0.5471176505088806\n",
      "Warning: nan gradient found. The current loss is:  1.4123541116714478\n",
      "Warning: nan gradient found. The current loss is:  0.2909476161003113\n",
      "Current batch training loss: 0.290948  [1408000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.2411058247089386\n",
      "Warning: nan gradient found. The current loss is:  0.4911668300628662\n",
      "Warning: nan gradient found. The current loss is:  1.048169493675232\n",
      "Warning: nan gradient found. The current loss is:  0.9707473516464233\n",
      "Warning: nan gradient found. The current loss is:  0.3538194000720978\n",
      "Warning: nan gradient found. The current loss is:  0.5954126715660095\n",
      "Warning: nan gradient found. The current loss is:  0.9959437847137451\n",
      "Warning: nan gradient found. The current loss is:  0.3274673521518707\n",
      "Warning: nan gradient found. The current loss is:  1.6144745349884033\n",
      "Warning: nan gradient found. The current loss is:  0.1336602419614792\n",
      "Warning: nan gradient found. The current loss is:  0.9325457215309143\n",
      "Warning: nan gradient found. The current loss is:  1.439239501953125\n",
      "Warning: nan gradient found. The current loss is:  0.5670508146286011\n",
      "Warning: nan gradient found. The current loss is:  1.227006196975708\n",
      "Warning: nan gradient found. The current loss is:  0.7123705148696899\n",
      "Warning: nan gradient found. The current loss is:  1.5855698585510254\n",
      "Warning: nan gradient found. The current loss is:  0.41042888164520264\n",
      "Warning: nan gradient found. The current loss is:  0.8311264514923096\n",
      "Warning: nan gradient found. The current loss is:  0.8249712586402893\n",
      "Warning: nan gradient found. The current loss is:  0.8187465667724609\n",
      "Warning: nan gradient found. The current loss is:  0.6817935705184937\n",
      "Warning: nan gradient found. The current loss is:  0.4147164225578308\n",
      "Warning: nan gradient found. The current loss is:  0.6170480847358704\n",
      "Warning: nan gradient found. The current loss is:  0.05293422192335129\n",
      "Warning: nan gradient found. The current loss is:  0.825747013092041\n",
      "Warning: nan gradient found. The current loss is:  0.7807585000991821\n",
      "Warning: nan gradient found. The current loss is:  0.39502936601638794\n",
      "Warning: nan gradient found. The current loss is:  1.1903190612792969\n",
      "Warning: nan gradient found. The current loss is:  0.31691494584083557\n",
      "Warning: nan gradient found. The current loss is:  1.022096037864685\n",
      "Warning: nan gradient found. The current loss is:  0.23536859452724457\n",
      "Warning: nan gradient found. The current loss is:  0.37588101625442505\n",
      "Warning: nan gradient found. The current loss is:  0.19121521711349487\n",
      "Warning: nan gradient found. The current loss is:  0.5383796691894531\n",
      "Warning: nan gradient found. The current loss is:  0.9130295515060425\n",
      "Warning: nan gradient found. The current loss is:  0.5110656023025513\n",
      "Warning: nan gradient found. The current loss is:  0.17015966773033142\n",
      "Warning: nan gradient found. The current loss is:  0.5107507109642029\n",
      "Warning: nan gradient found. The current loss is:  0.24905869364738464\n",
      "Warning: nan gradient found. The current loss is:  0.8708465695381165\n",
      "Warning: nan gradient found. The current loss is:  0.9126787185668945\n",
      "Warning: nan gradient found. The current loss is:  0.9408947229385376\n",
      "Warning: nan gradient found. The current loss is:  0.04452105611562729\n",
      "Warning: nan gradient found. The current loss is:  0.46592170000076294\n",
      "Warning: nan gradient found. The current loss is:  1.4457623958587646\n",
      "Warning: nan gradient found. The current loss is:  0.9204558730125427\n",
      "Warning: nan gradient found. The current loss is:  0.21471500396728516\n",
      "Warning: nan gradient found. The current loss is:  0.7864549160003662\n",
      "Warning: nan gradient found. The current loss is:  3.1613473892211914\n",
      "Warning: nan gradient found. The current loss is:  0.47125810384750366\n",
      "Warning: nan gradient found. The current loss is:  0.35322338342666626\n",
      "Warning: nan gradient found. The current loss is:  0.5512285232543945\n",
      "Warning: nan gradient found. The current loss is:  1.1943814754486084\n",
      "Warning: nan gradient found. The current loss is:  0.8466721773147583\n",
      "Warning: nan gradient found. The current loss is:  0.3143547773361206\n",
      "Warning: nan gradient found. The current loss is:  0.9164738655090332\n",
      "Warning: nan gradient found. The current loss is:  0.817313551902771\n",
      "Warning: nan gradient found. The current loss is:  0.7114161849021912\n",
      "Warning: nan gradient found. The current loss is:  0.6739403009414673\n",
      "Warning: nan gradient found. The current loss is:  0.5714994072914124\n",
      "Warning: nan gradient found. The current loss is:  0.6398441791534424\n",
      "Warning: nan gradient found. The current loss is:  0.3821333646774292\n",
      "Warning: nan gradient found. The current loss is:  0.0834902822971344\n",
      "Warning: nan gradient found. The current loss is:  0.568777859210968\n",
      "Warning: nan gradient found. The current loss is:  1.1251945495605469\n",
      "Warning: nan gradient found. The current loss is:  1.066394567489624\n",
      "Warning: nan gradient found. The current loss is:  1.02443265914917\n",
      "Warning: nan gradient found. The current loss is:  1.0732104778289795\n",
      "Warning: nan gradient found. The current loss is:  0.9169228076934814\n",
      "Warning: nan gradient found. The current loss is:  0.3156881630420685\n",
      "Warning: nan gradient found. The current loss is:  0.4667820334434509\n",
      "Warning: nan gradient found. The current loss is:  0.7172980308532715\n",
      "Warning: nan gradient found. The current loss is:  0.44419246912002563\n",
      "Warning: nan gradient found. The current loss is:  0.42285704612731934\n",
      "Warning: nan gradient found. The current loss is:  0.7239327430725098\n",
      "Warning: nan gradient found. The current loss is:  0.6990221738815308\n",
      "Warning: nan gradient found. The current loss is:  0.3955920934677124\n",
      "Warning: nan gradient found. The current loss is:  0.44373542070388794\n",
      "Warning: nan gradient found. The current loss is:  0.5969165563583374\n",
      "Warning: nan gradient found. The current loss is:  1.2310714721679688\n",
      "Warning: nan gradient found. The current loss is:  0.7147715091705322\n",
      "Warning: nan gradient found. The current loss is:  0.5521695613861084\n",
      "Warning: nan gradient found. The current loss is:  0.3016091585159302\n",
      "Warning: nan gradient found. The current loss is:  0.33408114314079285\n",
      "Warning: nan gradient found. The current loss is:  0.8434317111968994\n",
      "Warning: nan gradient found. The current loss is:  0.835963785648346\n",
      "Warning: nan gradient found. The current loss is:  1.0613747835159302\n",
      "Warning: nan gradient found. The current loss is:  0.8610315322875977\n",
      "Warning: nan gradient found. The current loss is:  0.9880756735801697\n",
      "Warning: nan gradient found. The current loss is:  0.7007978558540344\n",
      "Warning: nan gradient found. The current loss is:  1.0912954807281494\n",
      "Warning: nan gradient found. The current loss is:  0.6564967632293701\n",
      "Warning: nan gradient found. The current loss is:  1.7614030838012695\n",
      "Warning: nan gradient found. The current loss is:  0.44706031680107117\n",
      "Warning: nan gradient found. The current loss is:  0.4431290626525879\n",
      "Warning: nan gradient found. The current loss is:  0.18358781933784485\n",
      "Warning: nan gradient found. The current loss is:  0.6735485196113586\n",
      "Warning: nan gradient found. The current loss is:  0.4772765636444092\n",
      "Warning: nan gradient found. The current loss is:  0.34522342681884766\n",
      "Warning: nan gradient found. The current loss is:  1.7948527336120605\n",
      "Current batch training loss: 1.794853  [1433600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5612348318099976\n",
      "Warning: nan gradient found. The current loss is:  0.2358846217393875\n",
      "Warning: nan gradient found. The current loss is:  0.5697077512741089\n",
      "Warning: nan gradient found. The current loss is:  0.9965309500694275\n",
      "Warning: nan gradient found. The current loss is:  0.22486276924610138\n",
      "Warning: nan gradient found. The current loss is:  1.180936574935913\n",
      "Warning: nan gradient found. The current loss is:  1.833824872970581\n",
      "Warning: nan gradient found. The current loss is:  0.607385516166687\n",
      "Warning: nan gradient found. The current loss is:  0.370644211769104\n",
      "Warning: nan gradient found. The current loss is:  1.0208854675292969\n",
      "Warning: nan gradient found. The current loss is:  0.6467698216438293\n",
      "Warning: nan gradient found. The current loss is:  0.6882724165916443\n",
      "Warning: nan gradient found. The current loss is:  0.7675122022628784\n",
      "Warning: nan gradient found. The current loss is:  0.6615031361579895\n",
      "Warning: nan gradient found. The current loss is:  0.20010539889335632\n",
      "Warning: nan gradient found. The current loss is:  0.20378422737121582\n",
      "Warning: nan gradient found. The current loss is:  0.6594041585922241\n",
      "Warning: nan gradient found. The current loss is:  0.35782158374786377\n",
      "Warning: nan gradient found. The current loss is:  0.6892215609550476\n",
      "Warning: nan gradient found. The current loss is:  0.5582526326179504\n",
      "Warning: nan gradient found. The current loss is:  0.8451442718505859\n",
      "Warning: nan gradient found. The current loss is:  0.9498159885406494\n",
      "Warning: nan gradient found. The current loss is:  0.4962611794471741\n",
      "Warning: nan gradient found. The current loss is:  0.30682820081710815\n",
      "Warning: nan gradient found. The current loss is:  0.4160236418247223\n",
      "Warning: nan gradient found. The current loss is:  0.7307775616645813\n",
      "Warning: nan gradient found. The current loss is:  1.0636448860168457\n",
      "Warning: nan gradient found. The current loss is:  0.6444079279899597\n",
      "Warning: nan gradient found. The current loss is:  0.3883296251296997\n",
      "Warning: nan gradient found. The current loss is:  0.7684112787246704\n",
      "Warning: nan gradient found. The current loss is:  0.5262402296066284\n",
      "Warning: nan gradient found. The current loss is:  0.8187174797058105\n",
      "Warning: nan gradient found. The current loss is:  0.7139897346496582\n",
      "Warning: nan gradient found. The current loss is:  0.5912359952926636\n",
      "Warning: nan gradient found. The current loss is:  0.771558403968811\n",
      "Warning: nan gradient found. The current loss is:  0.6772396564483643\n",
      "Warning: nan gradient found. The current loss is:  0.6887841820716858\n",
      "Warning: nan gradient found. The current loss is:  1.30014169216156\n",
      "Warning: nan gradient found. The current loss is:  1.1164343357086182\n",
      "Warning: nan gradient found. The current loss is:  0.8189170360565186\n",
      "Warning: nan gradient found. The current loss is:  1.2881397008895874\n",
      "Warning: nan gradient found. The current loss is:  0.2946634888648987\n",
      "Warning: nan gradient found. The current loss is:  0.702848494052887\n",
      "Warning: nan gradient found. The current loss is:  0.7460618019104004\n",
      "Warning: nan gradient found. The current loss is:  0.4067114591598511\n",
      "Warning: nan gradient found. The current loss is:  0.1979513019323349\n",
      "Warning: nan gradient found. The current loss is:  0.4769459068775177\n",
      "Warning: nan gradient found. The current loss is:  0.5220397710800171\n",
      "Warning: nan gradient found. The current loss is:  0.6608719825744629\n",
      "Warning: nan gradient found. The current loss is:  1.1557550430297852\n",
      "Warning: nan gradient found. The current loss is:  1.1381407976150513\n",
      "Warning: nan gradient found. The current loss is:  1.3664227724075317\n",
      "Warning: nan gradient found. The current loss is:  0.49628883600234985\n",
      "Warning: nan gradient found. The current loss is:  1.25202476978302\n",
      "Warning: nan gradient found. The current loss is:  0.7604759931564331\n",
      "Warning: nan gradient found. The current loss is:  0.6246294379234314\n",
      "Warning: nan gradient found. The current loss is:  0.07352448999881744\n",
      "Warning: nan gradient found. The current loss is:  0.8980175256729126\n",
      "Warning: nan gradient found. The current loss is:  4.255820274353027\n",
      "Warning: nan gradient found. The current loss is:  0.4606412351131439\n",
      "Warning: nan gradient found. The current loss is:  1.0742591619491577\n",
      "Warning: nan gradient found. The current loss is:  0.49511635303497314\n",
      "Warning: nan gradient found. The current loss is:  0.474353551864624\n",
      "Warning: nan gradient found. The current loss is:  1.104995846748352\n",
      "Warning: nan gradient found. The current loss is:  0.44041234254837036\n",
      "Warning: nan gradient found. The current loss is:  0.9415451884269714\n",
      "Warning: nan gradient found. The current loss is:  0.6085759401321411\n",
      "Warning: nan gradient found. The current loss is:  0.5657487511634827\n",
      "Warning: nan gradient found. The current loss is:  0.7017353773117065\n",
      "Warning: nan gradient found. The current loss is:  0.33915287256240845\n",
      "Warning: nan gradient found. The current loss is:  0.5202498435974121\n",
      "Warning: nan gradient found. The current loss is:  0.050576016306877136\n",
      "Warning: nan gradient found. The current loss is:  0.4970886707305908\n",
      "Warning: nan gradient found. The current loss is:  0.349911093711853\n",
      "Warning: nan gradient found. The current loss is:  0.4687379002571106\n",
      "Warning: nan gradient found. The current loss is:  0.7723301649093628\n",
      "Warning: nan gradient found. The current loss is:  0.24036511778831482\n",
      "Warning: nan gradient found. The current loss is:  0.5095911622047424\n",
      "Warning: nan gradient found. The current loss is:  0.309279203414917\n",
      "Warning: nan gradient found. The current loss is:  0.4565187692642212\n",
      "Warning: nan gradient found. The current loss is:  0.36332938075065613\n",
      "Warning: nan gradient found. The current loss is:  0.5252278447151184\n",
      "Warning: nan gradient found. The current loss is:  0.5600895881652832\n",
      "Warning: nan gradient found. The current loss is:  0.5606057047843933\n",
      "Warning: nan gradient found. The current loss is:  0.16146785020828247\n",
      "Warning: nan gradient found. The current loss is:  0.43523526191711426\n",
      "Warning: nan gradient found. The current loss is:  1.0092518329620361\n",
      "Warning: nan gradient found. The current loss is:  1.1215794086456299\n",
      "Warning: nan gradient found. The current loss is:  0.9879170656204224\n",
      "Warning: nan gradient found. The current loss is:  0.7628482580184937\n",
      "Warning: nan gradient found. The current loss is:  0.09990665316581726\n",
      "Warning: nan gradient found. The current loss is:  0.7528614401817322\n",
      "Warning: nan gradient found. The current loss is:  1.1233021020889282\n",
      "Warning: nan gradient found. The current loss is:  0.5900382995605469\n",
      "Warning: nan gradient found. The current loss is:  0.4164358973503113\n",
      "Warning: nan gradient found. The current loss is:  0.4872414469718933\n",
      "Warning: nan gradient found. The current loss is:  0.559780478477478\n",
      "Warning: nan gradient found. The current loss is:  1.993334412574768\n",
      "Warning: nan gradient found. The current loss is:  -0.040464770048856735\n",
      "Warning: nan gradient found. The current loss is:  1.2391427755355835\n",
      "Current batch training loss: 1.239143  [1459200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.33721864223480225\n",
      "Warning: nan gradient found. The current loss is:  0.697754979133606\n",
      "Warning: nan gradient found. The current loss is:  0.1331854611635208\n",
      "Warning: nan gradient found. The current loss is:  0.805888831615448\n",
      "Warning: nan gradient found. The current loss is:  0.6697986125946045\n",
      "Warning: nan gradient found. The current loss is:  0.8643331527709961\n",
      "Warning: nan gradient found. The current loss is:  0.4612358808517456\n",
      "Warning: nan gradient found. The current loss is:  0.23362863063812256\n",
      "Warning: nan gradient found. The current loss is:  0.21657559275627136\n",
      "Warning: nan gradient found. The current loss is:  0.550544798374176\n",
      "Warning: nan gradient found. The current loss is:  0.6033157706260681\n",
      "Warning: nan gradient found. The current loss is:  0.3057701587677002\n",
      "Warning: nan gradient found. The current loss is:  0.3284280300140381\n",
      "Warning: nan gradient found. The current loss is:  0.7171764373779297\n",
      "Warning: nan gradient found. The current loss is:  1.530438780784607\n",
      "Warning: nan gradient found. The current loss is:  0.606063187122345\n",
      "Warning: nan gradient found. The current loss is:  1.0378165245056152\n",
      "Warning: nan gradient found. The current loss is:  0.6634185314178467\n",
      "Warning: nan gradient found. The current loss is:  0.7542107105255127\n",
      "Warning: nan gradient found. The current loss is:  0.5256388187408447\n",
      "Warning: nan gradient found. The current loss is:  0.6705601215362549\n",
      "Warning: nan gradient found. The current loss is:  0.16244849562644958\n",
      "Warning: nan gradient found. The current loss is:  0.9269334077835083\n",
      "Warning: nan gradient found. The current loss is:  0.9192389249801636\n",
      "Warning: nan gradient found. The current loss is:  0.7135982513427734\n",
      "Warning: nan gradient found. The current loss is:  2.752135753631592\n",
      "Warning: nan gradient found. The current loss is:  0.7839664816856384\n",
      "Warning: nan gradient found. The current loss is:  0.5056980848312378\n",
      "Warning: nan gradient found. The current loss is:  1.0640463829040527\n",
      "Warning: nan gradient found. The current loss is:  0.9460813999176025\n",
      "Warning: nan gradient found. The current loss is:  0.8590707778930664\n",
      "Warning: nan gradient found. The current loss is:  0.6265407800674438\n",
      "Warning: nan gradient found. The current loss is:  0.25830909609794617\n",
      "Warning: nan gradient found. The current loss is:  0.38735148310661316\n",
      "Warning: nan gradient found. The current loss is:  0.40215760469436646\n",
      "Warning: nan gradient found. The current loss is:  0.24103227257728577\n",
      "Warning: nan gradient found. The current loss is:  0.942919909954071\n",
      "Warning: nan gradient found. The current loss is:  0.5575284361839294\n",
      "Warning: nan gradient found. The current loss is:  0.8785514831542969\n",
      "Warning: nan gradient found. The current loss is:  0.56422358751297\n",
      "Warning: nan gradient found. The current loss is:  1.500345230102539\n",
      "Warning: nan gradient found. The current loss is:  0.7282602190971375\n",
      "Warning: nan gradient found. The current loss is:  1.0663944482803345\n",
      "Warning: nan gradient found. The current loss is:  1.120118498802185\n",
      "Warning: nan gradient found. The current loss is:  0.6288921236991882\n",
      "Warning: nan gradient found. The current loss is:  0.5354379415512085\n",
      "Warning: nan gradient found. The current loss is:  0.5031952857971191\n",
      "Warning: nan gradient found. The current loss is:  0.8099735975265503\n",
      "Warning: nan gradient found. The current loss is:  0.5538079142570496\n",
      "Warning: nan gradient found. The current loss is:  0.8719376921653748\n",
      "Warning: nan gradient found. The current loss is:  0.6124035716056824\n",
      "Warning: nan gradient found. The current loss is:  0.2551403045654297\n",
      "Warning: nan gradient found. The current loss is:  0.2582835257053375\n",
      "Warning: nan gradient found. The current loss is:  1.0231932401657104\n",
      "Warning: nan gradient found. The current loss is:  0.5461546182632446\n",
      "Warning: nan gradient found. The current loss is:  0.14951077103614807\n",
      "Warning: nan gradient found. The current loss is:  0.3812727630138397\n",
      "Warning: nan gradient found. The current loss is:  0.575239896774292\n",
      "Warning: nan gradient found. The current loss is:  1.0401654243469238\n",
      "Warning: nan gradient found. The current loss is:  0.5340288281440735\n",
      "Warning: nan gradient found. The current loss is:  0.2637772262096405\n",
      "Warning: nan gradient found. The current loss is:  1.1083027124404907\n",
      "Warning: nan gradient found. The current loss is:  0.8720042705535889\n",
      "Warning: nan gradient found. The current loss is:  0.3338945508003235\n",
      "Warning: nan gradient found. The current loss is:  0.3594340682029724\n",
      "Warning: nan gradient found. The current loss is:  0.8409836888313293\n",
      "Warning: nan gradient found. The current loss is:  0.6983652114868164\n",
      "Warning: nan gradient found. The current loss is:  0.287700355052948\n",
      "Warning: nan gradient found. The current loss is:  0.3627287447452545\n",
      "Warning: nan gradient found. The current loss is:  0.9716991186141968\n",
      "Warning: nan gradient found. The current loss is:  0.15156683325767517\n",
      "Warning: nan gradient found. The current loss is:  0.551021933555603\n",
      "Warning: nan gradient found. The current loss is:  0.4356927275657654\n",
      "Warning: nan gradient found. The current loss is:  0.09498189389705658\n",
      "Warning: nan gradient found. The current loss is:  0.5816177725791931\n",
      "Warning: nan gradient found. The current loss is:  0.940073549747467\n",
      "Warning: nan gradient found. The current loss is:  0.45780301094055176\n",
      "Warning: nan gradient found. The current loss is:  0.1513012945652008\n",
      "Warning: nan gradient found. The current loss is:  0.4894031882286072\n",
      "Warning: nan gradient found. The current loss is:  0.48568493127822876\n",
      "Warning: nan gradient found. The current loss is:  0.5422762632369995\n",
      "Warning: nan gradient found. The current loss is:  0.38255825638771057\n",
      "Warning: nan gradient found. The current loss is:  1.2673219442367554\n",
      "Warning: nan gradient found. The current loss is:  1.263526201248169\n",
      "Warning: nan gradient found. The current loss is:  0.6456693410873413\n",
      "Warning: nan gradient found. The current loss is:  1.2896742820739746\n",
      "Warning: nan gradient found. The current loss is:  0.16122585535049438\n",
      "Warning: nan gradient found. The current loss is:  0.5710782408714294\n",
      "Warning: nan gradient found. The current loss is:  0.43778687715530396\n",
      "Warning: nan gradient found. The current loss is:  0.6619918346405029\n",
      "Warning: nan gradient found. The current loss is:  0.8772047758102417\n",
      "Warning: nan gradient found. The current loss is:  1.5361807346343994\n",
      "Warning: nan gradient found. The current loss is:  0.6961448788642883\n",
      "Warning: nan gradient found. The current loss is:  1.0801632404327393\n",
      "Warning: nan gradient found. The current loss is:  0.8786054849624634\n",
      "Warning: nan gradient found. The current loss is:  0.24128030240535736\n",
      "Warning: nan gradient found. The current loss is:  0.9012770652770996\n",
      "Warning: nan gradient found. The current loss is:  0.6822768449783325\n",
      "Warning: nan gradient found. The current loss is:  0.8807425498962402\n",
      "Warning: nan gradient found. The current loss is:  0.46010953187942505\n",
      "Current batch training loss: 0.460110  [1484800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6387975811958313\n",
      "Warning: nan gradient found. The current loss is:  0.6896027326583862\n",
      "Warning: nan gradient found. The current loss is:  1.8343863487243652\n",
      "Warning: nan gradient found. The current loss is:  0.7288815975189209\n",
      "Warning: nan gradient found. The current loss is:  1.0666370391845703\n",
      "Warning: nan gradient found. The current loss is:  0.8677741885185242\n",
      "Warning: nan gradient found. The current loss is:  0.15640076994895935\n",
      "Warning: nan gradient found. The current loss is:  0.8360552191734314\n",
      "Warning: nan gradient found. The current loss is:  0.5353444218635559\n",
      "Warning: nan gradient found. The current loss is:  1.4781110286712646\n",
      "Warning: nan gradient found. The current loss is:  0.9350810647010803\n",
      "Warning: nan gradient found. The current loss is:  0.6158938407897949\n",
      "Warning: nan gradient found. The current loss is:  1.0844922065734863\n",
      "Warning: nan gradient found. The current loss is:  0.8113752603530884\n",
      "Warning: nan gradient found. The current loss is:  0.4041781425476074\n",
      "Warning: nan gradient found. The current loss is:  0.5519294142723083\n",
      "Warning: nan gradient found. The current loss is:  0.43806254863739014\n",
      "Warning: nan gradient found. The current loss is:  0.8476371765136719\n",
      "Warning: nan gradient found. The current loss is:  0.47785770893096924\n",
      "Warning: nan gradient found. The current loss is:  0.7757843732833862\n",
      "Warning: nan gradient found. The current loss is:  1.5305203199386597\n",
      "Warning: nan gradient found. The current loss is:  0.7205300331115723\n",
      "Warning: nan gradient found. The current loss is:  0.4226851463317871\n",
      "Warning: nan gradient found. The current loss is:  0.24875321984291077\n",
      "Warning: nan gradient found. The current loss is:  0.8150851726531982\n",
      "Warning: nan gradient found. The current loss is:  0.778003454208374\n",
      "Warning: nan gradient found. The current loss is:  0.7261847257614136\n",
      "Warning: nan gradient found. The current loss is:  1.6923011541366577\n",
      "Warning: nan gradient found. The current loss is:  0.4420803189277649\n",
      "Warning: nan gradient found. The current loss is:  0.5884785652160645\n",
      "Warning: nan gradient found. The current loss is:  0.7390641570091248\n",
      "Warning: nan gradient found. The current loss is:  0.4503272771835327\n",
      "Warning: nan gradient found. The current loss is:  0.3474959135055542\n",
      "Warning: nan gradient found. The current loss is:  0.06498023122549057\n",
      "Warning: nan gradient found. The current loss is:  0.5182608962059021\n",
      "Warning: nan gradient found. The current loss is:  0.6981549263000488\n",
      "Warning: nan gradient found. The current loss is:  1.266542911529541\n",
      "Warning: nan gradient found. The current loss is:  1.2111220359802246\n",
      "Warning: nan gradient found. The current loss is:  0.2320912778377533\n",
      "Warning: nan gradient found. The current loss is:  0.9589307308197021\n",
      "Warning: nan gradient found. The current loss is:  0.4550015330314636\n",
      "Warning: nan gradient found. The current loss is:  0.5481135845184326\n",
      "Warning: nan gradient found. The current loss is:  1.2285701036453247\n",
      "Warning: nan gradient found. The current loss is:  0.6647442579269409\n",
      "Warning: nan gradient found. The current loss is:  0.7148177027702332\n",
      "Warning: nan gradient found. The current loss is:  0.5055063962936401\n",
      "Warning: nan gradient found. The current loss is:  0.6356431245803833\n",
      "Warning: nan gradient found. The current loss is:  0.7899734973907471\n",
      "Warning: nan gradient found. The current loss is:  0.06726458668708801\n",
      "Warning: nan gradient found. The current loss is:  0.6728286743164062\n",
      "Warning: nan gradient found. The current loss is:  1.3684223890304565\n",
      "Warning: nan gradient found. The current loss is:  1.1641730070114136\n",
      "Warning: nan gradient found. The current loss is:  0.6915246248245239\n",
      "Warning: nan gradient found. The current loss is:  0.2081700563430786\n",
      "Warning: nan gradient found. The current loss is:  0.5068524479866028\n",
      "Warning: nan gradient found. The current loss is:  0.5792865753173828\n",
      "Warning: nan gradient found. The current loss is:  0.22267702221870422\n",
      "Warning: nan gradient found. The current loss is:  0.6223729848861694\n",
      "Warning: nan gradient found. The current loss is:  0.57309889793396\n",
      "Warning: nan gradient found. The current loss is:  2.281379222869873\n",
      "Warning: nan gradient found. The current loss is:  0.7628510594367981\n",
      "Warning: nan gradient found. The current loss is:  0.38422876596450806\n",
      "Warning: nan gradient found. The current loss is:  1.6282908916473389\n",
      "Warning: nan gradient found. The current loss is:  0.777630090713501\n",
      "Warning: nan gradient found. The current loss is:  0.6030396223068237\n",
      "Warning: nan gradient found. The current loss is:  0.7863143086433411\n",
      "Warning: nan gradient found. The current loss is:  0.08685879409313202\n",
      "Warning: nan gradient found. The current loss is:  0.6295557022094727\n",
      "Warning: nan gradient found. The current loss is:  0.3741893470287323\n",
      "Warning: nan gradient found. The current loss is:  0.20806607604026794\n",
      "Warning: nan gradient found. The current loss is:  0.23617954552173615\n",
      "Warning: nan gradient found. The current loss is:  0.47313159704208374\n",
      "Warning: nan gradient found. The current loss is:  0.7703549861907959\n",
      "Warning: nan gradient found. The current loss is:  1.077834963798523\n",
      "Warning: nan gradient found. The current loss is:  0.6289932727813721\n",
      "Warning: nan gradient found. The current loss is:  1.54017174243927\n",
      "Warning: nan gradient found. The current loss is:  0.41731762886047363\n",
      "Warning: nan gradient found. The current loss is:  0.35094153881073\n",
      "Warning: nan gradient found. The current loss is:  1.3809130191802979\n",
      "Warning: nan gradient found. The current loss is:  1.2157247066497803\n",
      "Warning: nan gradient found. The current loss is:  0.8960303664207458\n",
      "Warning: nan gradient found. The current loss is:  0.6881502866744995\n",
      "Warning: nan gradient found. The current loss is:  0.4056398868560791\n",
      "Warning: nan gradient found. The current loss is:  0.4250060021877289\n",
      "Warning: nan gradient found. The current loss is:  1.174387812614441\n",
      "Warning: nan gradient found. The current loss is:  0.2896421551704407\n",
      "Warning: nan gradient found. The current loss is:  0.776270866394043\n",
      "Warning: nan gradient found. The current loss is:  1.267077922821045\n",
      "Warning: nan gradient found. The current loss is:  1.126204252243042\n",
      "Warning: nan gradient found. The current loss is:  1.2527395486831665\n",
      "Warning: nan gradient found. The current loss is:  0.5090153217315674\n",
      "Warning: nan gradient found. The current loss is:  0.8271186351776123\n",
      "Warning: nan gradient found. The current loss is:  0.512485146522522\n",
      "Warning: nan gradient found. The current loss is:  0.40789902210235596\n",
      "Warning: nan gradient found. The current loss is:  0.24385994672775269\n",
      "Warning: nan gradient found. The current loss is:  0.6444119215011597\n",
      "Warning: nan gradient found. The current loss is:  1.1296409368515015\n",
      "Warning: nan gradient found. The current loss is:  0.9996187686920166\n",
      "Warning: nan gradient found. The current loss is:  0.31386539340019226\n",
      "Warning: nan gradient found. The current loss is:  0.6145379543304443\n",
      "Current batch training loss: 0.614538  [1510400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6171652674674988\n",
      "Warning: nan gradient found. The current loss is:  0.665299654006958\n",
      "Warning: nan gradient found. The current loss is:  0.83372962474823\n",
      "Warning: nan gradient found. The current loss is:  1.0526729822158813\n",
      "Warning: nan gradient found. The current loss is:  0.8103351593017578\n",
      "Warning: nan gradient found. The current loss is:  1.2094676494598389\n",
      "Warning: nan gradient found. The current loss is:  0.21035927534103394\n",
      "Warning: nan gradient found. The current loss is:  0.6114005446434021\n",
      "Warning: nan gradient found. The current loss is:  0.6862618327140808\n",
      "Warning: nan gradient found. The current loss is:  0.6314237117767334\n",
      "Warning: nan gradient found. The current loss is:  0.29027026891708374\n",
      "Warning: nan gradient found. The current loss is:  0.7913596630096436\n",
      "Warning: nan gradient found. The current loss is:  0.2640719413757324\n",
      "Warning: nan gradient found. The current loss is:  0.4900389313697815\n",
      "Warning: nan gradient found. The current loss is:  0.6869258880615234\n",
      "Warning: nan gradient found. The current loss is:  0.4859006404876709\n",
      "Warning: nan gradient found. The current loss is:  0.306639164686203\n",
      "Warning: nan gradient found. The current loss is:  0.35362982749938965\n",
      "Warning: nan gradient found. The current loss is:  1.4252651929855347\n",
      "Warning: nan gradient found. The current loss is:  1.34157395362854\n",
      "Warning: nan gradient found. The current loss is:  1.3686585426330566\n",
      "Warning: nan gradient found. The current loss is:  0.6121053695678711\n",
      "Warning: nan gradient found. The current loss is:  1.1292308568954468\n",
      "Warning: nan gradient found. The current loss is:  0.38425421714782715\n",
      "Warning: nan gradient found. The current loss is:  1.1849989891052246\n",
      "Warning: nan gradient found. The current loss is:  1.4865763187408447\n",
      "Warning: nan gradient found. The current loss is:  0.6841912269592285\n",
      "Warning: nan gradient found. The current loss is:  0.4186541438102722\n",
      "Warning: nan gradient found. The current loss is:  0.8431293368339539\n",
      "Warning: nan gradient found. The current loss is:  0.25660794973373413\n",
      "Warning: nan gradient found. The current loss is:  0.5958728790283203\n",
      "Warning: nan gradient found. The current loss is:  1.8672072887420654\n",
      "Warning: nan gradient found. The current loss is:  0.8174452781677246\n",
      "Warning: nan gradient found. The current loss is:  0.5588770508766174\n",
      "Warning: nan gradient found. The current loss is:  0.23795606195926666\n",
      "Warning: nan gradient found. The current loss is:  0.27029916644096375\n",
      "Warning: nan gradient found. The current loss is:  0.7377293109893799\n",
      "Warning: nan gradient found. The current loss is:  1.0293419361114502\n",
      "Warning: nan gradient found. The current loss is:  0.6987048387527466\n",
      "Warning: nan gradient found. The current loss is:  2.134665012359619\n",
      "Warning: nan gradient found. The current loss is:  0.22604471445083618\n",
      "Warning: nan gradient found. The current loss is:  0.8610397577285767\n",
      "Warning: nan gradient found. The current loss is:  0.4702078700065613\n",
      "Warning: nan gradient found. The current loss is:  0.1888332962989807\n",
      "Warning: nan gradient found. The current loss is:  0.7974169254302979\n",
      "Warning: nan gradient found. The current loss is:  0.4552067518234253\n",
      "Warning: nan gradient found. The current loss is:  0.22124499082565308\n",
      "Warning: nan gradient found. The current loss is:  1.2313557863235474\n",
      "Warning: nan gradient found. The current loss is:  0.7914683818817139\n",
      "Warning: nan gradient found. The current loss is:  0.36822962760925293\n",
      "Warning: nan gradient found. The current loss is:  1.870901107788086\n",
      "Warning: nan gradient found. The current loss is:  0.7640427947044373\n",
      "Warning: nan gradient found. The current loss is:  0.5731151103973389\n",
      "Warning: nan gradient found. The current loss is:  0.2848421335220337\n",
      "Warning: nan gradient found. The current loss is:  2.1792702674865723\n",
      "Warning: nan gradient found. The current loss is:  0.7071865797042847\n",
      "Warning: nan gradient found. The current loss is:  0.45348700881004333\n",
      "Warning: nan gradient found. The current loss is:  0.338517427444458\n",
      "Warning: nan gradient found. The current loss is:  0.7933056354522705\n",
      "Warning: nan gradient found. The current loss is:  0.4620755910873413\n",
      "Warning: nan gradient found. The current loss is:  0.3347260653972626\n",
      "Warning: nan gradient found. The current loss is:  0.48493629693984985\n",
      "Warning: nan gradient found. The current loss is:  0.8167529106140137\n",
      "Warning: nan gradient found. The current loss is:  0.6813116073608398\n",
      "Warning: nan gradient found. The current loss is:  1.868393898010254\n",
      "Warning: nan gradient found. The current loss is:  0.7794739007949829\n",
      "Warning: nan gradient found. The current loss is:  0.7018789649009705\n",
      "Warning: nan gradient found. The current loss is:  1.5871645212173462\n",
      "Warning: nan gradient found. The current loss is:  1.673710823059082\n",
      "Warning: nan gradient found. The current loss is:  1.648275375366211\n",
      "Warning: nan gradient found. The current loss is:  1.3953137397766113\n",
      "Warning: nan gradient found. The current loss is:  0.8453676700592041\n",
      "Warning: nan gradient found. The current loss is:  0.49952590465545654\n",
      "Warning: nan gradient found. The current loss is:  1.1223397254943848\n",
      "Warning: nan gradient found. The current loss is:  0.4354845881462097\n",
      "Warning: nan gradient found. The current loss is:  0.31805241107940674\n",
      "Warning: nan gradient found. The current loss is:  0.41635751724243164\n",
      "Warning: nan gradient found. The current loss is:  0.5100706815719604\n",
      "Warning: nan gradient found. The current loss is:  0.902216374874115\n",
      "Warning: nan gradient found. The current loss is:  0.6080104112625122\n",
      "Warning: nan gradient found. The current loss is:  0.8335097432136536\n",
      "Warning: nan gradient found. The current loss is:  0.22190991044044495\n",
      "Warning: nan gradient found. The current loss is:  0.537469744682312\n",
      "Warning: nan gradient found. The current loss is:  1.60201096534729\n",
      "Warning: nan gradient found. The current loss is:  0.43349796533584595\n",
      "Warning: nan gradient found. The current loss is:  0.7866042256355286\n",
      "Warning: nan gradient found. The current loss is:  1.7274221181869507\n",
      "Warning: nan gradient found. The current loss is:  1.5769377946853638\n",
      "Warning: nan gradient found. The current loss is:  1.180643916130066\n",
      "Warning: nan gradient found. The current loss is:  1.1620581150054932\n",
      "Warning: nan gradient found. The current loss is:  0.6150178909301758\n",
      "Warning: nan gradient found. The current loss is:  1.2162134647369385\n",
      "Warning: nan gradient found. The current loss is:  0.9167219400405884\n",
      "Warning: nan gradient found. The current loss is:  0.6341766119003296\n",
      "Warning: nan gradient found. The current loss is:  0.8002666234970093\n",
      "Warning: nan gradient found. The current loss is:  1.484397053718567\n",
      "Warning: nan gradient found. The current loss is:  0.7080037593841553\n",
      "Warning: nan gradient found. The current loss is:  0.6264498233795166\n",
      "Warning: nan gradient found. The current loss is:  0.3018798530101776\n",
      "Warning: nan gradient found. The current loss is:  0.6164772510528564\n",
      "Current batch training loss: 0.616477  [1536000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.3393951952457428\n",
      "Warning: nan gradient found. The current loss is:  0.8783355355262756\n",
      "Warning: nan gradient found. The current loss is:  0.8416621685028076\n",
      "Warning: nan gradient found. The current loss is:  1.562453269958496\n",
      "Warning: nan gradient found. The current loss is:  0.933279275894165\n",
      "Warning: nan gradient found. The current loss is:  0.5344526767730713\n",
      "Warning: nan gradient found. The current loss is:  0.9550819396972656\n",
      "Warning: nan gradient found. The current loss is:  0.4158806800842285\n",
      "Warning: nan gradient found. The current loss is:  1.1774425506591797\n",
      "Warning: nan gradient found. The current loss is:  0.2257736623287201\n",
      "Warning: nan gradient found. The current loss is:  0.5733758807182312\n",
      "Warning: nan gradient found. The current loss is:  1.3068006038665771\n",
      "Warning: nan gradient found. The current loss is:  0.7777758836746216\n",
      "Warning: nan gradient found. The current loss is:  0.4593006372451782\n",
      "Warning: nan gradient found. The current loss is:  0.8874926567077637\n",
      "Warning: nan gradient found. The current loss is:  0.3496047556400299\n",
      "Warning: nan gradient found. The current loss is:  0.47855430841445923\n",
      "Warning: nan gradient found. The current loss is:  1.0913913249969482\n",
      "Warning: nan gradient found. The current loss is:  0.8678700923919678\n",
      "Warning: nan gradient found. The current loss is:  0.22530806064605713\n",
      "Warning: nan gradient found. The current loss is:  0.6242074966430664\n",
      "Warning: nan gradient found. The current loss is:  0.14034506678581238\n",
      "Warning: nan gradient found. The current loss is:  0.41447335481643677\n",
      "Warning: nan gradient found. The current loss is:  0.8307764530181885\n",
      "Warning: nan gradient found. The current loss is:  0.6627815365791321\n",
      "Warning: nan gradient found. The current loss is:  0.606372058391571\n",
      "Warning: nan gradient found. The current loss is:  0.6892801523208618\n",
      "Warning: nan gradient found. The current loss is:  1.062339186668396\n",
      "Warning: nan gradient found. The current loss is:  0.854016900062561\n",
      "Warning: nan gradient found. The current loss is:  0.8071643114089966\n",
      "Warning: nan gradient found. The current loss is:  0.8845959901809692\n",
      "Warning: nan gradient found. The current loss is:  0.3828873038291931\n",
      "Warning: nan gradient found. The current loss is:  0.800501823425293\n",
      "Warning: nan gradient found. The current loss is:  1.3098046779632568\n",
      "Warning: nan gradient found. The current loss is:  0.3442804515361786\n",
      "Warning: nan gradient found. The current loss is:  0.5877912640571594\n",
      "Warning: nan gradient found. The current loss is:  0.9247032403945923\n",
      "Warning: nan gradient found. The current loss is:  0.43499141931533813\n",
      "Warning: nan gradient found. The current loss is:  0.5683807134628296\n",
      "Warning: nan gradient found. The current loss is:  0.5365854501724243\n",
      "Warning: nan gradient found. The current loss is:  0.67274010181427\n",
      "Warning: nan gradient found. The current loss is:  0.2722899913787842\n",
      "Warning: nan gradient found. The current loss is:  1.2162175178527832\n",
      "Warning: nan gradient found. The current loss is:  0.5152469277381897\n",
      "Warning: nan gradient found. The current loss is:  1.4958629608154297\n",
      "Warning: nan gradient found. The current loss is:  0.810227632522583\n",
      "Warning: nan gradient found. The current loss is:  0.6840574741363525\n",
      "Warning: nan gradient found. The current loss is:  1.3934462070465088\n",
      "Warning: nan gradient found. The current loss is:  0.5033977031707764\n",
      "Warning: nan gradient found. The current loss is:  0.852526068687439\n",
      "Warning: nan gradient found. The current loss is:  0.7121078968048096\n",
      "Warning: nan gradient found. The current loss is:  1.0523439645767212\n",
      "Warning: nan gradient found. The current loss is:  0.5264102220535278\n",
      "Warning: nan gradient found. The current loss is:  0.6243664026260376\n",
      "Warning: nan gradient found. The current loss is:  1.225790023803711\n",
      "Warning: nan gradient found. The current loss is:  0.9113752841949463\n",
      "Warning: nan gradient found. The current loss is:  0.7772669196128845\n",
      "Warning: nan gradient found. The current loss is:  1.0037217140197754\n",
      "Warning: nan gradient found. The current loss is:  0.0920860767364502\n",
      "Warning: nan gradient found. The current loss is:  0.8322048187255859\n",
      "Warning: nan gradient found. The current loss is:  0.9629579782485962\n",
      "Warning: nan gradient found. The current loss is:  0.27458542585372925\n",
      "Warning: nan gradient found. The current loss is:  0.26058971881866455\n",
      "Warning: nan gradient found. The current loss is:  0.32980865240097046\n",
      "Warning: nan gradient found. The current loss is:  0.7179566621780396\n",
      "Warning: nan gradient found. The current loss is:  1.0481412410736084\n",
      "Warning: nan gradient found. The current loss is:  0.4657588005065918\n",
      "Warning: nan gradient found. The current loss is:  0.6345678567886353\n",
      "Warning: nan gradient found. The current loss is:  0.5387365818023682\n",
      "Warning: nan gradient found. The current loss is:  0.7999934554100037\n",
      "Warning: nan gradient found. The current loss is:  0.4862203001976013\n",
      "Warning: nan gradient found. The current loss is:  0.5380721092224121\n",
      "Warning: nan gradient found. The current loss is:  0.14027993381023407\n",
      "Warning: nan gradient found. The current loss is:  0.9430257678031921\n",
      "Warning: nan gradient found. The current loss is:  0.2552400827407837\n",
      "Warning: nan gradient found. The current loss is:  0.3907703757286072\n",
      "Warning: nan gradient found. The current loss is:  1.2097699642181396\n",
      "Warning: nan gradient found. The current loss is:  1.2162727117538452\n",
      "Warning: nan gradient found. The current loss is:  0.7894458770751953\n",
      "Warning: nan gradient found. The current loss is:  0.533595085144043\n",
      "Warning: nan gradient found. The current loss is:  0.9152803421020508\n",
      "Warning: nan gradient found. The current loss is:  0.7705179452896118\n",
      "Warning: nan gradient found. The current loss is:  0.1816350817680359\n",
      "Warning: nan gradient found. The current loss is:  1.361699104309082\n",
      "Warning: nan gradient found. The current loss is:  0.7954738140106201\n",
      "Warning: nan gradient found. The current loss is:  0.46959853172302246\n",
      "Warning: nan gradient found. The current loss is:  0.5319121479988098\n",
      "Warning: nan gradient found. The current loss is:  0.35590362548828125\n",
      "Warning: nan gradient found. The current loss is:  0.24618640542030334\n",
      "Warning: nan gradient found. The current loss is:  0.3720831871032715\n",
      "Warning: nan gradient found. The current loss is:  0.6357667446136475\n",
      "Warning: nan gradient found. The current loss is:  0.7315357327461243\n",
      "Warning: nan gradient found. The current loss is:  0.9490830898284912\n",
      "Warning: nan gradient found. The current loss is:  0.6439377665519714\n",
      "Warning: nan gradient found. The current loss is:  0.4518583416938782\n",
      "Warning: nan gradient found. The current loss is:  0.6151955723762512\n",
      "Warning: nan gradient found. The current loss is:  0.6065735816955566\n",
      "Warning: nan gradient found. The current loss is:  0.6597490906715393\n",
      "Warning: nan gradient found. The current loss is:  0.5637838244438171\n",
      "Warning: nan gradient found. The current loss is:  0.7882905006408691\n",
      "Current batch training loss: 0.788291  [1561600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5210685729980469\n",
      "Warning: nan gradient found. The current loss is:  0.16231584548950195\n",
      "Warning: nan gradient found. The current loss is:  0.8901269435882568\n",
      "Warning: nan gradient found. The current loss is:  1.4429121017456055\n",
      "Warning: nan gradient found. The current loss is:  0.6728168725967407\n",
      "Warning: nan gradient found. The current loss is:  0.42850470542907715\n",
      "Warning: nan gradient found. The current loss is:  0.7045917510986328\n",
      "Warning: nan gradient found. The current loss is:  0.23324760794639587\n",
      "Warning: nan gradient found. The current loss is:  0.3710371255874634\n",
      "Warning: nan gradient found. The current loss is:  0.7487514019012451\n",
      "Warning: nan gradient found. The current loss is:  0.3243323564529419\n",
      "Warning: nan gradient found. The current loss is:  0.8821301460266113\n",
      "Warning: nan gradient found. The current loss is:  0.989993691444397\n",
      "Warning: nan gradient found. The current loss is:  1.1759692430496216\n",
      "Warning: nan gradient found. The current loss is:  0.9915556907653809\n",
      "Warning: nan gradient found. The current loss is:  0.6212215423583984\n",
      "Warning: nan gradient found. The current loss is:  0.9895342588424683\n",
      "Warning: nan gradient found. The current loss is:  1.5473676919937134\n",
      "Warning: nan gradient found. The current loss is:  0.48056334257125854\n",
      "Warning: nan gradient found. The current loss is:  0.23729822039604187\n",
      "Warning: nan gradient found. The current loss is:  0.5491726398468018\n",
      "Warning: nan gradient found. The current loss is:  0.46920934319496155\n",
      "Warning: nan gradient found. The current loss is:  0.5751904845237732\n",
      "Warning: nan gradient found. The current loss is:  0.9710686802864075\n",
      "Warning: nan gradient found. The current loss is:  3.1889240741729736\n",
      "Warning: nan gradient found. The current loss is:  0.19611556828022003\n",
      "Warning: nan gradient found. The current loss is:  0.8632711172103882\n",
      "Warning: nan gradient found. The current loss is:  -0.004738599061965942\n",
      "Warning: nan gradient found. The current loss is:  1.3253545761108398\n",
      "Warning: nan gradient found. The current loss is:  0.255412220954895\n",
      "Warning: nan gradient found. The current loss is:  1.1064578294754028\n",
      "Warning: nan gradient found. The current loss is:  1.8768513202667236\n",
      "Warning: nan gradient found. The current loss is:  0.6260942816734314\n",
      "Warning: nan gradient found. The current loss is:  0.3614532947540283\n",
      "Warning: nan gradient found. The current loss is:  0.1611054539680481\n",
      "Warning: nan gradient found. The current loss is:  0.8346126079559326\n",
      "Warning: nan gradient found. The current loss is:  1.063025712966919\n",
      "Warning: nan gradient found. The current loss is:  0.759637176990509\n",
      "Warning: nan gradient found. The current loss is:  0.5289680361747742\n",
      "Warning: nan gradient found. The current loss is:  0.38008400797843933\n",
      "Warning: nan gradient found. The current loss is:  0.904349684715271\n",
      "Warning: nan gradient found. The current loss is:  0.6349990963935852\n",
      "Warning: nan gradient found. The current loss is:  0.7148802876472473\n",
      "Warning: nan gradient found. The current loss is:  0.5567448735237122\n",
      "Warning: nan gradient found. The current loss is:  1.295986533164978\n",
      "Warning: nan gradient found. The current loss is:  0.48145830631256104\n",
      "Warning: nan gradient found. The current loss is:  0.45084232091903687\n",
      "Warning: nan gradient found. The current loss is:  0.6905395984649658\n",
      "Warning: nan gradient found. The current loss is:  0.995013415813446\n",
      "Warning: nan gradient found. The current loss is:  0.6918275356292725\n",
      "Warning: nan gradient found. The current loss is:  0.9314051866531372\n",
      "Warning: nan gradient found. The current loss is:  0.37598341703414917\n",
      "Warning: nan gradient found. The current loss is:  0.2796003222465515\n",
      "Warning: nan gradient found. The current loss is:  0.9951014518737793\n",
      "Warning: nan gradient found. The current loss is:  1.1341547966003418\n",
      "Warning: nan gradient found. The current loss is:  0.6297638416290283\n",
      "Warning: nan gradient found. The current loss is:  0.3973960876464844\n",
      "Warning: nan gradient found. The current loss is:  0.231453537940979\n",
      "Warning: nan gradient found. The current loss is:  0.3469966948032379\n",
      "Warning: nan gradient found. The current loss is:  0.5364046096801758\n",
      "Warning: nan gradient found. The current loss is:  0.877275824546814\n",
      "Warning: nan gradient found. The current loss is:  0.7148277759552002\n",
      "Warning: nan gradient found. The current loss is:  0.4664362072944641\n",
      "Warning: nan gradient found. The current loss is:  0.4159095287322998\n",
      "Warning: nan gradient found. The current loss is:  0.4156828224658966\n",
      "Warning: nan gradient found. The current loss is:  0.5129210948944092\n",
      "Warning: nan gradient found. The current loss is:  0.13475222885608673\n",
      "Warning: nan gradient found. The current loss is:  0.6998620629310608\n",
      "Warning: nan gradient found. The current loss is:  1.0028393268585205\n",
      "Warning: nan gradient found. The current loss is:  0.517899751663208\n",
      "Warning: nan gradient found. The current loss is:  -0.021466441452503204\n",
      "Warning: nan gradient found. The current loss is:  0.645271360874176\n",
      "Warning: nan gradient found. The current loss is:  0.7055511474609375\n",
      "Warning: nan gradient found. The current loss is:  0.292656272649765\n",
      "Warning: nan gradient found. The current loss is:  0.6152322888374329\n",
      "Warning: nan gradient found. The current loss is:  0.9570256471633911\n",
      "Warning: nan gradient found. The current loss is:  0.5841374397277832\n",
      "Warning: nan gradient found. The current loss is:  1.2243139743804932\n",
      "Warning: nan gradient found. The current loss is:  0.7948083281517029\n",
      "Warning: nan gradient found. The current loss is:  0.3695623278617859\n",
      "Warning: nan gradient found. The current loss is:  0.28325408697128296\n",
      "Warning: nan gradient found. The current loss is:  0.9737699627876282\n",
      "Warning: nan gradient found. The current loss is:  0.4660691022872925\n",
      "Warning: nan gradient found. The current loss is:  0.37171006202697754\n",
      "Warning: nan gradient found. The current loss is:  0.21640077233314514\n",
      "Warning: nan gradient found. The current loss is:  0.6374595165252686\n",
      "Warning: nan gradient found. The current loss is:  0.7629377841949463\n",
      "Warning: nan gradient found. The current loss is:  0.3078240156173706\n",
      "Warning: nan gradient found. The current loss is:  1.1647056341171265\n",
      "Warning: nan gradient found. The current loss is:  0.5520734786987305\n",
      "Warning: nan gradient found. The current loss is:  0.5101858377456665\n",
      "Warning: nan gradient found. The current loss is:  1.4568283557891846\n",
      "Warning: nan gradient found. The current loss is:  0.7231091260910034\n",
      "Warning: nan gradient found. The current loss is:  -0.14619562029838562\n",
      "Warning: nan gradient found. The current loss is:  0.8037570714950562\n",
      "Warning: nan gradient found. The current loss is:  0.3577101528644562\n",
      "Warning: nan gradient found. The current loss is:  0.8601836562156677\n",
      "Warning: nan gradient found. The current loss is:  0.6522330641746521\n",
      "Warning: nan gradient found. The current loss is:  0.4047647714614868\n",
      "Warning: nan gradient found. The current loss is:  0.5708520412445068\n",
      "Current batch training loss: 0.570852  [1587200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9580910205841064\n",
      "Warning: nan gradient found. The current loss is:  0.28425583243370056\n",
      "Warning: nan gradient found. The current loss is:  0.8299123048782349\n",
      "Warning: nan gradient found. The current loss is:  1.4108171463012695\n",
      "Warning: nan gradient found. The current loss is:  0.9071944952011108\n",
      "Warning: nan gradient found. The current loss is:  0.700803279876709\n",
      "Warning: nan gradient found. The current loss is:  0.4704095423221588\n",
      "Warning: nan gradient found. The current loss is:  1.610222339630127\n",
      "Warning: nan gradient found. The current loss is:  0.48209306597709656\n",
      "Warning: nan gradient found. The current loss is:  0.540878176689148\n",
      "Warning: nan gradient found. The current loss is:  0.6351766586303711\n",
      "Warning: nan gradient found. The current loss is:  0.08876825869083405\n",
      "Warning: nan gradient found. The current loss is:  0.5207144021987915\n",
      "Warning: nan gradient found. The current loss is:  0.32744476199150085\n",
      "Warning: nan gradient found. The current loss is:  0.776630699634552\n",
      "Warning: nan gradient found. The current loss is:  0.6772403120994568\n",
      "Warning: nan gradient found. The current loss is:  0.48834967613220215\n",
      "Warning: nan gradient found. The current loss is:  1.04085111618042\n",
      "Warning: nan gradient found. The current loss is:  0.8190698027610779\n",
      "Warning: nan gradient found. The current loss is:  0.5087231397628784\n",
      "Warning: nan gradient found. The current loss is:  0.8368615508079529\n",
      "Warning: nan gradient found. The current loss is:  0.4097798764705658\n",
      "Warning: nan gradient found. The current loss is:  0.4301989674568176\n",
      "Warning: nan gradient found. The current loss is:  0.5632503032684326\n",
      "Warning: nan gradient found. The current loss is:  1.391747236251831\n",
      "Warning: nan gradient found. The current loss is:  0.3419448137283325\n",
      "Warning: nan gradient found. The current loss is:  0.22064732015132904\n",
      "Warning: nan gradient found. The current loss is:  0.4582856297492981\n",
      "Warning: nan gradient found. The current loss is:  1.3594584465026855\n",
      "Warning: nan gradient found. The current loss is:  0.467892587184906\n",
      "Warning: nan gradient found. The current loss is:  0.3153422176837921\n",
      "Warning: nan gradient found. The current loss is:  0.6892597675323486\n",
      "Warning: nan gradient found. The current loss is:  0.523079514503479\n",
      "Warning: nan gradient found. The current loss is:  0.5622016191482544\n",
      "Warning: nan gradient found. The current loss is:  0.3574182987213135\n",
      "Warning: nan gradient found. The current loss is:  0.707797646522522\n",
      "Warning: nan gradient found. The current loss is:  0.9861483573913574\n",
      "Warning: nan gradient found. The current loss is:  0.8066763877868652\n",
      "Warning: nan gradient found. The current loss is:  0.5075242519378662\n",
      "Warning: nan gradient found. The current loss is:  2.4248974323272705\n",
      "Warning: nan gradient found. The current loss is:  0.1721528321504593\n",
      "Warning: nan gradient found. The current loss is:  1.245202660560608\n",
      "Warning: nan gradient found. The current loss is:  0.18662086129188538\n",
      "Warning: nan gradient found. The current loss is:  1.7894755601882935\n",
      "Warning: nan gradient found. The current loss is:  0.7960461378097534\n",
      "Warning: nan gradient found. The current loss is:  0.2923416495323181\n",
      "Warning: nan gradient found. The current loss is:  0.9794562458992004\n",
      "Warning: nan gradient found. The current loss is:  1.2661069631576538\n",
      "Warning: nan gradient found. The current loss is:  1.2710272073745728\n",
      "Warning: nan gradient found. The current loss is:  0.6926250457763672\n",
      "Warning: nan gradient found. The current loss is:  0.40604931116104126\n",
      "Warning: nan gradient found. The current loss is:  0.3765513598918915\n",
      "Warning: nan gradient found. The current loss is:  0.07969678193330765\n",
      "Warning: nan gradient found. The current loss is:  0.2959151864051819\n",
      "Warning: nan gradient found. The current loss is:  0.5836275815963745\n",
      "Warning: nan gradient found. The current loss is:  0.18905918300151825\n",
      "Warning: nan gradient found. The current loss is:  1.2199597358703613\n",
      "Warning: nan gradient found. The current loss is:  0.44565191864967346\n",
      "Warning: nan gradient found. The current loss is:  0.9513875246047974\n",
      "Warning: nan gradient found. The current loss is:  0.26868751645088196\n",
      "Warning: nan gradient found. The current loss is:  0.8998832702636719\n",
      "Warning: nan gradient found. The current loss is:  0.8557912111282349\n",
      "Warning: nan gradient found. The current loss is:  0.47040051221847534\n",
      "Warning: nan gradient found. The current loss is:  0.7575972676277161\n",
      "Warning: nan gradient found. The current loss is:  0.398313045501709\n",
      "Warning: nan gradient found. The current loss is:  0.3533317446708679\n",
      "Warning: nan gradient found. The current loss is:  1.4043000936508179\n",
      "Warning: nan gradient found. The current loss is:  1.1283506155014038\n",
      "Warning: nan gradient found. The current loss is:  0.49907687306404114\n",
      "Warning: nan gradient found. The current loss is:  0.3510860502719879\n",
      "Warning: nan gradient found. The current loss is:  1.3357309103012085\n",
      "Warning: nan gradient found. The current loss is:  1.5998667478561401\n",
      "Warning: nan gradient found. The current loss is:  0.38672974705696106\n",
      "Warning: nan gradient found. The current loss is:  0.33680108189582825\n",
      "Warning: nan gradient found. The current loss is:  1.4661962985992432\n",
      "Warning: nan gradient found. The current loss is:  1.4144716262817383\n",
      "Warning: nan gradient found. The current loss is:  0.7741315960884094\n",
      "Warning: nan gradient found. The current loss is:  1.0036495923995972\n",
      "Warning: nan gradient found. The current loss is:  1.1983792781829834\n",
      "Warning: nan gradient found. The current loss is:  0.3956875503063202\n",
      "Warning: nan gradient found. The current loss is:  0.5673702955245972\n",
      "Warning: nan gradient found. The current loss is:  0.7855633497238159\n",
      "Warning: nan gradient found. The current loss is:  0.7120110988616943\n",
      "Warning: nan gradient found. The current loss is:  0.3206668794155121\n",
      "Warning: nan gradient found. The current loss is:  0.45937758684158325\n",
      "Warning: nan gradient found. The current loss is:  0.32900506258010864\n",
      "Warning: nan gradient found. The current loss is:  0.7523069381713867\n",
      "Warning: nan gradient found. The current loss is:  0.6411776542663574\n",
      "Warning: nan gradient found. The current loss is:  0.9907917380332947\n",
      "Warning: nan gradient found. The current loss is:  0.697978675365448\n",
      "Warning: nan gradient found. The current loss is:  0.7915830016136169\n",
      "Warning: nan gradient found. The current loss is:  0.8404595255851746\n",
      "Warning: nan gradient found. The current loss is:  0.25359857082366943\n",
      "Warning: nan gradient found. The current loss is:  0.6870328187942505\n",
      "Warning: nan gradient found. The current loss is:  0.573171079158783\n",
      "Warning: nan gradient found. The current loss is:  0.4673054814338684\n",
      "Warning: nan gradient found. The current loss is:  0.32229292392730713\n",
      "Warning: nan gradient found. The current loss is:  0.29017943143844604\n",
      "Warning: nan gradient found. The current loss is:  0.45476144552230835\n",
      "Warning: nan gradient found. The current loss is:  0.9769477844238281\n",
      "Current batch training loss: 0.976948  [1612800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5644261837005615\n",
      "Warning: nan gradient found. The current loss is:  0.32383233308792114\n",
      "Warning: nan gradient found. The current loss is:  0.7042931318283081\n",
      "Warning: nan gradient found. The current loss is:  0.46684569120407104\n",
      "Warning: nan gradient found. The current loss is:  0.28046470880508423\n",
      "Warning: nan gradient found. The current loss is:  0.8999482989311218\n",
      "Warning: nan gradient found. The current loss is:  0.9362279176712036\n",
      "Warning: nan gradient found. The current loss is:  0.7150932550430298\n",
      "Warning: nan gradient found. The current loss is:  0.6962380409240723\n",
      "Warning: nan gradient found. The current loss is:  0.25705358386039734\n",
      "Warning: nan gradient found. The current loss is:  0.19603681564331055\n",
      "Warning: nan gradient found. The current loss is:  3.408034563064575\n",
      "Warning: nan gradient found. The current loss is:  0.9639670848846436\n",
      "Warning: nan gradient found. The current loss is:  1.004800796508789\n",
      "Warning: nan gradient found. The current loss is:  0.800317645072937\n",
      "Warning: nan gradient found. The current loss is:  0.9932773113250732\n",
      "Warning: nan gradient found. The current loss is:  0.3422124981880188\n",
      "Warning: nan gradient found. The current loss is:  0.062229663133621216\n",
      "Warning: nan gradient found. The current loss is:  0.5557795166969299\n",
      "Warning: nan gradient found. The current loss is:  0.2980426549911499\n",
      "Warning: nan gradient found. The current loss is:  0.7953343391418457\n",
      "Warning: nan gradient found. The current loss is:  0.5626336336135864\n",
      "Warning: nan gradient found. The current loss is:  1.0195469856262207\n",
      "Warning: nan gradient found. The current loss is:  0.680396556854248\n",
      "Warning: nan gradient found. The current loss is:  0.466670423746109\n",
      "Warning: nan gradient found. The current loss is:  0.8674811720848083\n",
      "Warning: nan gradient found. The current loss is:  0.6331533789634705\n",
      "Warning: nan gradient found. The current loss is:  0.5382971167564392\n",
      "Warning: nan gradient found. The current loss is:  0.46795254945755005\n",
      "Warning: nan gradient found. The current loss is:  0.27508360147476196\n",
      "Warning: nan gradient found. The current loss is:  0.7888945937156677\n",
      "Warning: nan gradient found. The current loss is:  0.7924711108207703\n",
      "Warning: nan gradient found. The current loss is:  0.4930744171142578\n",
      "Warning: nan gradient found. The current loss is:  0.23777151107788086\n",
      "Warning: nan gradient found. The current loss is:  0.6016913652420044\n",
      "Warning: nan gradient found. The current loss is:  0.6843387484550476\n",
      "Warning: nan gradient found. The current loss is:  0.12707176804542542\n",
      "Warning: nan gradient found. The current loss is:  0.8109034299850464\n",
      "Warning: nan gradient found. The current loss is:  0.7062307596206665\n",
      "Warning: nan gradient found. The current loss is:  0.13957098126411438\n",
      "Warning: nan gradient found. The current loss is:  0.5260798931121826\n",
      "Warning: nan gradient found. The current loss is:  0.2591932415962219\n",
      "Warning: nan gradient found. The current loss is:  0.5075130462646484\n",
      "Warning: nan gradient found. The current loss is:  0.8134187459945679\n",
      "Warning: nan gradient found. The current loss is:  1.0883747339248657\n",
      "Warning: nan gradient found. The current loss is:  2.4877219200134277\n",
      "Warning: nan gradient found. The current loss is:  0.5027233958244324\n",
      "Warning: nan gradient found. The current loss is:  0.9239929914474487\n",
      "Warning: nan gradient found. The current loss is:  0.5397123098373413\n",
      "Warning: nan gradient found. The current loss is:  0.8533202409744263\n",
      "Warning: nan gradient found. The current loss is:  0.8556181788444519\n",
      "Warning: nan gradient found. The current loss is:  0.8306143283843994\n",
      "Warning: nan gradient found. The current loss is:  1.248314619064331\n",
      "Warning: nan gradient found. The current loss is:  0.9499527812004089\n",
      "Warning: nan gradient found. The current loss is:  0.3789529800415039\n",
      "Warning: nan gradient found. The current loss is:  2.116297721862793\n",
      "Warning: nan gradient found. The current loss is:  0.223183736205101\n",
      "Warning: nan gradient found. The current loss is:  0.7384692430496216\n",
      "Warning: nan gradient found. The current loss is:  0.7097769975662231\n",
      "Warning: nan gradient found. The current loss is:  1.3488322496414185\n",
      "Warning: nan gradient found. The current loss is:  0.9133456945419312\n",
      "Warning: nan gradient found. The current loss is:  0.546249508857727\n",
      "Warning: nan gradient found. The current loss is:  0.33310139179229736\n",
      "Warning: nan gradient found. The current loss is:  0.9713293313980103\n",
      "Warning: nan gradient found. The current loss is:  0.5290707349777222\n",
      "Warning: nan gradient found. The current loss is:  0.49279946088790894\n",
      "Warning: nan gradient found. The current loss is:  0.6778026819229126\n",
      "Warning: nan gradient found. The current loss is:  1.1718779802322388\n",
      "Warning: nan gradient found. The current loss is:  0.3095870614051819\n",
      "Warning: nan gradient found. The current loss is:  0.4510734975337982\n",
      "Warning: nan gradient found. The current loss is:  1.087456226348877\n",
      "Warning: nan gradient found. The current loss is:  0.36347097158432007\n",
      "Warning: nan gradient found. The current loss is:  0.7510449886322021\n",
      "Warning: nan gradient found. The current loss is:  0.7887222170829773\n",
      "Warning: nan gradient found. The current loss is:  1.3209744691848755\n",
      "Warning: nan gradient found. The current loss is:  0.710299015045166\n",
      "Warning: nan gradient found. The current loss is:  1.6220453977584839\n",
      "Warning: nan gradient found. The current loss is:  1.3633410930633545\n",
      "Warning: nan gradient found. The current loss is:  0.48943647742271423\n",
      "Warning: nan gradient found. The current loss is:  0.5649821758270264\n",
      "Warning: nan gradient found. The current loss is:  0.37296009063720703\n",
      "Warning: nan gradient found. The current loss is:  0.43103277683258057\n",
      "Warning: nan gradient found. The current loss is:  1.2367281913757324\n",
      "Warning: nan gradient found. The current loss is:  1.5511577129364014\n",
      "Warning: nan gradient found. The current loss is:  0.22022148966789246\n",
      "Warning: nan gradient found. The current loss is:  0.34818315505981445\n",
      "Warning: nan gradient found. The current loss is:  0.45142149925231934\n",
      "Warning: nan gradient found. The current loss is:  1.2502319812774658\n",
      "Warning: nan gradient found. The current loss is:  0.5391077399253845\n",
      "Warning: nan gradient found. The current loss is:  0.6091257333755493\n",
      "Warning: nan gradient found. The current loss is:  1.4387496709823608\n",
      "Warning: nan gradient found. The current loss is:  2.1323635578155518\n",
      "Warning: nan gradient found. The current loss is:  0.6884811520576477\n",
      "Warning: nan gradient found. The current loss is:  2.1014091968536377\n",
      "Warning: nan gradient found. The current loss is:  0.9968852400779724\n",
      "Warning: nan gradient found. The current loss is:  0.6640744209289551\n",
      "Warning: nan gradient found. The current loss is:  0.8060149550437927\n",
      "Warning: nan gradient found. The current loss is:  0.28310731053352356\n",
      "Warning: nan gradient found. The current loss is:  0.3195222318172455\n",
      "Warning: nan gradient found. The current loss is:  1.5663812160491943\n",
      "Current batch training loss: 1.566381  [1638400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7173408269882202\n",
      "Warning: nan gradient found. The current loss is:  0.704474687576294\n",
      "Warning: nan gradient found. The current loss is:  1.0438517332077026\n",
      "Warning: nan gradient found. The current loss is:  0.5208390355110168\n",
      "Warning: nan gradient found. The current loss is:  0.6046201586723328\n",
      "Warning: nan gradient found. The current loss is:  0.8609955906867981\n",
      "Warning: nan gradient found. The current loss is:  0.5292931199073792\n",
      "Warning: nan gradient found. The current loss is:  0.31932395696640015\n",
      "Warning: nan gradient found. The current loss is:  0.8153324723243713\n",
      "Warning: nan gradient found. The current loss is:  0.31679466366767883\n",
      "Warning: nan gradient found. The current loss is:  0.9320963025093079\n",
      "Warning: nan gradient found. The current loss is:  1.7476222515106201\n",
      "Warning: nan gradient found. The current loss is:  0.7749307155609131\n",
      "Warning: nan gradient found. The current loss is:  0.4649847745895386\n",
      "Warning: nan gradient found. The current loss is:  0.3586880564689636\n",
      "Warning: nan gradient found. The current loss is:  0.4088802933692932\n",
      "Warning: nan gradient found. The current loss is:  0.6418756246566772\n",
      "Warning: nan gradient found. The current loss is:  0.6440132856369019\n",
      "Warning: nan gradient found. The current loss is:  0.7672725915908813\n",
      "Warning: nan gradient found. The current loss is:  0.9167388677597046\n",
      "Warning: nan gradient found. The current loss is:  0.6881771683692932\n",
      "Warning: nan gradient found. The current loss is:  0.21179711818695068\n",
      "Warning: nan gradient found. The current loss is:  0.14590314030647278\n",
      "Warning: nan gradient found. The current loss is:  0.40680840611457825\n",
      "Warning: nan gradient found. The current loss is:  0.9910205006599426\n",
      "Warning: nan gradient found. The current loss is:  0.9350799322128296\n",
      "Warning: nan gradient found. The current loss is:  0.38880372047424316\n",
      "Warning: nan gradient found. The current loss is:  0.5386095643043518\n",
      "Warning: nan gradient found. The current loss is:  0.8618344068527222\n",
      "Warning: nan gradient found. The current loss is:  0.35041388869285583\n",
      "Warning: nan gradient found. The current loss is:  1.9755194187164307\n",
      "Warning: nan gradient found. The current loss is:  0.8439998626708984\n",
      "Warning: nan gradient found. The current loss is:  1.1170902252197266\n",
      "Warning: nan gradient found. The current loss is:  0.6455651521682739\n",
      "Warning: nan gradient found. The current loss is:  1.303637981414795\n",
      "Warning: nan gradient found. The current loss is:  0.6464201211929321\n",
      "Warning: nan gradient found. The current loss is:  1.1426904201507568\n",
      "Warning: nan gradient found. The current loss is:  0.7996320724487305\n",
      "Warning: nan gradient found. The current loss is:  0.862604558467865\n",
      "Warning: nan gradient found. The current loss is:  0.6904510855674744\n",
      "Warning: nan gradient found. The current loss is:  1.1604647636413574\n",
      "Warning: nan gradient found. The current loss is:  0.917777419090271\n",
      "Warning: nan gradient found. The current loss is:  0.5725948810577393\n",
      "Warning: nan gradient found. The current loss is:  0.42733174562454224\n",
      "Warning: nan gradient found. The current loss is:  1.236125111579895\n",
      "Warning: nan gradient found. The current loss is:  0.2756139934062958\n",
      "Warning: nan gradient found. The current loss is:  1.923761010169983\n",
      "Warning: nan gradient found. The current loss is:  0.6533724069595337\n",
      "Warning: nan gradient found. The current loss is:  0.3439263105392456\n",
      "Warning: nan gradient found. The current loss is:  0.3147701025009155\n",
      "Warning: nan gradient found. The current loss is:  0.47774791717529297\n",
      "Warning: nan gradient found. The current loss is:  0.22433365881443024\n",
      "Warning: nan gradient found. The current loss is:  0.4225921928882599\n",
      "Warning: nan gradient found. The current loss is:  1.5881073474884033\n",
      "Warning: nan gradient found. The current loss is:  0.12363883852958679\n",
      "Warning: nan gradient found. The current loss is:  0.603202760219574\n",
      "Warning: nan gradient found. The current loss is:  0.6558303833007812\n",
      "Warning: nan gradient found. The current loss is:  0.40595167875289917\n",
      "Warning: nan gradient found. The current loss is:  0.18957509100437164\n",
      "Warning: nan gradient found. The current loss is:  0.4316314160823822\n",
      "Warning: nan gradient found. The current loss is:  0.33186525106430054\n",
      "Warning: nan gradient found. The current loss is:  0.659855842590332\n",
      "Warning: nan gradient found. The current loss is:  0.6849327683448792\n",
      "Warning: nan gradient found. The current loss is:  0.8455678820610046\n",
      "Warning: nan gradient found. The current loss is:  1.0202455520629883\n",
      "Warning: nan gradient found. The current loss is:  0.1394382119178772\n",
      "Warning: nan gradient found. The current loss is:  0.6044383645057678\n",
      "Warning: nan gradient found. The current loss is:  1.2180936336517334\n",
      "Warning: nan gradient found. The current loss is:  0.49560993909835815\n",
      "Warning: nan gradient found. The current loss is:  -0.06601639091968536\n",
      "Warning: nan gradient found. The current loss is:  0.8797122240066528\n",
      "Warning: nan gradient found. The current loss is:  0.9463995695114136\n",
      "Warning: nan gradient found. The current loss is:  0.2800537943840027\n",
      "Warning: nan gradient found. The current loss is:  0.35381871461868286\n",
      "Warning: nan gradient found. The current loss is:  0.07443563640117645\n",
      "Warning: nan gradient found. The current loss is:  0.23672914505004883\n",
      "Warning: nan gradient found. The current loss is:  1.1774119138717651\n",
      "Warning: nan gradient found. The current loss is:  0.8174605965614319\n",
      "Warning: nan gradient found. The current loss is:  0.26624876260757446\n",
      "Warning: nan gradient found. The current loss is:  0.2517927587032318\n",
      "Warning: nan gradient found. The current loss is:  0.46906399726867676\n",
      "Warning: nan gradient found. The current loss is:  1.6205072402954102\n",
      "Warning: nan gradient found. The current loss is:  0.8324315547943115\n",
      "Warning: nan gradient found. The current loss is:  0.7637288570404053\n",
      "Warning: nan gradient found. The current loss is:  0.6185785531997681\n",
      "Warning: nan gradient found. The current loss is:  0.4882928729057312\n",
      "Warning: nan gradient found. The current loss is:  0.8598814606666565\n",
      "Warning: nan gradient found. The current loss is:  0.22643443942070007\n",
      "Warning: nan gradient found. The current loss is:  0.666253924369812\n",
      "Warning: nan gradient found. The current loss is:  0.4071387052536011\n",
      "Warning: nan gradient found. The current loss is:  1.1340388059616089\n",
      "Warning: nan gradient found. The current loss is:  0.6172076463699341\n",
      "Warning: nan gradient found. The current loss is:  0.22453412413597107\n",
      "Warning: nan gradient found. The current loss is:  1.697616457939148\n",
      "Warning: nan gradient found. The current loss is:  0.8616502285003662\n",
      "Warning: nan gradient found. The current loss is:  0.9855406284332275\n",
      "Warning: nan gradient found. The current loss is:  0.3209676742553711\n",
      "Warning: nan gradient found. The current loss is:  0.4624165892601013\n",
      "Warning: nan gradient found. The current loss is:  0.5137230157852173\n",
      "Warning: nan gradient found. The current loss is:  0.6916344165802002\n",
      "Current batch training loss: 0.691634  [1664000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.7850841283798218\n",
      "Warning: nan gradient found. The current loss is:  1.812286376953125\n",
      "Warning: nan gradient found. The current loss is:  0.8252555131912231\n",
      "Warning: nan gradient found. The current loss is:  0.8493008613586426\n",
      "Warning: nan gradient found. The current loss is:  0.621834397315979\n",
      "Warning: nan gradient found. The current loss is:  0.28094467520713806\n",
      "Warning: nan gradient found. The current loss is:  1.0512218475341797\n",
      "Warning: nan gradient found. The current loss is:  0.9019603729248047\n",
      "Warning: nan gradient found. The current loss is:  1.4947559833526611\n",
      "Warning: nan gradient found. The current loss is:  0.8945426344871521\n",
      "Warning: nan gradient found. The current loss is:  0.7468775510787964\n",
      "Warning: nan gradient found. The current loss is:  0.5260918140411377\n",
      "Warning: nan gradient found. The current loss is:  0.9744113683700562\n",
      "Warning: nan gradient found. The current loss is:  1.0790090560913086\n",
      "Warning: nan gradient found. The current loss is:  0.4357224702835083\n",
      "Warning: nan gradient found. The current loss is:  2.8352675437927246\n",
      "Warning: nan gradient found. The current loss is:  2.001857042312622\n",
      "Warning: nan gradient found. The current loss is:  0.8889337182044983\n",
      "Warning: nan gradient found. The current loss is:  1.3476169109344482\n",
      "Warning: nan gradient found. The current loss is:  0.5144016742706299\n",
      "Warning: nan gradient found. The current loss is:  0.4119410514831543\n",
      "Warning: nan gradient found. The current loss is:  1.2537072896957397\n",
      "Warning: nan gradient found. The current loss is:  0.27396053075790405\n",
      "Warning: nan gradient found. The current loss is:  1.139894962310791\n",
      "Warning: nan gradient found. The current loss is:  0.3190258741378784\n",
      "Warning: nan gradient found. The current loss is:  0.5902544856071472\n",
      "Warning: nan gradient found. The current loss is:  0.9690748453140259\n",
      "Warning: nan gradient found. The current loss is:  0.5769355893135071\n",
      "Warning: nan gradient found. The current loss is:  0.8641250729560852\n",
      "Warning: nan gradient found. The current loss is:  0.3160426616668701\n",
      "Warning: nan gradient found. The current loss is:  0.7343096733093262\n",
      "Warning: nan gradient found. The current loss is:  1.098836064338684\n",
      "Warning: nan gradient found. The current loss is:  0.6392018795013428\n",
      "Warning: nan gradient found. The current loss is:  0.6714568138122559\n",
      "Warning: nan gradient found. The current loss is:  0.8294402360916138\n",
      "Warning: nan gradient found. The current loss is:  0.7741521000862122\n",
      "Warning: nan gradient found. The current loss is:  0.6881154775619507\n",
      "Warning: nan gradient found. The current loss is:  1.0699796676635742\n",
      "Warning: nan gradient found. The current loss is:  0.4253906011581421\n",
      "Warning: nan gradient found. The current loss is:  0.8053628206253052\n",
      "Warning: nan gradient found. The current loss is:  0.28846943378448486\n",
      "Warning: nan gradient found. The current loss is:  1.061206340789795\n",
      "Warning: nan gradient found. The current loss is:  0.533069908618927\n",
      "Warning: nan gradient found. The current loss is:  0.44697439670562744\n",
      "Warning: nan gradient found. The current loss is:  0.9414006471633911\n",
      "Warning: nan gradient found. The current loss is:  0.5893944501876831\n",
      "Warning: nan gradient found. The current loss is:  0.27162784337997437\n",
      "Warning: nan gradient found. The current loss is:  1.1017835140228271\n",
      "Warning: nan gradient found. The current loss is:  0.6134001612663269\n",
      "Warning: nan gradient found. The current loss is:  1.15829598903656\n",
      "Warning: nan gradient found. The current loss is:  0.9316823482513428\n",
      "Warning: nan gradient found. The current loss is:  0.5414615273475647\n",
      "Warning: nan gradient found. The current loss is:  2.020596742630005\n",
      "Warning: nan gradient found. The current loss is:  0.5822203159332275\n",
      "Warning: nan gradient found. The current loss is:  1.0407888889312744\n",
      "Warning: nan gradient found. The current loss is:  0.15809296071529388\n",
      "Warning: nan gradient found. The current loss is:  0.9263620972633362\n",
      "Warning: nan gradient found. The current loss is:  0.901684045791626\n",
      "Warning: nan gradient found. The current loss is:  1.581925868988037\n",
      "Warning: nan gradient found. The current loss is:  0.534679651260376\n",
      "Warning: nan gradient found. The current loss is:  0.6547821760177612\n",
      "Warning: nan gradient found. The current loss is:  0.5562974214553833\n",
      "Warning: nan gradient found. The current loss is:  0.4448695182800293\n",
      "Warning: nan gradient found. The current loss is:  0.6065525412559509\n",
      "Warning: nan gradient found. The current loss is:  0.6041156053543091\n",
      "Warning: nan gradient found. The current loss is:  0.274575412273407\n",
      "Warning: nan gradient found. The current loss is:  0.9495944976806641\n",
      "Warning: nan gradient found. The current loss is:  1.2228384017944336\n",
      "Warning: nan gradient found. The current loss is:  0.5938317179679871\n",
      "Warning: nan gradient found. The current loss is:  0.355993390083313\n",
      "Warning: nan gradient found. The current loss is:  0.9313681125640869\n",
      "Warning: nan gradient found. The current loss is:  1.5610774755477905\n",
      "Warning: nan gradient found. The current loss is:  1.0644501447677612\n",
      "Warning: nan gradient found. The current loss is:  1.0976539850234985\n",
      "Warning: nan gradient found. The current loss is:  0.9645002484321594\n",
      "Warning: nan gradient found. The current loss is:  1.135692834854126\n",
      "Warning: nan gradient found. The current loss is:  0.2535160779953003\n",
      "Warning: nan gradient found. The current loss is:  1.1631920337677002\n",
      "Warning: nan gradient found. The current loss is:  0.3692648410797119\n",
      "Warning: nan gradient found. The current loss is:  1.0588549375534058\n",
      "Warning: nan gradient found. The current loss is:  1.2991540431976318\n",
      "Warning: nan gradient found. The current loss is:  0.28750962018966675\n",
      "Warning: nan gradient found. The current loss is:  1.1509432792663574\n",
      "Warning: nan gradient found. The current loss is:  0.7618115544319153\n",
      "Warning: nan gradient found. The current loss is:  0.5088452696800232\n",
      "Warning: nan gradient found. The current loss is:  0.6612257957458496\n",
      "Warning: nan gradient found. The current loss is:  0.10724703967571259\n",
      "Warning: nan gradient found. The current loss is:  0.715270459651947\n",
      "Warning: nan gradient found. The current loss is:  1.0073820352554321\n",
      "Warning: nan gradient found. The current loss is:  0.8119158148765564\n",
      "Warning: nan gradient found. The current loss is:  0.2796201705932617\n",
      "Warning: nan gradient found. The current loss is:  0.42475083470344543\n",
      "Warning: nan gradient found. The current loss is:  0.6726773977279663\n",
      "Warning: nan gradient found. The current loss is:  0.10595384240150452\n",
      "Warning: nan gradient found. The current loss is:  0.9505267143249512\n",
      "Warning: nan gradient found. The current loss is:  0.48987576365470886\n",
      "Warning: nan gradient found. The current loss is:  0.5277872681617737\n",
      "Warning: nan gradient found. The current loss is:  0.5456099510192871\n",
      "Warning: nan gradient found. The current loss is:  0.4370483160018921\n",
      "Warning: nan gradient found. The current loss is:  0.6079676747322083\n",
      "Current batch training loss: 0.607968  [1689600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.45514804124832153\n",
      "Warning: nan gradient found. The current loss is:  0.6963373422622681\n",
      "Warning: nan gradient found. The current loss is:  1.025524616241455\n",
      "Warning: nan gradient found. The current loss is:  0.6997127532958984\n",
      "Warning: nan gradient found. The current loss is:  0.6881796717643738\n",
      "Warning: nan gradient found. The current loss is:  0.6752709150314331\n",
      "Warning: nan gradient found. The current loss is:  0.3923424780368805\n",
      "Warning: nan gradient found. The current loss is:  1.3604730367660522\n",
      "Warning: nan gradient found. The current loss is:  0.6530880928039551\n",
      "Warning: nan gradient found. The current loss is:  0.5725007057189941\n",
      "Warning: nan gradient found. The current loss is:  1.0980889797210693\n",
      "Warning: nan gradient found. The current loss is:  0.7321141958236694\n",
      "Warning: nan gradient found. The current loss is:  0.5021900534629822\n",
      "Warning: nan gradient found. The current loss is:  0.7390702962875366\n",
      "Warning: nan gradient found. The current loss is:  0.4331008493900299\n",
      "Warning: nan gradient found. The current loss is:  0.8394534587860107\n",
      "Warning: nan gradient found. The current loss is:  0.4880121946334839\n",
      "Warning: nan gradient found. The current loss is:  0.7480086088180542\n",
      "Warning: nan gradient found. The current loss is:  0.6643106937408447\n",
      "Warning: nan gradient found. The current loss is:  0.28343814611434937\n",
      "Warning: nan gradient found. The current loss is:  1.0222901105880737\n",
      "Warning: nan gradient found. The current loss is:  0.04932301864027977\n",
      "Warning: nan gradient found. The current loss is:  0.6593631505966187\n",
      "Warning: nan gradient found. The current loss is:  0.915709376335144\n",
      "Warning: nan gradient found. The current loss is:  0.573431670665741\n",
      "Warning: nan gradient found. The current loss is:  0.2885739505290985\n",
      "Warning: nan gradient found. The current loss is:  1.573498010635376\n",
      "Warning: nan gradient found. The current loss is:  0.9347283244132996\n",
      "Warning: nan gradient found. The current loss is:  0.4973846673965454\n",
      "Warning: nan gradient found. The current loss is:  0.12687315046787262\n",
      "Warning: nan gradient found. The current loss is:  0.0586448535323143\n",
      "Warning: nan gradient found. The current loss is:  0.6947625875473022\n",
      "Warning: nan gradient found. The current loss is:  0.6970833539962769\n",
      "Warning: nan gradient found. The current loss is:  0.5720268487930298\n",
      "Warning: nan gradient found. The current loss is:  1.526456594467163\n",
      "Warning: nan gradient found. The current loss is:  0.17673540115356445\n",
      "Warning: nan gradient found. The current loss is:  0.7755422592163086\n",
      "Warning: nan gradient found. The current loss is:  1.1836837530136108\n",
      "Warning: nan gradient found. The current loss is:  0.8259919881820679\n",
      "Warning: nan gradient found. The current loss is:  1.1007565259933472\n",
      "Warning: nan gradient found. The current loss is:  1.0832493305206299\n",
      "Warning: nan gradient found. The current loss is:  0.6776034832000732\n",
      "Warning: nan gradient found. The current loss is:  0.382439523935318\n",
      "Warning: nan gradient found. The current loss is:  0.4848089814186096\n",
      "Warning: nan gradient found. The current loss is:  0.4389440417289734\n",
      "Warning: nan gradient found. The current loss is:  0.7618667483329773\n",
      "Warning: nan gradient found. The current loss is:  0.32303476333618164\n",
      "Warning: nan gradient found. The current loss is:  0.22935505211353302\n",
      "Warning: nan gradient found. The current loss is:  0.9236583709716797\n",
      "Warning: nan gradient found. The current loss is:  0.8561989068984985\n",
      "Warning: nan gradient found. The current loss is:  0.4752202332019806\n",
      "Warning: nan gradient found. The current loss is:  0.7148185968399048\n",
      "Warning: nan gradient found. The current loss is:  1.0198750495910645\n",
      "Warning: nan gradient found. The current loss is:  1.9752655029296875\n",
      "Warning: nan gradient found. The current loss is:  0.7640650272369385\n",
      "Warning: nan gradient found. The current loss is:  0.5231007933616638\n",
      "Warning: nan gradient found. The current loss is:  0.4655340313911438\n",
      "Warning: nan gradient found. The current loss is:  0.8507194519042969\n",
      "Warning: nan gradient found. The current loss is:  0.6540457010269165\n",
      "Warning: nan gradient found. The current loss is:  0.947589635848999\n",
      "Warning: nan gradient found. The current loss is:  1.0181981325149536\n",
      "Warning: nan gradient found. The current loss is:  0.7782935500144958\n",
      "Warning: nan gradient found. The current loss is:  0.8367798328399658\n",
      "Warning: nan gradient found. The current loss is:  0.5719007253646851\n",
      "Warning: nan gradient found. The current loss is:  1.4328876733779907\n",
      "Warning: nan gradient found. The current loss is:  1.2242056131362915\n",
      "Warning: nan gradient found. The current loss is:  0.7095082402229309\n",
      "Warning: nan gradient found. The current loss is:  1.3536441326141357\n",
      "Warning: nan gradient found. The current loss is:  0.35021522641181946\n",
      "Warning: nan gradient found. The current loss is:  1.1109355688095093\n",
      "Warning: nan gradient found. The current loss is:  0.15692943334579468\n",
      "Warning: nan gradient found. The current loss is:  0.5812856554985046\n",
      "Warning: nan gradient found. The current loss is:  1.0332982540130615\n",
      "Warning: nan gradient found. The current loss is:  0.4136725664138794\n",
      "Warning: nan gradient found. The current loss is:  0.46680256724357605\n",
      "Warning: nan gradient found. The current loss is:  0.349288672208786\n",
      "Warning: nan gradient found. The current loss is:  0.37681955099105835\n",
      "Warning: nan gradient found. The current loss is:  0.7484385967254639\n",
      "Warning: nan gradient found. The current loss is:  0.0440363809466362\n",
      "Warning: nan gradient found. The current loss is:  0.43276843428611755\n",
      "Warning: nan gradient found. The current loss is:  0.6562167406082153\n",
      "Warning: nan gradient found. The current loss is:  1.1138590574264526\n",
      "Warning: nan gradient found. The current loss is:  0.7442128658294678\n",
      "Warning: nan gradient found. The current loss is:  0.5286716222763062\n",
      "Warning: nan gradient found. The current loss is:  0.8111891746520996\n",
      "Warning: nan gradient found. The current loss is:  0.11060187220573425\n",
      "Warning: nan gradient found. The current loss is:  0.36295366287231445\n",
      "Warning: nan gradient found. The current loss is:  0.7015659809112549\n",
      "Warning: nan gradient found. The current loss is:  -0.09328041225671768\n",
      "Warning: nan gradient found. The current loss is:  0.6506413221359253\n",
      "Warning: nan gradient found. The current loss is:  1.8373092412948608\n",
      "Warning: nan gradient found. The current loss is:  0.5066895484924316\n",
      "Warning: nan gradient found. The current loss is:  0.7661864757537842\n",
      "Warning: nan gradient found. The current loss is:  1.303310751914978\n",
      "Warning: nan gradient found. The current loss is:  0.47273167967796326\n",
      "Warning: nan gradient found. The current loss is:  0.4542798101902008\n",
      "Warning: nan gradient found. The current loss is:  1.1473345756530762\n",
      "Warning: nan gradient found. The current loss is:  1.2741148471832275\n",
      "Warning: nan gradient found. The current loss is:  0.5189517736434937\n",
      "Warning: nan gradient found. The current loss is:  0.8504953980445862\n",
      "Current batch training loss: 0.850495  [1715200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6265672445297241\n",
      "Warning: nan gradient found. The current loss is:  1.6207817792892456\n",
      "Warning: nan gradient found. The current loss is:  0.833672285079956\n",
      "Warning: nan gradient found. The current loss is:  3.3375377655029297\n",
      "Warning: nan gradient found. The current loss is:  1.0269947052001953\n",
      "Warning: nan gradient found. The current loss is:  0.06751812994480133\n",
      "Warning: nan gradient found. The current loss is:  0.0948818176984787\n",
      "Warning: nan gradient found. The current loss is:  0.6751025915145874\n",
      "Warning: nan gradient found. The current loss is:  0.9049999713897705\n",
      "Warning: nan gradient found. The current loss is:  0.5848730802536011\n",
      "Warning: nan gradient found. The current loss is:  0.952938437461853\n",
      "Warning: nan gradient found. The current loss is:  0.46433550119400024\n",
      "Warning: nan gradient found. The current loss is:  0.15353137254714966\n",
      "Warning: nan gradient found. The current loss is:  0.7387707233428955\n",
      "Warning: nan gradient found. The current loss is:  0.6433457732200623\n",
      "Warning: nan gradient found. The current loss is:  0.3974829912185669\n",
      "Warning: nan gradient found. The current loss is:  0.3658513128757477\n",
      "Warning: nan gradient found. The current loss is:  0.4075494110584259\n",
      "Warning: nan gradient found. The current loss is:  0.25963306427001953\n",
      "Warning: nan gradient found. The current loss is:  0.8728334903717041\n",
      "Warning: nan gradient found. The current loss is:  1.266711950302124\n",
      "Warning: nan gradient found. The current loss is:  0.3382548391819\n",
      "Warning: nan gradient found. The current loss is:  0.38656383752822876\n",
      "Warning: nan gradient found. The current loss is:  1.3188631534576416\n",
      "Warning: nan gradient found. The current loss is:  0.04543197527527809\n",
      "Warning: nan gradient found. The current loss is:  0.641575038433075\n",
      "Warning: nan gradient found. The current loss is:  0.3533196449279785\n",
      "Warning: nan gradient found. The current loss is:  0.31314176321029663\n",
      "Warning: nan gradient found. The current loss is:  1.255667805671692\n",
      "Warning: nan gradient found. The current loss is:  0.526124119758606\n",
      "Warning: nan gradient found. The current loss is:  1.0921037197113037\n",
      "Warning: nan gradient found. The current loss is:  1.0040302276611328\n",
      "Warning: nan gradient found. The current loss is:  0.4640694558620453\n",
      "Warning: nan gradient found. The current loss is:  1.501382827758789\n",
      "Warning: nan gradient found. The current loss is:  2.6035428047180176\n",
      "Warning: nan gradient found. The current loss is:  0.9904280304908752\n",
      "Warning: nan gradient found. The current loss is:  0.7568585872650146\n",
      "Warning: nan gradient found. The current loss is:  1.228759765625\n",
      "Warning: nan gradient found. The current loss is:  0.6654280424118042\n",
      "Warning: nan gradient found. The current loss is:  0.905704915523529\n",
      "Warning: nan gradient found. The current loss is:  0.5695056915283203\n",
      "Warning: nan gradient found. The current loss is:  0.5247015953063965\n",
      "Warning: nan gradient found. The current loss is:  1.2608635425567627\n",
      "Warning: nan gradient found. The current loss is:  0.7729466557502747\n",
      "Warning: nan gradient found. The current loss is:  0.4834586977958679\n",
      "Warning: nan gradient found. The current loss is:  0.5647743344306946\n",
      "Warning: nan gradient found. The current loss is:  0.2248314619064331\n",
      "Warning: nan gradient found. The current loss is:  0.4857271611690521\n",
      "Warning: nan gradient found. The current loss is:  0.5152408480644226\n",
      "Warning: nan gradient found. The current loss is:  0.1836244761943817\n",
      "Warning: nan gradient found. The current loss is:  0.2511415481567383\n",
      "Warning: nan gradient found. The current loss is:  0.1757519543170929\n",
      "Warning: nan gradient found. The current loss is:  0.3156760036945343\n",
      "Warning: nan gradient found. The current loss is:  0.613060712814331\n",
      "Warning: nan gradient found. The current loss is:  0.18869994580745697\n",
      "Warning: nan gradient found. The current loss is:  0.2542077302932739\n",
      "Warning: nan gradient found. The current loss is:  0.8523349761962891\n",
      "Warning: nan gradient found. The current loss is:  1.009641408920288\n",
      "Warning: nan gradient found. The current loss is:  1.1135928630828857\n",
      "Warning: nan gradient found. The current loss is:  0.5379458665847778\n",
      "Warning: nan gradient found. The current loss is:  0.22393310070037842\n",
      "Warning: nan gradient found. The current loss is:  1.3150644302368164\n",
      "Warning: nan gradient found. The current loss is:  0.554462730884552\n",
      "Warning: nan gradient found. The current loss is:  0.45928889513015747\n",
      "Warning: nan gradient found. The current loss is:  0.388620525598526\n",
      "Warning: nan gradient found. The current loss is:  0.13041071593761444\n",
      "Warning: nan gradient found. The current loss is:  0.2055472880601883\n",
      "Warning: nan gradient found. The current loss is:  1.029158115386963\n",
      "Warning: nan gradient found. The current loss is:  2.833237648010254\n",
      "Warning: nan gradient found. The current loss is:  0.0966542512178421\n",
      "Warning: nan gradient found. The current loss is:  0.20042695105075836\n",
      "Warning: nan gradient found. The current loss is:  0.7881249785423279\n",
      "Warning: nan gradient found. The current loss is:  0.4025813341140747\n",
      "Warning: nan gradient found. The current loss is:  0.25069460272789\n",
      "Warning: nan gradient found. The current loss is:  0.5931625366210938\n",
      "Warning: nan gradient found. The current loss is:  0.8974926471710205\n",
      "Warning: nan gradient found. The current loss is:  0.41504985094070435\n",
      "Warning: nan gradient found. The current loss is:  0.6413521766662598\n",
      "Warning: nan gradient found. The current loss is:  0.771826982498169\n",
      "Warning: nan gradient found. The current loss is:  0.5680669546127319\n",
      "Warning: nan gradient found. The current loss is:  0.8689104318618774\n",
      "Warning: nan gradient found. The current loss is:  0.5182052850723267\n",
      "Warning: nan gradient found. The current loss is:  0.4876902103424072\n",
      "Warning: nan gradient found. The current loss is:  0.5386117696762085\n",
      "Warning: nan gradient found. The current loss is:  0.38201141357421875\n",
      "Warning: nan gradient found. The current loss is:  1.2355265617370605\n",
      "Warning: nan gradient found. The current loss is:  1.8514420986175537\n",
      "Warning: nan gradient found. The current loss is:  0.11231465637683868\n",
      "Warning: nan gradient found. The current loss is:  0.7866385579109192\n",
      "Warning: nan gradient found. The current loss is:  0.5013899803161621\n",
      "Warning: nan gradient found. The current loss is:  0.1160493940114975\n",
      "Warning: nan gradient found. The current loss is:  0.8046040534973145\n",
      "Warning: nan gradient found. The current loss is:  1.106780767440796\n",
      "Warning: nan gradient found. The current loss is:  1.2135090827941895\n",
      "Warning: nan gradient found. The current loss is:  0.5666173100471497\n",
      "Warning: nan gradient found. The current loss is:  2.3349602222442627\n",
      "Warning: nan gradient found. The current loss is:  0.4406103193759918\n",
      "Warning: nan gradient found. The current loss is:  0.5751667022705078\n",
      "Warning: nan gradient found. The current loss is:  0.912137508392334\n",
      "Warning: nan gradient found. The current loss is:  0.171747624874115\n",
      "Current batch training loss: 0.171748  [1740800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6332563161849976\n",
      "Warning: nan gradient found. The current loss is:  1.9542317390441895\n",
      "Warning: nan gradient found. The current loss is:  0.24161279201507568\n",
      "Warning: nan gradient found. The current loss is:  0.8028289675712585\n",
      "Warning: nan gradient found. The current loss is:  1.0692899227142334\n",
      "Warning: nan gradient found. The current loss is:  0.4816795587539673\n",
      "Warning: nan gradient found. The current loss is:  0.6162515878677368\n",
      "Warning: nan gradient found. The current loss is:  1.0153844356536865\n",
      "Warning: nan gradient found. The current loss is:  0.5554730296134949\n",
      "Warning: nan gradient found. The current loss is:  0.8566049337387085\n",
      "Warning: nan gradient found. The current loss is:  0.5450409054756165\n",
      "Warning: nan gradient found. The current loss is:  0.20442071557044983\n",
      "Warning: nan gradient found. The current loss is:  0.6995716094970703\n",
      "Warning: nan gradient found. The current loss is:  0.7241865396499634\n",
      "Warning: nan gradient found. The current loss is:  0.46402043104171753\n",
      "Warning: nan gradient found. The current loss is:  0.8359063267707825\n",
      "Warning: nan gradient found. The current loss is:  0.7130460739135742\n",
      "Warning: nan gradient found. The current loss is:  0.47445976734161377\n",
      "Warning: nan gradient found. The current loss is:  0.6227998733520508\n",
      "Warning: nan gradient found. The current loss is:  0.4915712773799896\n",
      "Warning: nan gradient found. The current loss is:  2.6065027713775635\n",
      "Warning: nan gradient found. The current loss is:  0.27603158354759216\n",
      "Warning: nan gradient found. The current loss is:  0.5282256603240967\n",
      "Warning: nan gradient found. The current loss is:  0.36757874488830566\n",
      "Warning: nan gradient found. The current loss is:  0.506805419921875\n",
      "Warning: nan gradient found. The current loss is:  0.9887089729309082\n",
      "Warning: nan gradient found. The current loss is:  1.0737429857254028\n",
      "Warning: nan gradient found. The current loss is:  0.5439993143081665\n",
      "Warning: nan gradient found. The current loss is:  0.21087411046028137\n",
      "Warning: nan gradient found. The current loss is:  0.5747588872909546\n",
      "Warning: nan gradient found. The current loss is:  0.7753015756607056\n",
      "Warning: nan gradient found. The current loss is:  0.02611907385289669\n",
      "Warning: nan gradient found. The current loss is:  0.5654634833335876\n",
      "Warning: nan gradient found. The current loss is:  0.30276238918304443\n",
      "Warning: nan gradient found. The current loss is:  0.4297131896018982\n",
      "Warning: nan gradient found. The current loss is:  1.3584933280944824\n",
      "Warning: nan gradient found. The current loss is:  0.8800321221351624\n",
      "Warning: nan gradient found. The current loss is:  1.0748209953308105\n",
      "Warning: nan gradient found. The current loss is:  1.3695999383926392\n",
      "Warning: nan gradient found. The current loss is:  0.7059645652770996\n",
      "Warning: nan gradient found. The current loss is:  0.7737233638763428\n",
      "Warning: nan gradient found. The current loss is:  0.4869644343852997\n",
      "Warning: nan gradient found. The current loss is:  0.9148668050765991\n",
      "Warning: nan gradient found. The current loss is:  0.40009403228759766\n",
      "Warning: nan gradient found. The current loss is:  0.3354971408843994\n",
      "Warning: nan gradient found. The current loss is:  0.4442397356033325\n",
      "Warning: nan gradient found. The current loss is:  0.6085671782493591\n",
      "Warning: nan gradient found. The current loss is:  0.2701294720172882\n",
      "Warning: nan gradient found. The current loss is:  0.3524326682090759\n",
      "Warning: nan gradient found. The current loss is:  0.6039795279502869\n",
      "Warning: nan gradient found. The current loss is:  0.5804581642150879\n",
      "Warning: nan gradient found. The current loss is:  0.929953932762146\n",
      "Warning: nan gradient found. The current loss is:  0.7201981544494629\n",
      "Warning: nan gradient found. The current loss is:  1.5266997814178467\n",
      "Warning: nan gradient found. The current loss is:  0.6806413531303406\n",
      "Warning: nan gradient found. The current loss is:  0.8293619155883789\n",
      "Warning: nan gradient found. The current loss is:  0.43071603775024414\n",
      "Warning: nan gradient found. The current loss is:  0.4408705234527588\n",
      "Warning: nan gradient found. The current loss is:  1.561069369316101\n",
      "Warning: nan gradient found. The current loss is:  0.2351219356060028\n",
      "Warning: nan gradient found. The current loss is:  0.5625748038291931\n",
      "Warning: nan gradient found. The current loss is:  0.70755934715271\n",
      "Warning: nan gradient found. The current loss is:  0.8569508790969849\n",
      "Warning: nan gradient found. The current loss is:  0.6383098363876343\n",
      "Warning: nan gradient found. The current loss is:  0.227000430226326\n",
      "Warning: nan gradient found. The current loss is:  0.18545615673065186\n",
      "Warning: nan gradient found. The current loss is:  0.9234125018119812\n",
      "Warning: nan gradient found. The current loss is:  1.0850536823272705\n",
      "Warning: nan gradient found. The current loss is:  0.7318364977836609\n",
      "Warning: nan gradient found. The current loss is:  0.5277917981147766\n",
      "Warning: nan gradient found. The current loss is:  1.2967948913574219\n",
      "Warning: nan gradient found. The current loss is:  1.6017427444458008\n",
      "Warning: nan gradient found. The current loss is:  0.6242521405220032\n",
      "Warning: nan gradient found. The current loss is:  0.7705760598182678\n",
      "Warning: nan gradient found. The current loss is:  0.4108281135559082\n",
      "Warning: nan gradient found. The current loss is:  0.23252242803573608\n",
      "Warning: nan gradient found. The current loss is:  0.14823585748672485\n",
      "Warning: nan gradient found. The current loss is:  0.5687220692634583\n",
      "Warning: nan gradient found. The current loss is:  1.3299951553344727\n",
      "Warning: nan gradient found. The current loss is:  0.7697126865386963\n",
      "Warning: nan gradient found. The current loss is:  0.8699204325675964\n",
      "Warning: nan gradient found. The current loss is:  0.8423981666564941\n",
      "Warning: nan gradient found. The current loss is:  0.4144836664199829\n",
      "Warning: nan gradient found. The current loss is:  0.6735780239105225\n",
      "Warning: nan gradient found. The current loss is:  0.32736480236053467\n",
      "Warning: nan gradient found. The current loss is:  0.43740105628967285\n",
      "Warning: nan gradient found. The current loss is:  0.2762356996536255\n",
      "Warning: nan gradient found. The current loss is:  0.6447831392288208\n",
      "Warning: nan gradient found. The current loss is:  0.4797682762145996\n",
      "Warning: nan gradient found. The current loss is:  0.6992914080619812\n",
      "Warning: nan gradient found. The current loss is:  1.1025934219360352\n",
      "Warning: nan gradient found. The current loss is:  1.205458641052246\n",
      "Warning: nan gradient found. The current loss is:  0.7378592491149902\n",
      "Warning: nan gradient found. The current loss is:  0.04983264207839966\n",
      "Warning: nan gradient found. The current loss is:  0.5621497631072998\n",
      "Warning: nan gradient found. The current loss is:  0.8832564353942871\n",
      "Warning: nan gradient found. The current loss is:  0.4853883385658264\n",
      "Warning: nan gradient found. The current loss is:  0.8102809190750122\n",
      "Warning: nan gradient found. The current loss is:  1.5718547105789185\n",
      "Warning: nan gradient found. The current loss is:  0.4923841953277588\n",
      "Current batch training loss: 0.492384  [1766400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4834117293357849\n",
      "Warning: nan gradient found. The current loss is:  0.5187579393386841\n",
      "Warning: nan gradient found. The current loss is:  0.31463271379470825\n",
      "Warning: nan gradient found. The current loss is:  0.8546223044395447\n",
      "Warning: nan gradient found. The current loss is:  1.288072109222412\n",
      "Warning: nan gradient found. The current loss is:  0.9601522088050842\n",
      "Warning: nan gradient found. The current loss is:  0.36139047145843506\n",
      "Warning: nan gradient found. The current loss is:  0.2537075877189636\n",
      "Warning: nan gradient found. The current loss is:  0.2745547592639923\n",
      "Warning: nan gradient found. The current loss is:  0.5733896493911743\n",
      "Warning: nan gradient found. The current loss is:  0.698486328125\n",
      "Warning: nan gradient found. The current loss is:  1.034077525138855\n",
      "Warning: nan gradient found. The current loss is:  1.0578078031539917\n",
      "Warning: nan gradient found. The current loss is:  0.6106419563293457\n",
      "Warning: nan gradient found. The current loss is:  1.4627864360809326\n",
      "Warning: nan gradient found. The current loss is:  0.8751240968704224\n",
      "Warning: nan gradient found. The current loss is:  2.7468655109405518\n",
      "Warning: nan gradient found. The current loss is:  0.8953307271003723\n",
      "Warning: nan gradient found. The current loss is:  0.36895328760147095\n",
      "Warning: nan gradient found. The current loss is:  1.039768934249878\n",
      "Warning: nan gradient found. The current loss is:  1.0097216367721558\n",
      "Warning: nan gradient found. The current loss is:  0.4624188244342804\n",
      "Warning: nan gradient found. The current loss is:  0.5670121908187866\n",
      "Warning: nan gradient found. The current loss is:  -0.05304710566997528\n",
      "Warning: nan gradient found. The current loss is:  1.075784683227539\n",
      "Warning: nan gradient found. The current loss is:  0.7772883176803589\n",
      "Warning: nan gradient found. The current loss is:  0.9604528546333313\n",
      "Warning: nan gradient found. The current loss is:  0.42120030522346497\n",
      "Warning: nan gradient found. The current loss is:  0.7467511892318726\n",
      "Warning: nan gradient found. The current loss is:  0.38744664192199707\n",
      "Warning: nan gradient found. The current loss is:  0.9565107226371765\n",
      "Warning: nan gradient found. The current loss is:  0.8524549603462219\n",
      "Warning: nan gradient found. The current loss is:  0.8281693458557129\n",
      "Warning: nan gradient found. The current loss is:  0.7522358894348145\n",
      "Warning: nan gradient found. The current loss is:  1.1046640872955322\n",
      "Warning: nan gradient found. The current loss is:  2.3245835304260254\n",
      "Warning: nan gradient found. The current loss is:  0.9515908360481262\n",
      "Warning: nan gradient found. The current loss is:  0.6643663048744202\n",
      "Warning: nan gradient found. The current loss is:  0.3771323561668396\n",
      "Warning: nan gradient found. The current loss is:  0.4916497468948364\n",
      "Warning: nan gradient found. The current loss is:  0.5276753902435303\n",
      "Warning: nan gradient found. The current loss is:  0.22742021083831787\n",
      "Warning: nan gradient found. The current loss is:  0.6974503993988037\n",
      "Warning: nan gradient found. The current loss is:  3.1129701137542725\n",
      "Warning: nan gradient found. The current loss is:  0.5345455408096313\n",
      "Warning: nan gradient found. The current loss is:  1.778334617614746\n",
      "Warning: nan gradient found. The current loss is:  0.5419419407844543\n",
      "Warning: nan gradient found. The current loss is:  1.0068544149398804\n",
      "Warning: nan gradient found. The current loss is:  0.39818477630615234\n",
      "Warning: nan gradient found. The current loss is:  1.1236155033111572\n",
      "Warning: nan gradient found. The current loss is:  0.3838917911052704\n",
      "Warning: nan gradient found. The current loss is:  0.5822511315345764\n",
      "Warning: nan gradient found. The current loss is:  0.3098694086074829\n",
      "Warning: nan gradient found. The current loss is:  1.3053441047668457\n",
      "Warning: nan gradient found. The current loss is:  0.6152341961860657\n",
      "Warning: nan gradient found. The current loss is:  0.29731255769729614\n",
      "Warning: nan gradient found. The current loss is:  0.734430730342865\n",
      "Warning: nan gradient found. The current loss is:  0.6008860468864441\n",
      "Warning: nan gradient found. The current loss is:  1.082807183265686\n",
      "Warning: nan gradient found. The current loss is:  0.45533427596092224\n",
      "Warning: nan gradient found. The current loss is:  0.34969204664230347\n",
      "Warning: nan gradient found. The current loss is:  0.6003285646438599\n",
      "Warning: nan gradient found. The current loss is:  0.7237815856933594\n",
      "Warning: nan gradient found. The current loss is:  0.4676814079284668\n",
      "Warning: nan gradient found. The current loss is:  1.2069268226623535\n",
      "Warning: nan gradient found. The current loss is:  0.5838304758071899\n",
      "Warning: nan gradient found. The current loss is:  0.6235301494598389\n",
      "Warning: nan gradient found. The current loss is:  0.7979167699813843\n",
      "Warning: nan gradient found. The current loss is:  0.5485676527023315\n",
      "Warning: nan gradient found. The current loss is:  0.9386299848556519\n",
      "Warning: nan gradient found. The current loss is:  0.808493435382843\n",
      "Warning: nan gradient found. The current loss is:  0.6139689683914185\n",
      "Warning: nan gradient found. The current loss is:  0.8326412439346313\n",
      "Warning: nan gradient found. The current loss is:  0.8325632810592651\n",
      "Warning: nan gradient found. The current loss is:  0.8977674841880798\n",
      "Warning: nan gradient found. The current loss is:  0.2662982642650604\n",
      "Warning: nan gradient found. The current loss is:  1.3154672384262085\n",
      "Warning: nan gradient found. The current loss is:  0.5285459756851196\n",
      "Warning: nan gradient found. The current loss is:  0.6928054690361023\n",
      "Warning: nan gradient found. The current loss is:  1.007962703704834\n",
      "Warning: nan gradient found. The current loss is:  0.8261617422103882\n",
      "Warning: nan gradient found. The current loss is:  0.7412675023078918\n",
      "Warning: nan gradient found. The current loss is:  0.2820361852645874\n",
      "Warning: nan gradient found. The current loss is:  0.4139189124107361\n",
      "Warning: nan gradient found. The current loss is:  0.6391077041625977\n",
      "Warning: nan gradient found. The current loss is:  0.8501557111740112\n",
      "Warning: nan gradient found. The current loss is:  1.025899887084961\n",
      "Warning: nan gradient found. The current loss is:  0.18943853676319122\n",
      "Warning: nan gradient found. The current loss is:  0.34961944818496704\n",
      "Warning: nan gradient found. The current loss is:  0.44897839426994324\n",
      "Warning: nan gradient found. The current loss is:  0.447237491607666\n",
      "Warning: nan gradient found. The current loss is:  0.5136607885360718\n",
      "Warning: nan gradient found. The current loss is:  0.7969738245010376\n",
      "Warning: nan gradient found. The current loss is:  0.5769418478012085\n",
      "Warning: nan gradient found. The current loss is:  0.8440604209899902\n",
      "Warning: nan gradient found. The current loss is:  0.350570410490036\n",
      "Warning: nan gradient found. The current loss is:  0.4716466963291168\n",
      "Warning: nan gradient found. The current loss is:  0.34375542402267456\n",
      "Warning: nan gradient found. The current loss is:  0.6905245184898376\n",
      "Warning: nan gradient found. The current loss is:  0.6421887874603271\n",
      "Current batch training loss: 0.642189  [1792000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  1.7104504108428955\n",
      "Warning: nan gradient found. The current loss is:  0.33950716257095337\n",
      "Warning: nan gradient found. The current loss is:  0.38277554512023926\n",
      "Warning: nan gradient found. The current loss is:  0.592361330986023\n",
      "Warning: nan gradient found. The current loss is:  1.4522367715835571\n",
      "Warning: nan gradient found. The current loss is:  0.3745421767234802\n",
      "Warning: nan gradient found. The current loss is:  0.9808231592178345\n",
      "Warning: nan gradient found. The current loss is:  0.7254070043563843\n",
      "Warning: nan gradient found. The current loss is:  0.4460785388946533\n",
      "Warning: nan gradient found. The current loss is:  1.301847219467163\n",
      "Warning: nan gradient found. The current loss is:  1.247285008430481\n",
      "Warning: nan gradient found. The current loss is:  1.832831621170044\n",
      "Warning: nan gradient found. The current loss is:  0.30289557576179504\n",
      "Warning: nan gradient found. The current loss is:  1.0929534435272217\n",
      "Warning: nan gradient found. The current loss is:  0.7662614583969116\n",
      "Warning: nan gradient found. The current loss is:  1.0162121057510376\n",
      "Warning: nan gradient found. The current loss is:  0.7285745739936829\n",
      "Warning: nan gradient found. The current loss is:  1.2067408561706543\n",
      "Warning: nan gradient found. The current loss is:  0.7201516628265381\n",
      "Warning: nan gradient found. The current loss is:  0.7150013446807861\n",
      "Warning: nan gradient found. The current loss is:  0.32849106192588806\n",
      "Warning: nan gradient found. The current loss is:  0.6170791387557983\n",
      "Warning: nan gradient found. The current loss is:  0.6411663889884949\n",
      "Warning: nan gradient found. The current loss is:  0.9363464117050171\n",
      "Warning: nan gradient found. The current loss is:  0.3826976418495178\n",
      "Warning: nan gradient found. The current loss is:  0.09748373925685883\n",
      "Warning: nan gradient found. The current loss is:  0.9158673286437988\n",
      "Warning: nan gradient found. The current loss is:  1.004957914352417\n",
      "Warning: nan gradient found. The current loss is:  1.4445905685424805\n",
      "Warning: nan gradient found. The current loss is:  0.4265294671058655\n",
      "Warning: nan gradient found. The current loss is:  0.35979729890823364\n",
      "Warning: nan gradient found. The current loss is:  0.9751353859901428\n",
      "Warning: nan gradient found. The current loss is:  1.179757833480835\n",
      "Warning: nan gradient found. The current loss is:  0.1425396203994751\n",
      "Warning: nan gradient found. The current loss is:  0.43906331062316895\n",
      "Warning: nan gradient found. The current loss is:  0.8550575375556946\n",
      "Warning: nan gradient found. The current loss is:  0.3522922992706299\n",
      "Warning: nan gradient found. The current loss is:  0.7417130470275879\n",
      "Warning: nan gradient found. The current loss is:  0.47975969314575195\n",
      "Warning: nan gradient found. The current loss is:  0.8982972502708435\n",
      "Warning: nan gradient found. The current loss is:  1.1054978370666504\n",
      "Warning: nan gradient found. The current loss is:  0.5484985113143921\n",
      "Warning: nan gradient found. The current loss is:  0.5554390549659729\n",
      "Warning: nan gradient found. The current loss is:  0.6320409774780273\n",
      "Warning: nan gradient found. The current loss is:  0.7143646478652954\n",
      "Warning: nan gradient found. The current loss is:  0.512031614780426\n",
      "Warning: nan gradient found. The current loss is:  0.3980771601200104\n",
      "Warning: nan gradient found. The current loss is:  1.0794765949249268\n",
      "Warning: nan gradient found. The current loss is:  0.3210645914077759\n",
      "Warning: nan gradient found. The current loss is:  0.553217351436615\n",
      "Warning: nan gradient found. The current loss is:  2.8609654903411865\n",
      "Warning: nan gradient found. The current loss is:  1.289754867553711\n",
      "Warning: nan gradient found. The current loss is:  3.1104936599731445\n",
      "Warning: nan gradient found. The current loss is:  0.5983960032463074\n",
      "Warning: nan gradient found. The current loss is:  0.5513513088226318\n",
      "Warning: nan gradient found. The current loss is:  0.3666760325431824\n",
      "Warning: nan gradient found. The current loss is:  0.33946648240089417\n",
      "Warning: nan gradient found. The current loss is:  0.7743806838989258\n",
      "Warning: nan gradient found. The current loss is:  0.2363678216934204\n",
      "Warning: nan gradient found. The current loss is:  0.2854592204093933\n",
      "Warning: nan gradient found. The current loss is:  0.5784083604812622\n",
      "Warning: nan gradient found. The current loss is:  0.41701459884643555\n",
      "Warning: nan gradient found. The current loss is:  1.038570523262024\n",
      "Warning: nan gradient found. The current loss is:  0.2883821129798889\n",
      "Warning: nan gradient found. The current loss is:  0.8664796948432922\n",
      "Warning: nan gradient found. The current loss is:  0.38932716846466064\n",
      "Warning: nan gradient found. The current loss is:  0.47266578674316406\n",
      "Warning: nan gradient found. The current loss is:  2.038733720779419\n",
      "Warning: nan gradient found. The current loss is:  0.5715811252593994\n",
      "Warning: nan gradient found. The current loss is:  0.920531153678894\n",
      "Warning: nan gradient found. The current loss is:  0.1819208413362503\n",
      "Warning: nan gradient found. The current loss is:  1.1001828908920288\n",
      "Warning: nan gradient found. The current loss is:  1.2104742527008057\n",
      "Warning: nan gradient found. The current loss is:  0.5954188704490662\n",
      "Warning: nan gradient found. The current loss is:  0.22826720774173737\n",
      "Warning: nan gradient found. The current loss is:  2.1405489444732666\n",
      "Warning: nan gradient found. The current loss is:  0.48472368717193604\n",
      "Warning: nan gradient found. The current loss is:  0.3507320284843445\n",
      "Warning: nan gradient found. The current loss is:  0.3867003321647644\n",
      "Warning: nan gradient found. The current loss is:  0.9131208658218384\n",
      "Warning: nan gradient found. The current loss is:  0.48796164989471436\n",
      "Warning: nan gradient found. The current loss is:  0.7299302816390991\n",
      "Warning: nan gradient found. The current loss is:  0.5014804601669312\n",
      "Warning: nan gradient found. The current loss is:  0.6909437775611877\n",
      "Warning: nan gradient found. The current loss is:  0.8279850482940674\n",
      "Warning: nan gradient found. The current loss is:  0.05382297560572624\n",
      "Warning: nan gradient found. The current loss is:  0.44201064109802246\n",
      "Warning: nan gradient found. The current loss is:  0.9204224944114685\n",
      "Warning: nan gradient found. The current loss is:  0.49554917216300964\n",
      "Warning: nan gradient found. The current loss is:  0.8562566041946411\n",
      "Warning: nan gradient found. The current loss is:  0.9300132989883423\n",
      "Warning: nan gradient found. The current loss is:  1.4995031356811523\n",
      "Warning: nan gradient found. The current loss is:  0.5239933133125305\n",
      "Warning: nan gradient found. The current loss is:  0.9663937091827393\n",
      "Warning: nan gradient found. The current loss is:  0.5193926095962524\n",
      "Warning: nan gradient found. The current loss is:  1.5989995002746582\n",
      "Warning: nan gradient found. The current loss is:  1.5160326957702637\n",
      "Warning: nan gradient found. The current loss is:  1.867750883102417\n",
      "Warning: nan gradient found. The current loss is:  0.3190321624279022\n",
      "Warning: nan gradient found. The current loss is:  0.8028177618980408\n",
      "Current batch training loss: 0.802818  [1817600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5524173974990845\n",
      "Warning: nan gradient found. The current loss is:  0.2987406849861145\n",
      "Warning: nan gradient found. The current loss is:  0.7619246244430542\n",
      "Warning: nan gradient found. The current loss is:  0.5518286824226379\n",
      "Warning: nan gradient found. The current loss is:  0.3436289429664612\n",
      "Warning: nan gradient found. The current loss is:  0.42873409390449524\n",
      "Warning: nan gradient found. The current loss is:  0.565328061580658\n",
      "Warning: nan gradient found. The current loss is:  1.4209263324737549\n",
      "Warning: nan gradient found. The current loss is:  0.6737406253814697\n",
      "Warning: nan gradient found. The current loss is:  0.5837993621826172\n",
      "Warning: nan gradient found. The current loss is:  0.5123504400253296\n",
      "Warning: nan gradient found. The current loss is:  0.6323005557060242\n",
      "Warning: nan gradient found. The current loss is:  0.6279111504554749\n",
      "Warning: nan gradient found. The current loss is:  0.24763038754463196\n",
      "Warning: nan gradient found. The current loss is:  1.5146284103393555\n",
      "Warning: nan gradient found. The current loss is:  0.5008698105812073\n",
      "Warning: nan gradient found. The current loss is:  0.5764632225036621\n",
      "Warning: nan gradient found. The current loss is:  0.7481794953346252\n",
      "Warning: nan gradient found. The current loss is:  0.597358763217926\n",
      "Warning: nan gradient found. The current loss is:  0.7681013345718384\n",
      "Warning: nan gradient found. The current loss is:  0.9978798627853394\n",
      "Warning: nan gradient found. The current loss is:  0.7184817790985107\n",
      "Warning: nan gradient found. The current loss is:  0.7212883234024048\n",
      "Warning: nan gradient found. The current loss is:  0.4161791205406189\n",
      "Warning: nan gradient found. The current loss is:  0.4724365472793579\n",
      "Warning: nan gradient found. The current loss is:  0.4935350716114044\n",
      "Warning: nan gradient found. The current loss is:  0.28176605701446533\n",
      "Warning: nan gradient found. The current loss is:  0.22625446319580078\n",
      "Warning: nan gradient found. The current loss is:  0.45092058181762695\n",
      "Warning: nan gradient found. The current loss is:  0.716421365737915\n",
      "Warning: nan gradient found. The current loss is:  0.7241079807281494\n",
      "Warning: nan gradient found. The current loss is:  0.5015318989753723\n",
      "Warning: nan gradient found. The current loss is:  0.47496187686920166\n",
      "Warning: nan gradient found. The current loss is:  0.5042198896408081\n",
      "Warning: nan gradient found. The current loss is:  0.8130350708961487\n",
      "Warning: nan gradient found. The current loss is:  0.7185846567153931\n",
      "Warning: nan gradient found. The current loss is:  0.44544991850852966\n",
      "Warning: nan gradient found. The current loss is:  1.2525932788848877\n",
      "Warning: nan gradient found. The current loss is:  0.39766061305999756\n",
      "Warning: nan gradient found. The current loss is:  1.4920371770858765\n",
      "Warning: nan gradient found. The current loss is:  0.45191341638565063\n",
      "Warning: nan gradient found. The current loss is:  0.3122527599334717\n",
      "Warning: nan gradient found. The current loss is:  1.1487836837768555\n",
      "Warning: nan gradient found. The current loss is:  0.6867270469665527\n",
      "Warning: nan gradient found. The current loss is:  0.41845089197158813\n",
      "Warning: nan gradient found. The current loss is:  0.5264441967010498\n",
      "Warning: nan gradient found. The current loss is:  0.4376868009567261\n",
      "Warning: nan gradient found. The current loss is:  0.32085782289505005\n",
      "Warning: nan gradient found. The current loss is:  1.3777873516082764\n",
      "Warning: nan gradient found. The current loss is:  0.8565481901168823\n",
      "Warning: nan gradient found. The current loss is:  0.4566302001476288\n",
      "Warning: nan gradient found. The current loss is:  0.49413615465164185\n",
      "Warning: nan gradient found. The current loss is:  0.6819138526916504\n",
      "Warning: nan gradient found. The current loss is:  0.9600727558135986\n",
      "Warning: nan gradient found. The current loss is:  0.6247905492782593\n",
      "Warning: nan gradient found. The current loss is:  0.602410614490509\n",
      "Warning: nan gradient found. The current loss is:  0.336739182472229\n",
      "Warning: nan gradient found. The current loss is:  0.8674424290657043\n",
      "Warning: nan gradient found. The current loss is:  0.7260795831680298\n",
      "Warning: nan gradient found. The current loss is:  1.213336706161499\n",
      "Warning: nan gradient found. The current loss is:  0.6236637830734253\n",
      "Warning: nan gradient found. The current loss is:  1.1085587739944458\n",
      "Warning: nan gradient found. The current loss is:  0.9245288372039795\n",
      "Warning: nan gradient found. The current loss is:  0.5049867630004883\n",
      "Warning: nan gradient found. The current loss is:  0.2671639323234558\n",
      "Warning: nan gradient found. The current loss is:  1.2683428525924683\n",
      "Warning: nan gradient found. The current loss is:  0.547473669052124\n",
      "Warning: nan gradient found. The current loss is:  0.951542317867279\n",
      "Warning: nan gradient found. The current loss is:  0.09025384485721588\n",
      "Warning: nan gradient found. The current loss is:  0.931215763092041\n",
      "Warning: nan gradient found. The current loss is:  0.948844850063324\n",
      "Warning: nan gradient found. The current loss is:  0.26644206047058105\n",
      "Warning: nan gradient found. The current loss is:  0.42598479986190796\n",
      "Warning: nan gradient found. The current loss is:  1.5146842002868652\n",
      "Warning: nan gradient found. The current loss is:  0.45176541805267334\n",
      "Warning: nan gradient found. The current loss is:  0.5690295696258545\n",
      "Warning: nan gradient found. The current loss is:  0.4947085976600647\n",
      "Warning: nan gradient found. The current loss is:  0.7668948173522949\n",
      "Warning: nan gradient found. The current loss is:  0.5608612895011902\n",
      "Warning: nan gradient found. The current loss is:  1.3949693441390991\n",
      "Warning: nan gradient found. The current loss is:  0.8987849950790405\n",
      "Warning: nan gradient found. The current loss is:  0.5891276597976685\n",
      "Warning: nan gradient found. The current loss is:  1.5488263368606567\n",
      "Warning: nan gradient found. The current loss is:  0.7033637166023254\n",
      "Warning: nan gradient found. The current loss is:  0.9433037042617798\n",
      "Warning: nan gradient found. The current loss is:  0.4185698926448822\n",
      "Warning: nan gradient found. The current loss is:  0.22229696810245514\n",
      "Warning: nan gradient found. The current loss is:  0.5321081876754761\n",
      "Warning: nan gradient found. The current loss is:  1.275892734527588\n",
      "Warning: nan gradient found. The current loss is:  0.6011542081832886\n",
      "Warning: nan gradient found. The current loss is:  0.885341227054596\n",
      "Warning: nan gradient found. The current loss is:  0.22054269909858704\n",
      "Warning: nan gradient found. The current loss is:  0.8393288850784302\n",
      "Warning: nan gradient found. The current loss is:  0.5081015825271606\n",
      "Warning: nan gradient found. The current loss is:  1.5483602285385132\n",
      "Warning: nan gradient found. The current loss is:  1.0136507749557495\n",
      "Warning: nan gradient found. The current loss is:  0.3704831004142761\n",
      "Warning: nan gradient found. The current loss is:  0.4351488947868347\n",
      "Warning: nan gradient found. The current loss is:  0.32600855827331543\n",
      "Warning: nan gradient found. The current loss is:  0.9621007442474365\n",
      "Current batch training loss: 0.962101  [1843200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.42776378989219666\n",
      "Warning: nan gradient found. The current loss is:  0.8775672912597656\n",
      "Warning: nan gradient found. The current loss is:  0.2827948033809662\n",
      "Warning: nan gradient found. The current loss is:  0.7372192144393921\n",
      "Warning: nan gradient found. The current loss is:  0.9477512240409851\n",
      "Warning: nan gradient found. The current loss is:  0.2999773621559143\n",
      "Warning: nan gradient found. The current loss is:  0.4840293228626251\n",
      "Warning: nan gradient found. The current loss is:  1.4081332683563232\n",
      "Warning: nan gradient found. The current loss is:  1.0208897590637207\n",
      "Warning: nan gradient found. The current loss is:  1.3051800727844238\n",
      "Warning: nan gradient found. The current loss is:  0.5392357707023621\n",
      "Warning: nan gradient found. The current loss is:  0.1876639574766159\n",
      "Warning: nan gradient found. The current loss is:  0.31498101353645325\n",
      "Warning: nan gradient found. The current loss is:  1.1108098030090332\n",
      "Warning: nan gradient found. The current loss is:  1.3113758563995361\n",
      "Warning: nan gradient found. The current loss is:  1.3763868808746338\n",
      "Warning: nan gradient found. The current loss is:  0.7587270736694336\n",
      "Warning: nan gradient found. The current loss is:  0.8513540029525757\n",
      "Warning: nan gradient found. The current loss is:  0.23321470618247986\n",
      "Warning: nan gradient found. The current loss is:  0.2169722467660904\n",
      "Warning: nan gradient found. The current loss is:  0.4848921597003937\n",
      "Warning: nan gradient found. The current loss is:  0.5795828104019165\n",
      "Warning: nan gradient found. The current loss is:  0.09450650960206985\n",
      "Warning: nan gradient found. The current loss is:  1.868321418762207\n",
      "Warning: nan gradient found. The current loss is:  1.005190372467041\n",
      "Warning: nan gradient found. The current loss is:  0.2644021511077881\n",
      "Warning: nan gradient found. The current loss is:  0.298134982585907\n",
      "Warning: nan gradient found. The current loss is:  0.596320390701294\n",
      "Warning: nan gradient found. The current loss is:  0.06813766807317734\n",
      "Warning: nan gradient found. The current loss is:  1.1536967754364014\n",
      "Warning: nan gradient found. The current loss is:  0.8683578372001648\n",
      "Warning: nan gradient found. The current loss is:  0.7262221574783325\n",
      "Warning: nan gradient found. The current loss is:  0.2403840720653534\n",
      "Warning: nan gradient found. The current loss is:  0.27972209453582764\n",
      "Warning: nan gradient found. The current loss is:  0.4970570504665375\n",
      "Warning: nan gradient found. The current loss is:  0.2669815421104431\n",
      "Warning: nan gradient found. The current loss is:  0.2938033938407898\n",
      "Warning: nan gradient found. The current loss is:  0.793814480304718\n",
      "Warning: nan gradient found. The current loss is:  0.9868195652961731\n",
      "Warning: nan gradient found. The current loss is:  0.5896034240722656\n",
      "Warning: nan gradient found. The current loss is:  0.5610206127166748\n",
      "Warning: nan gradient found. The current loss is:  0.9157580137252808\n",
      "Warning: nan gradient found. The current loss is:  0.5779217481613159\n",
      "Warning: nan gradient found. The current loss is:  0.3651222884654999\n",
      "Warning: nan gradient found. The current loss is:  0.5932469367980957\n",
      "Warning: nan gradient found. The current loss is:  1.0805939435958862\n",
      "Warning: nan gradient found. The current loss is:  0.7123794555664062\n",
      "Warning: nan gradient found. The current loss is:  0.23839277029037476\n",
      "Warning: nan gradient found. The current loss is:  1.0934007167816162\n",
      "Warning: nan gradient found. The current loss is:  0.09450750052928925\n",
      "Warning: nan gradient found. The current loss is:  0.30145174264907837\n",
      "Warning: nan gradient found. The current loss is:  1.229017734527588\n",
      "Warning: nan gradient found. The current loss is:  1.0059376955032349\n",
      "Warning: nan gradient found. The current loss is:  0.15855588018894196\n",
      "Warning: nan gradient found. The current loss is:  0.10366540402173996\n",
      "Warning: nan gradient found. The current loss is:  1.1255946159362793\n",
      "Warning: nan gradient found. The current loss is:  0.3738992214202881\n",
      "Warning: nan gradient found. The current loss is:  0.6475799083709717\n",
      "Warning: nan gradient found. The current loss is:  0.5089199542999268\n",
      "Warning: nan gradient found. The current loss is:  0.9740380644798279\n",
      "Warning: nan gradient found. The current loss is:  0.9676041007041931\n",
      "Warning: nan gradient found. The current loss is:  0.6556611657142639\n",
      "Warning: nan gradient found. The current loss is:  1.102780818939209\n",
      "Warning: nan gradient found. The current loss is:  0.5157870054244995\n",
      "Warning: nan gradient found. The current loss is:  0.560804545879364\n",
      "Warning: nan gradient found. The current loss is:  0.9516156911849976\n",
      "Warning: nan gradient found. The current loss is:  0.44583213329315186\n",
      "Warning: nan gradient found. The current loss is:  0.594269871711731\n",
      "Warning: nan gradient found. The current loss is:  0.3491814434528351\n",
      "Warning: nan gradient found. The current loss is:  1.7455543279647827\n",
      "Warning: nan gradient found. The current loss is:  1.8739900588989258\n",
      "Warning: nan gradient found. The current loss is:  0.14004278182983398\n",
      "Warning: nan gradient found. The current loss is:  0.404304563999176\n",
      "Warning: nan gradient found. The current loss is:  0.30419835448265076\n",
      "Warning: nan gradient found. The current loss is:  0.8619458675384521\n",
      "Warning: nan gradient found. The current loss is:  0.24331101775169373\n",
      "Warning: nan gradient found. The current loss is:  0.5269953608512878\n",
      "Warning: nan gradient found. The current loss is:  0.31722885370254517\n",
      "Warning: nan gradient found. The current loss is:  0.5639394521713257\n",
      "Warning: nan gradient found. The current loss is:  0.43672987818717957\n",
      "Warning: nan gradient found. The current loss is:  0.31597453355789185\n",
      "Warning: nan gradient found. The current loss is:  0.5686457753181458\n",
      "Warning: nan gradient found. The current loss is:  0.15912947058677673\n",
      "Warning: nan gradient found. The current loss is:  0.9877674579620361\n",
      "Warning: nan gradient found. The current loss is:  0.8128682971000671\n",
      "Warning: nan gradient found. The current loss is:  1.3288006782531738\n",
      "Warning: nan gradient found. The current loss is:  0.29901835322380066\n",
      "Warning: nan gradient found. The current loss is:  1.202441930770874\n",
      "Warning: nan gradient found. The current loss is:  1.0141092538833618\n",
      "Warning: nan gradient found. The current loss is:  0.39487379789352417\n",
      "Warning: nan gradient found. The current loss is:  0.4973794221878052\n",
      "Warning: nan gradient found. The current loss is:  0.5969067215919495\n",
      "Warning: nan gradient found. The current loss is:  0.9671475291252136\n",
      "Warning: nan gradient found. The current loss is:  0.12775026261806488\n",
      "Warning: nan gradient found. The current loss is:  1.123166561126709\n",
      "Warning: nan gradient found. The current loss is:  0.7306212186813354\n",
      "Warning: nan gradient found. The current loss is:  0.16479438543319702\n",
      "Warning: nan gradient found. The current loss is:  0.4160853624343872\n",
      "Warning: nan gradient found. The current loss is:  0.8375523090362549\n",
      "Warning: nan gradient found. The current loss is:  0.008067645132541656\n",
      "Current batch training loss: 0.008068  [1868800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  2.8905630111694336\n",
      "Warning: nan gradient found. The current loss is:  1.2910776138305664\n",
      "Warning: nan gradient found. The current loss is:  0.3645481765270233\n",
      "Warning: nan gradient found. The current loss is:  1.6255557537078857\n",
      "Warning: nan gradient found. The current loss is:  1.1026532649993896\n",
      "Warning: nan gradient found. The current loss is:  0.7168377637863159\n",
      "Warning: nan gradient found. The current loss is:  1.1888017654418945\n",
      "Warning: nan gradient found. The current loss is:  0.06619162857532501\n",
      "Warning: nan gradient found. The current loss is:  1.2264187335968018\n",
      "Warning: nan gradient found. The current loss is:  0.2366974651813507\n",
      "Warning: nan gradient found. The current loss is:  0.46355366706848145\n",
      "Warning: nan gradient found. The current loss is:  1.3854520320892334\n",
      "Warning: nan gradient found. The current loss is:  0.4941753149032593\n",
      "Warning: nan gradient found. The current loss is:  1.1834344863891602\n",
      "Warning: nan gradient found. The current loss is:  1.4947638511657715\n",
      "Warning: nan gradient found. The current loss is:  0.38301631808280945\n",
      "Warning: nan gradient found. The current loss is:  0.31413745880126953\n",
      "Warning: nan gradient found. The current loss is:  0.7966668605804443\n",
      "Warning: nan gradient found. The current loss is:  1.1297553777694702\n",
      "Warning: nan gradient found. The current loss is:  0.9413944482803345\n",
      "Warning: nan gradient found. The current loss is:  0.3242766261100769\n",
      "Warning: nan gradient found. The current loss is:  0.13916194438934326\n",
      "Warning: nan gradient found. The current loss is:  0.8140689134597778\n",
      "Warning: nan gradient found. The current loss is:  1.1362428665161133\n",
      "Warning: nan gradient found. The current loss is:  1.6001940965652466\n",
      "Warning: nan gradient found. The current loss is:  0.5864546895027161\n",
      "Warning: nan gradient found. The current loss is:  0.7670035362243652\n",
      "Warning: nan gradient found. The current loss is:  0.2731098532676697\n",
      "Warning: nan gradient found. The current loss is:  0.495147168636322\n",
      "Warning: nan gradient found. The current loss is:  0.2747901678085327\n",
      "Warning: nan gradient found. The current loss is:  0.38985738158226013\n",
      "Warning: nan gradient found. The current loss is:  0.6949317455291748\n",
      "Warning: nan gradient found. The current loss is:  0.6845166087150574\n",
      "Warning: nan gradient found. The current loss is:  0.3981212377548218\n",
      "Warning: nan gradient found. The current loss is:  0.583366870880127\n",
      "Warning: nan gradient found. The current loss is:  0.7747212052345276\n",
      "Warning: nan gradient found. The current loss is:  0.6969107389450073\n",
      "Warning: nan gradient found. The current loss is:  0.9522463083267212\n",
      "Warning: nan gradient found. The current loss is:  1.0530200004577637\n",
      "Warning: nan gradient found. The current loss is:  0.24686211347579956\n",
      "Warning: nan gradient found. The current loss is:  1.474311113357544\n",
      "Warning: nan gradient found. The current loss is:  2.3739023208618164\n",
      "Warning: nan gradient found. The current loss is:  0.3985718786716461\n",
      "Warning: nan gradient found. The current loss is:  0.5818050503730774\n",
      "Warning: nan gradient found. The current loss is:  0.5930152535438538\n",
      "Warning: nan gradient found. The current loss is:  0.6620328426361084\n",
      "Warning: nan gradient found. The current loss is:  0.8669881820678711\n",
      "Warning: nan gradient found. The current loss is:  2.1676223278045654\n",
      "Warning: nan gradient found. The current loss is:  0.302161306142807\n",
      "Warning: nan gradient found. The current loss is:  0.6005022525787354\n",
      "Warning: nan gradient found. The current loss is:  1.0509451627731323\n",
      "Warning: nan gradient found. The current loss is:  1.1809402704238892\n",
      "Warning: nan gradient found. The current loss is:  0.9074480533599854\n",
      "Warning: nan gradient found. The current loss is:  1.47090482711792\n",
      "Warning: nan gradient found. The current loss is:  0.7739434838294983\n",
      "Warning: nan gradient found. The current loss is:  1.6278858184814453\n",
      "Warning: nan gradient found. The current loss is:  0.2517235279083252\n",
      "Warning: nan gradient found. The current loss is:  0.3888038396835327\n",
      "Warning: nan gradient found. The current loss is:  0.5291686058044434\n",
      "Warning: nan gradient found. The current loss is:  0.27574336528778076\n",
      "Warning: nan gradient found. The current loss is:  0.6931795477867126\n",
      "Warning: nan gradient found. The current loss is:  0.6428818702697754\n",
      "Warning: nan gradient found. The current loss is:  0.4558600187301636\n",
      "Warning: nan gradient found. The current loss is:  0.2920607924461365\n",
      "Warning: nan gradient found. The current loss is:  0.2787124514579773\n",
      "Warning: nan gradient found. The current loss is:  0.7726728916168213\n",
      "Warning: nan gradient found. The current loss is:  0.8523108959197998\n",
      "Warning: nan gradient found. The current loss is:  0.9813576340675354\n",
      "Warning: nan gradient found. The current loss is:  1.2552859783172607\n",
      "Warning: nan gradient found. The current loss is:  0.9797297120094299\n",
      "Warning: nan gradient found. The current loss is:  0.5668418407440186\n",
      "Warning: nan gradient found. The current loss is:  0.39095205068588257\n",
      "Warning: nan gradient found. The current loss is:  1.6206116676330566\n",
      "Warning: nan gradient found. The current loss is:  1.0353920459747314\n",
      "Warning: nan gradient found. The current loss is:  1.0343849658966064\n",
      "Warning: nan gradient found. The current loss is:  0.17505571246147156\n",
      "Warning: nan gradient found. The current loss is:  0.33989718556404114\n",
      "Warning: nan gradient found. The current loss is:  0.4387325644493103\n",
      "Warning: nan gradient found. The current loss is:  0.16689848899841309\n",
      "Warning: nan gradient found. The current loss is:  0.611224889755249\n",
      "Warning: nan gradient found. The current loss is:  0.4282933473587036\n",
      "Warning: nan gradient found. The current loss is:  0.7219725847244263\n",
      "Warning: nan gradient found. The current loss is:  0.21737194061279297\n",
      "Warning: nan gradient found. The current loss is:  0.3166680634021759\n",
      "Warning: nan gradient found. The current loss is:  1.062502145767212\n",
      "Warning: nan gradient found. The current loss is:  0.1367415189743042\n",
      "Warning: nan gradient found. The current loss is:  0.8981615304946899\n",
      "Warning: nan gradient found. The current loss is:  1.6375583410263062\n",
      "Warning: nan gradient found. The current loss is:  0.9696958661079407\n",
      "Warning: nan gradient found. The current loss is:  0.544517457485199\n",
      "Warning: nan gradient found. The current loss is:  0.3289636969566345\n",
      "Warning: nan gradient found. The current loss is:  0.9557487964630127\n",
      "Warning: nan gradient found. The current loss is:  0.3187086880207062\n",
      "Warning: nan gradient found. The current loss is:  1.8605810403823853\n",
      "Warning: nan gradient found. The current loss is:  1.9659377336502075\n",
      "Warning: nan gradient found. The current loss is:  0.2726936340332031\n",
      "Warning: nan gradient found. The current loss is:  0.44020336866378784\n",
      "Warning: nan gradient found. The current loss is:  1.0132473707199097\n",
      "Warning: nan gradient found. The current loss is:  0.8708312511444092\n",
      "Warning: nan gradient found. The current loss is:  0.7713230848312378\n",
      "Current batch training loss: 0.771323  [1894400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.38709449768066406\n",
      "Warning: nan gradient found. The current loss is:  0.9858970046043396\n",
      "Warning: nan gradient found. The current loss is:  0.5018090605735779\n",
      "Warning: nan gradient found. The current loss is:  1.7970292568206787\n",
      "Warning: nan gradient found. The current loss is:  1.103458046913147\n",
      "Warning: nan gradient found. The current loss is:  0.3495020270347595\n",
      "Warning: nan gradient found. The current loss is:  1.0982764959335327\n",
      "Warning: nan gradient found. The current loss is:  0.9826481342315674\n",
      "Warning: nan gradient found. The current loss is:  0.16502520442008972\n",
      "Warning: nan gradient found. The current loss is:  0.7634227871894836\n",
      "Warning: nan gradient found. The current loss is:  0.8720921277999878\n",
      "Warning: nan gradient found. The current loss is:  0.3538396954536438\n",
      "Warning: nan gradient found. The current loss is:  0.903973400592804\n",
      "Warning: nan gradient found. The current loss is:  0.3517230153083801\n",
      "Warning: nan gradient found. The current loss is:  1.255368947982788\n",
      "Warning: nan gradient found. The current loss is:  0.48652786016464233\n",
      "Warning: nan gradient found. The current loss is:  0.6675159931182861\n",
      "Warning: nan gradient found. The current loss is:  0.3376539945602417\n",
      "Warning: nan gradient found. The current loss is:  1.0000373125076294\n",
      "Warning: nan gradient found. The current loss is:  0.741474986076355\n",
      "Warning: nan gradient found. The current loss is:  0.386870801448822\n",
      "Warning: nan gradient found. The current loss is:  1.0198248624801636\n",
      "Warning: nan gradient found. The current loss is:  0.40039101243019104\n",
      "Warning: nan gradient found. The current loss is:  0.1729109287261963\n",
      "Warning: nan gradient found. The current loss is:  0.51027911901474\n",
      "Warning: nan gradient found. The current loss is:  0.4952279329299927\n",
      "Warning: nan gradient found. The current loss is:  1.1680371761322021\n",
      "Warning: nan gradient found. The current loss is:  0.3623810410499573\n",
      "Warning: nan gradient found. The current loss is:  0.8432006239891052\n",
      "Warning: nan gradient found. The current loss is:  1.1306625604629517\n",
      "Warning: nan gradient found. The current loss is:  0.7384887933731079\n",
      "Warning: nan gradient found. The current loss is:  5.840085506439209\n",
      "Warning: nan gradient found. The current loss is:  0.3787200152873993\n",
      "Warning: nan gradient found. The current loss is:  0.5195789337158203\n",
      "Warning: nan gradient found. The current loss is:  0.5630881190299988\n",
      "Warning: nan gradient found. The current loss is:  0.23676654696464539\n",
      "Warning: nan gradient found. The current loss is:  0.5321973562240601\n",
      "Warning: nan gradient found. The current loss is:  0.9200922250747681\n",
      "Warning: nan gradient found. The current loss is:  0.7257057428359985\n",
      "Warning: nan gradient found. The current loss is:  0.4255937337875366\n",
      "Warning: nan gradient found. The current loss is:  0.3774787485599518\n",
      "Warning: nan gradient found. The current loss is:  0.4164961576461792\n",
      "Warning: nan gradient found. The current loss is:  1.3946000337600708\n",
      "Warning: nan gradient found. The current loss is:  0.5056560635566711\n",
      "Warning: nan gradient found. The current loss is:  1.1560155153274536\n",
      "Warning: nan gradient found. The current loss is:  0.1875183880329132\n",
      "Warning: nan gradient found. The current loss is:  1.4922094345092773\n",
      "Warning: nan gradient found. The current loss is:  0.4734075665473938\n",
      "Warning: nan gradient found. The current loss is:  0.9624168276786804\n",
      "Warning: nan gradient found. The current loss is:  0.504940927028656\n",
      "Warning: nan gradient found. The current loss is:  0.34861406683921814\n",
      "Warning: nan gradient found. The current loss is:  0.31067728996276855\n",
      "Warning: nan gradient found. The current loss is:  0.19176995754241943\n",
      "Warning: nan gradient found. The current loss is:  0.4826657176017761\n",
      "Warning: nan gradient found. The current loss is:  0.8883037567138672\n",
      "Warning: nan gradient found. The current loss is:  0.5010138750076294\n",
      "Warning: nan gradient found. The current loss is:  0.9327905178070068\n",
      "Warning: nan gradient found. The current loss is:  0.5560014843940735\n",
      "Warning: nan gradient found. The current loss is:  0.29210764169692993\n",
      "Warning: nan gradient found. The current loss is:  0.8964909315109253\n",
      "Warning: nan gradient found. The current loss is:  0.5008825063705444\n",
      "Warning: nan gradient found. The current loss is:  0.5686229467391968\n",
      "Warning: nan gradient found. The current loss is:  1.3022856712341309\n",
      "Warning: nan gradient found. The current loss is:  0.5857704877853394\n",
      "Warning: nan gradient found. The current loss is:  0.39666780829429626\n",
      "Warning: nan gradient found. The current loss is:  0.3815072178840637\n",
      "Warning: nan gradient found. The current loss is:  0.981643557548523\n",
      "Warning: nan gradient found. The current loss is:  1.1797370910644531\n",
      "Warning: nan gradient found. The current loss is:  0.9961307644844055\n",
      "Warning: nan gradient found. The current loss is:  1.0875980854034424\n",
      "Warning: nan gradient found. The current loss is:  0.4795887768268585\n",
      "Warning: nan gradient found. The current loss is:  0.34602048993110657\n",
      "Warning: nan gradient found. The current loss is:  0.17348459362983704\n",
      "Warning: nan gradient found. The current loss is:  0.2561858296394348\n",
      "Warning: nan gradient found. The current loss is:  0.673433780670166\n",
      "Warning: nan gradient found. The current loss is:  0.5725178718566895\n",
      "Warning: nan gradient found. The current loss is:  0.6081515550613403\n",
      "Warning: nan gradient found. The current loss is:  1.9160749912261963\n",
      "Warning: nan gradient found. The current loss is:  0.940975546836853\n",
      "Warning: nan gradient found. The current loss is:  0.6636948585510254\n",
      "Warning: nan gradient found. The current loss is:  1.0005782842636108\n",
      "Warning: nan gradient found. The current loss is:  0.39401480555534363\n",
      "Warning: nan gradient found. The current loss is:  1.0618484020233154\n",
      "Warning: nan gradient found. The current loss is:  0.6160057187080383\n",
      "Warning: nan gradient found. The current loss is:  0.7380627393722534\n",
      "Warning: nan gradient found. The current loss is:  0.8225215077400208\n",
      "Warning: nan gradient found. The current loss is:  2.723405122756958\n",
      "Warning: nan gradient found. The current loss is:  1.148132562637329\n",
      "Warning: nan gradient found. The current loss is:  0.4802791178226471\n",
      "Warning: nan gradient found. The current loss is:  0.2271290123462677\n",
      "Warning: nan gradient found. The current loss is:  0.5787269473075867\n",
      "Warning: nan gradient found. The current loss is:  0.745581328868866\n",
      "Warning: nan gradient found. The current loss is:  0.5815435647964478\n",
      "Warning: nan gradient found. The current loss is:  0.7390644550323486\n",
      "Warning: nan gradient found. The current loss is:  0.9443360567092896\n",
      "Warning: nan gradient found. The current loss is:  0.5837751626968384\n",
      "Warning: nan gradient found. The current loss is:  0.5984708666801453\n",
      "Warning: nan gradient found. The current loss is:  0.21029362082481384\n",
      "Warning: nan gradient found. The current loss is:  1.8184568881988525\n",
      "Warning: nan gradient found. The current loss is:  1.364983320236206\n",
      "Current batch training loss: 1.364983  [1920000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.522929847240448\n",
      "Warning: nan gradient found. The current loss is:  0.8339152336120605\n",
      "Warning: nan gradient found. The current loss is:  0.87418532371521\n",
      "Warning: nan gradient found. The current loss is:  0.6137025356292725\n",
      "Warning: nan gradient found. The current loss is:  0.5403112173080444\n",
      "Warning: nan gradient found. The current loss is:  0.6667896509170532\n",
      "Warning: nan gradient found. The current loss is:  1.0211796760559082\n",
      "Warning: nan gradient found. The current loss is:  -0.000356525182723999\n",
      "Warning: nan gradient found. The current loss is:  0.5247148275375366\n",
      "Warning: nan gradient found. The current loss is:  1.4951083660125732\n",
      "Warning: nan gradient found. The current loss is:  0.6510992050170898\n",
      "Warning: nan gradient found. The current loss is:  0.4155198633670807\n",
      "Warning: nan gradient found. The current loss is:  0.7837492823600769\n",
      "Warning: nan gradient found. The current loss is:  0.8249361515045166\n",
      "Warning: nan gradient found. The current loss is:  0.273578405380249\n",
      "Warning: nan gradient found. The current loss is:  0.18519054353237152\n",
      "Warning: nan gradient found. The current loss is:  3.3334827423095703\n",
      "Warning: nan gradient found. The current loss is:  0.6917359828948975\n",
      "Warning: nan gradient found. The current loss is:  0.8060009479522705\n",
      "Warning: nan gradient found. The current loss is:  1.2044677734375\n",
      "Warning: nan gradient found. The current loss is:  0.41294416785240173\n",
      "Warning: nan gradient found. The current loss is:  1.3950432538986206\n",
      "Warning: nan gradient found. The current loss is:  1.670798420906067\n",
      "Warning: nan gradient found. The current loss is:  0.39612841606140137\n",
      "Warning: nan gradient found. The current loss is:  0.5426293015480042\n",
      "Warning: nan gradient found. The current loss is:  0.9624955058097839\n",
      "Warning: nan gradient found. The current loss is:  0.5804732441902161\n",
      "Warning: nan gradient found. The current loss is:  2.06669020652771\n",
      "Warning: nan gradient found. The current loss is:  0.8149867653846741\n",
      "Warning: nan gradient found. The current loss is:  0.5147662162780762\n",
      "Warning: nan gradient found. The current loss is:  0.5157431364059448\n",
      "Warning: nan gradient found. The current loss is:  0.6613110303878784\n",
      "Warning: nan gradient found. The current loss is:  0.5349273681640625\n",
      "Warning: nan gradient found. The current loss is:  0.7978103160858154\n",
      "Warning: nan gradient found. The current loss is:  0.6462018489837646\n",
      "Warning: nan gradient found. The current loss is:  0.9657952785491943\n",
      "Warning: nan gradient found. The current loss is:  0.8995212316513062\n",
      "Warning: nan gradient found. The current loss is:  1.004350185394287\n",
      "Warning: nan gradient found. The current loss is:  0.8672040700912476\n",
      "Warning: nan gradient found. The current loss is:  0.6630040407180786\n",
      "Warning: nan gradient found. The current loss is:  0.8352259397506714\n",
      "Warning: nan gradient found. The current loss is:  0.5546090602874756\n",
      "Warning: nan gradient found. The current loss is:  0.6985796093940735\n",
      "Warning: nan gradient found. The current loss is:  0.9331008195877075\n",
      "Warning: nan gradient found. The current loss is:  0.6081273555755615\n",
      "Warning: nan gradient found. The current loss is:  0.36587849259376526\n",
      "Warning: nan gradient found. The current loss is:  0.23874080181121826\n",
      "Warning: nan gradient found. The current loss is:  0.8657771348953247\n",
      "Warning: nan gradient found. The current loss is:  0.37103307247161865\n",
      "Warning: nan gradient found. The current loss is:  0.5082354545593262\n",
      "Warning: nan gradient found. The current loss is:  0.5846352577209473\n",
      "Warning: nan gradient found. The current loss is:  0.8623483180999756\n",
      "Warning: nan gradient found. The current loss is:  1.1542998552322388\n",
      "Warning: nan gradient found. The current loss is:  0.346983402967453\n",
      "Warning: nan gradient found. The current loss is:  0.3718222379684448\n",
      "Warning: nan gradient found. The current loss is:  0.40036240220069885\n",
      "Warning: nan gradient found. The current loss is:  0.12180003523826599\n",
      "Warning: nan gradient found. The current loss is:  0.18265771865844727\n",
      "Warning: nan gradient found. The current loss is:  0.9428819417953491\n",
      "Warning: nan gradient found. The current loss is:  1.2530015707015991\n",
      "Warning: nan gradient found. The current loss is:  0.7656125426292419\n",
      "Warning: nan gradient found. The current loss is:  0.479592502117157\n",
      "Warning: nan gradient found. The current loss is:  0.6058052182197571\n",
      "Warning: nan gradient found. The current loss is:  0.5457018613815308\n",
      "Warning: nan gradient found. The current loss is:  0.6526705026626587\n",
      "Warning: nan gradient found. The current loss is:  1.3120126724243164\n",
      "Warning: nan gradient found. The current loss is:  0.15195974707603455\n",
      "Warning: nan gradient found. The current loss is:  0.9576134085655212\n",
      "Warning: nan gradient found. The current loss is:  0.622179388999939\n",
      "Warning: nan gradient found. The current loss is:  0.03396845608949661\n",
      "Warning: nan gradient found. The current loss is:  0.3182488679885864\n",
      "Warning: nan gradient found. The current loss is:  0.6756008863449097\n",
      "Warning: nan gradient found. The current loss is:  0.47529202699661255\n",
      "Warning: nan gradient found. The current loss is:  1.3511362075805664\n",
      "Warning: nan gradient found. The current loss is:  0.37940219044685364\n",
      "Warning: nan gradient found. The current loss is:  0.4836791753768921\n",
      "Warning: nan gradient found. The current loss is:  0.5413215160369873\n",
      "Warning: nan gradient found. The current loss is:  0.521432638168335\n",
      "Warning: nan gradient found. The current loss is:  0.21313832700252533\n",
      "Warning: nan gradient found. The current loss is:  0.17304717004299164\n",
      "Warning: nan gradient found. The current loss is:  0.38197648525238037\n",
      "Warning: nan gradient found. The current loss is:  0.45875948667526245\n",
      "Warning: nan gradient found. The current loss is:  0.8992477655410767\n",
      "Warning: nan gradient found. The current loss is:  0.3586147427558899\n",
      "Warning: nan gradient found. The current loss is:  0.3222728371620178\n",
      "Warning: nan gradient found. The current loss is:  0.7867190837860107\n",
      "Warning: nan gradient found. The current loss is:  0.6978724002838135\n",
      "Warning: nan gradient found. The current loss is:  0.442211389541626\n",
      "Warning: nan gradient found. The current loss is:  0.39259737730026245\n",
      "Warning: nan gradient found. The current loss is:  1.4727596044540405\n",
      "Warning: nan gradient found. The current loss is:  1.1047096252441406\n",
      "Warning: nan gradient found. The current loss is:  0.6844366788864136\n",
      "Warning: nan gradient found. The current loss is:  0.5516524314880371\n",
      "Warning: nan gradient found. The current loss is:  1.2252811193466187\n",
      "Warning: nan gradient found. The current loss is:  0.3666805028915405\n",
      "Warning: nan gradient found. The current loss is:  0.9562557339668274\n",
      "Warning: nan gradient found. The current loss is:  0.6311956644058228\n",
      "Warning: nan gradient found. The current loss is:  0.7460305690765381\n",
      "Warning: nan gradient found. The current loss is:  0.3830113410949707\n",
      "Warning: nan gradient found. The current loss is:  0.18085630238056183\n",
      "Current batch training loss: 0.180856  [1945600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8679618239402771\n",
      "Warning: nan gradient found. The current loss is:  0.6107093095779419\n",
      "Warning: nan gradient found. The current loss is:  0.5398771166801453\n",
      "Warning: nan gradient found. The current loss is:  0.3249484598636627\n",
      "Warning: nan gradient found. The current loss is:  0.14481356739997864\n",
      "Warning: nan gradient found. The current loss is:  0.42708903551101685\n",
      "Warning: nan gradient found. The current loss is:  1.4534893035888672\n",
      "Warning: nan gradient found. The current loss is:  0.7954734563827515\n",
      "Warning: nan gradient found. The current loss is:  1.1881146430969238\n",
      "Warning: nan gradient found. The current loss is:  1.3857197761535645\n",
      "Warning: nan gradient found. The current loss is:  0.49551713466644287\n",
      "Warning: nan gradient found. The current loss is:  0.49440428614616394\n",
      "Warning: nan gradient found. The current loss is:  0.9310711622238159\n",
      "Warning: nan gradient found. The current loss is:  0.40587130188941956\n",
      "Warning: nan gradient found. The current loss is:  0.46339893341064453\n",
      "Warning: nan gradient found. The current loss is:  0.36855989694595337\n",
      "Warning: nan gradient found. The current loss is:  0.4064934551715851\n",
      "Warning: nan gradient found. The current loss is:  0.7886706590652466\n",
      "Warning: nan gradient found. The current loss is:  0.44104811549186707\n",
      "Warning: nan gradient found. The current loss is:  1.3203916549682617\n",
      "Warning: nan gradient found. The current loss is:  1.1200953722000122\n",
      "Warning: nan gradient found. The current loss is:  0.6579021215438843\n",
      "Warning: nan gradient found. The current loss is:  0.7943942546844482\n",
      "Warning: nan gradient found. The current loss is:  1.1923794746398926\n",
      "Warning: nan gradient found. The current loss is:  1.2197555303573608\n",
      "Warning: nan gradient found. The current loss is:  0.2328975349664688\n",
      "Warning: nan gradient found. The current loss is:  0.4133671224117279\n",
      "Warning: nan gradient found. The current loss is:  0.6969873905181885\n",
      "Warning: nan gradient found. The current loss is:  1.060166835784912\n",
      "Warning: nan gradient found. The current loss is:  0.4733016788959503\n",
      "Warning: nan gradient found. The current loss is:  0.581143856048584\n",
      "Warning: nan gradient found. The current loss is:  0.2945503890514374\n",
      "Warning: nan gradient found. The current loss is:  0.5268585085868835\n",
      "Warning: nan gradient found. The current loss is:  1.032157063484192\n",
      "Warning: nan gradient found. The current loss is:  0.6582459211349487\n",
      "Warning: nan gradient found. The current loss is:  0.22669687867164612\n",
      "Warning: nan gradient found. The current loss is:  1.1663200855255127\n",
      "Warning: nan gradient found. The current loss is:  0.6656998991966248\n",
      "Warning: nan gradient found. The current loss is:  0.342651903629303\n",
      "Warning: nan gradient found. The current loss is:  0.31628870964050293\n",
      "Warning: nan gradient found. The current loss is:  0.6265955567359924\n",
      "Warning: nan gradient found. The current loss is:  0.6366079449653625\n",
      "Warning: nan gradient found. The current loss is:  0.08591526746749878\n",
      "Warning: nan gradient found. The current loss is:  1.2461280822753906\n",
      "Warning: nan gradient found. The current loss is:  1.5394937992095947\n",
      "Warning: nan gradient found. The current loss is:  0.6252393126487732\n",
      "Warning: nan gradient found. The current loss is:  0.4679964780807495\n",
      "Warning: nan gradient found. The current loss is:  0.2694377303123474\n",
      "Warning: nan gradient found. The current loss is:  0.824286162853241\n",
      "Warning: nan gradient found. The current loss is:  0.5469406843185425\n",
      "Warning: nan gradient found. The current loss is:  0.46201515197753906\n",
      "Warning: nan gradient found. The current loss is:  0.590659499168396\n",
      "Warning: nan gradient found. The current loss is:  0.9317429065704346\n",
      "Warning: nan gradient found. The current loss is:  0.5120683908462524\n",
      "Warning: nan gradient found. The current loss is:  0.8750796318054199\n",
      "Warning: nan gradient found. The current loss is:  0.5644382238388062\n",
      "Warning: nan gradient found. The current loss is:  1.024280309677124\n",
      "Warning: nan gradient found. The current loss is:  0.5914525389671326\n",
      "Warning: nan gradient found. The current loss is:  0.7743496894836426\n",
      "Warning: nan gradient found. The current loss is:  0.369145929813385\n",
      "Warning: nan gradient found. The current loss is:  0.5226997137069702\n",
      "Warning: nan gradient found. The current loss is:  0.4594454765319824\n",
      "Warning: nan gradient found. The current loss is:  0.4035194516181946\n",
      "Warning: nan gradient found. The current loss is:  1.5054324865341187\n",
      "Warning: nan gradient found. The current loss is:  1.4762859344482422\n",
      "Warning: nan gradient found. The current loss is:  0.4995989501476288\n",
      "Warning: nan gradient found. The current loss is:  0.4083864092826843\n",
      "Warning: nan gradient found. The current loss is:  2.7411534786224365\n",
      "Warning: nan gradient found. The current loss is:  1.1663947105407715\n",
      "Warning: nan gradient found. The current loss is:  0.4532420039176941\n",
      "Warning: nan gradient found. The current loss is:  0.22526785731315613\n",
      "Warning: nan gradient found. The current loss is:  1.0601530075073242\n",
      "Warning: nan gradient found. The current loss is:  0.020261012017726898\n",
      "Warning: nan gradient found. The current loss is:  0.24411457777023315\n",
      "Warning: nan gradient found. The current loss is:  0.4151572585105896\n",
      "Warning: nan gradient found. The current loss is:  0.34379029273986816\n",
      "Warning: nan gradient found. The current loss is:  0.6778442859649658\n",
      "Warning: nan gradient found. The current loss is:  0.8585227727890015\n",
      "Warning: nan gradient found. The current loss is:  1.4410220384597778\n",
      "Warning: nan gradient found. The current loss is:  0.9715589284896851\n",
      "Warning: nan gradient found. The current loss is:  0.0462830476462841\n",
      "Warning: nan gradient found. The current loss is:  0.6231693029403687\n",
      "Warning: nan gradient found. The current loss is:  0.605419933795929\n",
      "Warning: nan gradient found. The current loss is:  0.556022584438324\n",
      "Warning: nan gradient found. The current loss is:  0.5799449682235718\n",
      "Warning: nan gradient found. The current loss is:  0.26788780093193054\n",
      "Warning: nan gradient found. The current loss is:  -0.02887384034693241\n",
      "Warning: nan gradient found. The current loss is:  1.2533400058746338\n",
      "Warning: nan gradient found. The current loss is:  0.3705751895904541\n",
      "Warning: nan gradient found. The current loss is:  0.26782798767089844\n",
      "Warning: nan gradient found. The current loss is:  0.13544385135173798\n",
      "Warning: nan gradient found. The current loss is:  1.7384989261627197\n",
      "Warning: nan gradient found. The current loss is:  6.259923458099365\n",
      "Warning: nan gradient found. The current loss is:  0.3184787929058075\n",
      "Warning: nan gradient found. The current loss is:  0.5301499366760254\n",
      "Warning: nan gradient found. The current loss is:  0.7973134517669678\n",
      "Warning: nan gradient found. The current loss is:  0.9686917066574097\n",
      "Warning: nan gradient found. The current loss is:  0.5405920147895813\n",
      "Warning: nan gradient found. The current loss is:  0.8450681567192078\n",
      "Warning: nan gradient found. The current loss is:  0.7335416078567505\n",
      "Current batch training loss: 0.733542  [1971200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8581974506378174\n",
      "Warning: nan gradient found. The current loss is:  0.3501768112182617\n",
      "Warning: nan gradient found. The current loss is:  0.7512959241867065\n",
      "Warning: nan gradient found. The current loss is:  0.7336127161979675\n",
      "Warning: nan gradient found. The current loss is:  1.0628198385238647\n",
      "Warning: nan gradient found. The current loss is:  0.5356079339981079\n",
      "Warning: nan gradient found. The current loss is:  0.8661438226699829\n",
      "Warning: nan gradient found. The current loss is:  0.3927823305130005\n",
      "Warning: nan gradient found. The current loss is:  0.23404254019260406\n",
      "Warning: nan gradient found. The current loss is:  0.5630503296852112\n",
      "Warning: nan gradient found. The current loss is:  0.7423778772354126\n",
      "Warning: nan gradient found. The current loss is:  1.0862493515014648\n",
      "Warning: nan gradient found. The current loss is:  0.6366271376609802\n",
      "Warning: nan gradient found. The current loss is:  0.3728889524936676\n",
      "Warning: nan gradient found. The current loss is:  0.3029361367225647\n",
      "Warning: nan gradient found. The current loss is:  0.7476043105125427\n",
      "Warning: nan gradient found. The current loss is:  1.6499130725860596\n",
      "Warning: nan gradient found. The current loss is:  0.8176477551460266\n",
      "Warning: nan gradient found. The current loss is:  1.232405185699463\n",
      "Warning: nan gradient found. The current loss is:  0.12290498614311218\n",
      "Warning: nan gradient found. The current loss is:  0.25394895672798157\n",
      "Warning: nan gradient found. The current loss is:  0.36733198165893555\n",
      "Warning: nan gradient found. The current loss is:  1.1821895837783813\n",
      "Warning: nan gradient found. The current loss is:  0.4399757385253906\n",
      "Warning: nan gradient found. The current loss is:  1.3548521995544434\n",
      "Warning: nan gradient found. The current loss is:  1.663985252380371\n",
      "Warning: nan gradient found. The current loss is:  1.0139892101287842\n",
      "Warning: nan gradient found. The current loss is:  1.3854758739471436\n",
      "Warning: nan gradient found. The current loss is:  0.8071117997169495\n",
      "Warning: nan gradient found. The current loss is:  0.18806932866573334\n",
      "Warning: nan gradient found. The current loss is:  0.36620861291885376\n",
      "Warning: nan gradient found. The current loss is:  0.45444992184638977\n",
      "Warning: nan gradient found. The current loss is:  0.3526614010334015\n",
      "Warning: nan gradient found. The current loss is:  1.8527302742004395\n",
      "Warning: nan gradient found. The current loss is:  0.7679837942123413\n",
      "Warning: nan gradient found. The current loss is:  0.25188302993774414\n",
      "Warning: nan gradient found. The current loss is:  0.3883041441440582\n",
      "Warning: nan gradient found. The current loss is:  0.753190279006958\n",
      "Warning: nan gradient found. The current loss is:  0.7323435544967651\n",
      "Warning: nan gradient found. The current loss is:  1.1903274059295654\n",
      "Warning: nan gradient found. The current loss is:  0.31821906566619873\n",
      "Warning: nan gradient found. The current loss is:  0.0781697928905487\n",
      "Warning: nan gradient found. The current loss is:  0.8952703475952148\n",
      "Warning: nan gradient found. The current loss is:  0.31610938906669617\n",
      "Warning: nan gradient found. The current loss is:  0.48031288385391235\n",
      "Warning: nan gradient found. The current loss is:  0.17915043234825134\n",
      "Warning: nan gradient found. The current loss is:  0.9382098913192749\n",
      "Warning: nan gradient found. The current loss is:  0.6519949436187744\n",
      "Warning: nan gradient found. The current loss is:  0.8735828399658203\n",
      "Warning: nan gradient found. The current loss is:  1.1681058406829834\n",
      "Warning: nan gradient found. The current loss is:  0.26533573865890503\n",
      "Warning: nan gradient found. The current loss is:  0.646701991558075\n",
      "Warning: nan gradient found. The current loss is:  0.3990023136138916\n",
      "Warning: nan gradient found. The current loss is:  0.4200107455253601\n",
      "Warning: nan gradient found. The current loss is:  2.5518200397491455\n",
      "Warning: nan gradient found. The current loss is:  0.6545978784561157\n",
      "Warning: nan gradient found. The current loss is:  0.6132534146308899\n",
      "Warning: nan gradient found. The current loss is:  1.0962191820144653\n",
      "Warning: nan gradient found. The current loss is:  1.6378440856933594\n",
      "Warning: nan gradient found. The current loss is:  1.2564926147460938\n",
      "Warning: nan gradient found. The current loss is:  1.1764484643936157\n",
      "Warning: nan gradient found. The current loss is:  0.6954403519630432\n",
      "Warning: nan gradient found. The current loss is:  0.44796741008758545\n",
      "Warning: nan gradient found. The current loss is:  0.6037498712539673\n",
      "Warning: nan gradient found. The current loss is:  2.2383124828338623\n",
      "Warning: nan gradient found. The current loss is:  0.43174412846565247\n",
      "Warning: nan gradient found. The current loss is:  1.0329633951187134\n",
      "Warning: nan gradient found. The current loss is:  1.1746129989624023\n",
      "Warning: nan gradient found. The current loss is:  0.8554235696792603\n",
      "Warning: nan gradient found. The current loss is:  0.8867998719215393\n",
      "Warning: nan gradient found. The current loss is:  0.7157812118530273\n",
      "Warning: nan gradient found. The current loss is:  0.49247920513153076\n",
      "Warning: nan gradient found. The current loss is:  1.5128984451293945\n",
      "Warning: nan gradient found. The current loss is:  1.1116472482681274\n",
      "Warning: nan gradient found. The current loss is:  0.7668639421463013\n",
      "Warning: nan gradient found. The current loss is:  0.9748764634132385\n",
      "Warning: nan gradient found. The current loss is:  0.16813185811042786\n",
      "Warning: nan gradient found. The current loss is:  0.3034577965736389\n",
      "Warning: nan gradient found. The current loss is:  0.8373112678527832\n",
      "Warning: nan gradient found. The current loss is:  0.6239514350891113\n",
      "Warning: nan gradient found. The current loss is:  0.6845656037330627\n",
      "Warning: nan gradient found. The current loss is:  0.7238897681236267\n",
      "Warning: nan gradient found. The current loss is:  0.3103399872779846\n",
      "Warning: nan gradient found. The current loss is:  0.5311911702156067\n",
      "Warning: nan gradient found. The current loss is:  0.7420406341552734\n",
      "Warning: nan gradient found. The current loss is:  0.4768167734146118\n",
      "Warning: nan gradient found. The current loss is:  0.18791696429252625\n",
      "Warning: nan gradient found. The current loss is:  0.3086327910423279\n",
      "Warning: nan gradient found. The current loss is:  0.8724473714828491\n",
      "Warning: nan gradient found. The current loss is:  0.2310013771057129\n",
      "Warning: nan gradient found. The current loss is:  1.410284399986267\n",
      "Warning: nan gradient found. The current loss is:  1.2950868606567383\n",
      "Warning: nan gradient found. The current loss is:  0.7141373157501221\n",
      "Warning: nan gradient found. The current loss is:  0.44884708523750305\n",
      "Warning: nan gradient found. The current loss is:  0.2887599468231201\n",
      "Warning: nan gradient found. The current loss is:  0.44646570086479187\n",
      "Warning: nan gradient found. The current loss is:  0.3735012412071228\n",
      "Warning: nan gradient found. The current loss is:  0.6806206107139587\n",
      "Warning: nan gradient found. The current loss is:  0.19698798656463623\n",
      "Warning: nan gradient found. The current loss is:  0.11895968019962311\n",
      "Current batch training loss: 0.118960  [1996800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.36880767345428467\n",
      "Warning: nan gradient found. The current loss is:  0.515506386756897\n",
      "Warning: nan gradient found. The current loss is:  0.4856169521808624\n",
      "Warning: nan gradient found. The current loss is:  0.5204795002937317\n",
      "Warning: nan gradient found. The current loss is:  0.7496035695075989\n",
      "Warning: nan gradient found. The current loss is:  0.34493303298950195\n",
      "Warning: nan gradient found. The current loss is:  0.501063346862793\n",
      "Warning: nan gradient found. The current loss is:  0.6175651550292969\n",
      "Warning: nan gradient found. The current loss is:  0.2759553790092468\n",
      "Warning: nan gradient found. The current loss is:  0.8631162047386169\n",
      "Warning: nan gradient found. The current loss is:  0.7246148586273193\n",
      "Warning: nan gradient found. The current loss is:  0.9748610258102417\n",
      "Warning: nan gradient found. The current loss is:  0.8108755946159363\n",
      "Warning: nan gradient found. The current loss is:  0.850119948387146\n",
      "Warning: nan gradient found. The current loss is:  1.1084188222885132\n",
      "Warning: nan gradient found. The current loss is:  0.4533896744251251\n",
      "Warning: nan gradient found. The current loss is:  0.5118054747581482\n",
      "Warning: nan gradient found. The current loss is:  0.4564892649650574\n",
      "Warning: nan gradient found. The current loss is:  1.0370217561721802\n",
      "Warning: nan gradient found. The current loss is:  0.6358806490898132\n",
      "Warning: nan gradient found. The current loss is:  1.8463637828826904\n",
      "Warning: nan gradient found. The current loss is:  0.6584581136703491\n",
      "Warning: nan gradient found. The current loss is:  0.7802480459213257\n",
      "Warning: nan gradient found. The current loss is:  0.42841076850891113\n",
      "Warning: nan gradient found. The current loss is:  0.7571264505386353\n",
      "Warning: nan gradient found. The current loss is:  0.7990893721580505\n",
      "Warning: nan gradient found. The current loss is:  1.1390365362167358\n",
      "Warning: nan gradient found. The current loss is:  0.4510670304298401\n",
      "Warning: nan gradient found. The current loss is:  0.731250524520874\n",
      "Warning: nan gradient found. The current loss is:  0.7437570095062256\n",
      "Warning: nan gradient found. The current loss is:  0.8480616211891174\n",
      "Warning: nan gradient found. The current loss is:  0.43430817127227783\n",
      "Warning: nan gradient found. The current loss is:  0.6170506477355957\n",
      "Warning: nan gradient found. The current loss is:  0.33207130432128906\n",
      "Warning: nan gradient found. The current loss is:  1.4324345588684082\n",
      "Warning: nan gradient found. The current loss is:  0.26062554121017456\n",
      "Warning: nan gradient found. The current loss is:  1.2010654211044312\n",
      "Warning: nan gradient found. The current loss is:  0.1017770767211914\n",
      "Warning: nan gradient found. The current loss is:  0.21553780138492584\n",
      "Warning: nan gradient found. The current loss is:  0.3675009608268738\n",
      "Warning: nan gradient found. The current loss is:  0.2651144862174988\n",
      "Warning: nan gradient found. The current loss is:  0.7397867441177368\n",
      "Warning: nan gradient found. The current loss is:  0.8830651640892029\n",
      "Warning: nan gradient found. The current loss is:  1.809878945350647\n",
      "Warning: nan gradient found. The current loss is:  0.6862733364105225\n",
      "Warning: nan gradient found. The current loss is:  0.4199788570404053\n",
      "Warning: nan gradient found. The current loss is:  0.6685556173324585\n",
      "Warning: nan gradient found. The current loss is:  1.5567474365234375\n",
      "Warning: nan gradient found. The current loss is:  0.7990331649780273\n",
      "Warning: nan gradient found. The current loss is:  0.8162367343902588\n",
      "Warning: nan gradient found. The current loss is:  0.46156781911849976\n",
      "Warning: nan gradient found. The current loss is:  0.5773493647575378\n",
      "Warning: nan gradient found. The current loss is:  0.2224113941192627\n",
      "Warning: nan gradient found. The current loss is:  0.46670207381248474\n",
      "Warning: nan gradient found. The current loss is:  1.0674008131027222\n",
      "Warning: nan gradient found. The current loss is:  1.1530656814575195\n",
      "Warning: nan gradient found. The current loss is:  0.5704981088638306\n",
      "Warning: nan gradient found. The current loss is:  0.9142130017280579\n",
      "Warning: nan gradient found. The current loss is:  0.14424791932106018\n",
      "Warning: nan gradient found. The current loss is:  0.8318920731544495\n",
      "Warning: nan gradient found. The current loss is:  0.9223035573959351\n",
      "Warning: nan gradient found. The current loss is:  0.631362795829773\n",
      "Warning: nan gradient found. The current loss is:  0.15276753902435303\n",
      "Warning: nan gradient found. The current loss is:  0.6365326642990112\n",
      "Warning: nan gradient found. The current loss is:  0.5382159948348999\n",
      "Warning: nan gradient found. The current loss is:  0.06018315255641937\n",
      "Warning: nan gradient found. The current loss is:  0.8648550510406494\n",
      "Warning: nan gradient found. The current loss is:  0.08655980229377747\n",
      "Warning: nan gradient found. The current loss is:  0.7570171356201172\n",
      "Warning: nan gradient found. The current loss is:  0.034034207463264465\n",
      "Warning: nan gradient found. The current loss is:  0.5970961451530457\n",
      "Warning: nan gradient found. The current loss is:  1.4221872091293335\n",
      "Warning: nan gradient found. The current loss is:  0.8361817598342896\n",
      "Warning: nan gradient found. The current loss is:  0.1730932891368866\n",
      "Warning: nan gradient found. The current loss is:  0.8357542753219604\n",
      "Warning: nan gradient found. The current loss is:  1.2599992752075195\n",
      "Warning: nan gradient found. The current loss is:  0.9224557876586914\n",
      "Warning: nan gradient found. The current loss is:  0.5361658334732056\n",
      "Warning: nan gradient found. The current loss is:  0.6591876745223999\n",
      "Warning: nan gradient found. The current loss is:  0.4689476191997528\n",
      "Warning: nan gradient found. The current loss is:  0.5205692648887634\n",
      "Warning: nan gradient found. The current loss is:  0.5060540437698364\n",
      "Warning: nan gradient found. The current loss is:  0.658862829208374\n",
      "Warning: nan gradient found. The current loss is:  0.5150305032730103\n",
      "Warning: nan gradient found. The current loss is:  1.2136372327804565\n",
      "Warning: nan gradient found. The current loss is:  0.3330635726451874\n",
      "Warning: nan gradient found. The current loss is:  0.5164089202880859\n",
      "Warning: nan gradient found. The current loss is:  0.9006321430206299\n",
      "Warning: nan gradient found. The current loss is:  0.9033629298210144\n",
      "Warning: nan gradient found. The current loss is:  0.5969300270080566\n",
      "Warning: nan gradient found. The current loss is:  0.6654850244522095\n",
      "Warning: nan gradient found. The current loss is:  1.1643415689468384\n",
      "Warning: nan gradient found. The current loss is:  1.271398901939392\n",
      "Warning: nan gradient found. The current loss is:  0.774859607219696\n",
      "Warning: nan gradient found. The current loss is:  0.3236849009990692\n",
      "Warning: nan gradient found. The current loss is:  0.5279676914215088\n",
      "Warning: nan gradient found. The current loss is:  0.6607141494750977\n",
      "Warning: nan gradient found. The current loss is:  1.2825618982315063\n",
      "Warning: nan gradient found. The current loss is:  0.6595482230186462\n",
      "Warning: nan gradient found. The current loss is:  2.894742012023926\n",
      "Current batch training loss: 2.894742  [2022400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8837740421295166\n",
      "Warning: nan gradient found. The current loss is:  1.1261557340621948\n",
      "Warning: nan gradient found. The current loss is:  0.568872332572937\n",
      "Warning: nan gradient found. The current loss is:  0.6821625828742981\n",
      "Warning: nan gradient found. The current loss is:  0.4270211458206177\n",
      "Warning: nan gradient found. The current loss is:  0.8479951024055481\n",
      "Warning: nan gradient found. The current loss is:  1.88008713722229\n",
      "Warning: nan gradient found. The current loss is:  0.7041113376617432\n",
      "Warning: nan gradient found. The current loss is:  2.7295539379119873\n",
      "Warning: nan gradient found. The current loss is:  0.24793767929077148\n",
      "Warning: nan gradient found. The current loss is:  0.9115318655967712\n",
      "Warning: nan gradient found. The current loss is:  0.5867087841033936\n",
      "Warning: nan gradient found. The current loss is:  0.5093002915382385\n",
      "Warning: nan gradient found. The current loss is:  0.3302568793296814\n",
      "Warning: nan gradient found. The current loss is:  0.5831250548362732\n",
      "Warning: nan gradient found. The current loss is:  0.2622719407081604\n",
      "Warning: nan gradient found. The current loss is:  0.696887731552124\n",
      "Warning: nan gradient found. The current loss is:  0.4955157935619354\n",
      "Warning: nan gradient found. The current loss is:  0.589695394039154\n",
      "Warning: nan gradient found. The current loss is:  0.6153603196144104\n",
      "Warning: nan gradient found. The current loss is:  -0.011677354574203491\n",
      "Warning: nan gradient found. The current loss is:  0.37037789821624756\n",
      "Warning: nan gradient found. The current loss is:  0.5412780046463013\n",
      "Warning: nan gradient found. The current loss is:  0.9974681735038757\n",
      "Warning: nan gradient found. The current loss is:  1.5616841316223145\n",
      "Warning: nan gradient found. The current loss is:  1.140842318534851\n",
      "Warning: nan gradient found. The current loss is:  0.6619704961776733\n",
      "Warning: nan gradient found. The current loss is:  0.6551292538642883\n",
      "Warning: nan gradient found. The current loss is:  0.6521570682525635\n",
      "Warning: nan gradient found. The current loss is:  0.2979058623313904\n",
      "Warning: nan gradient found. The current loss is:  1.0170483589172363\n",
      "Warning: nan gradient found. The current loss is:  0.38864409923553467\n",
      "Warning: nan gradient found. The current loss is:  0.726826548576355\n",
      "Warning: nan gradient found. The current loss is:  0.9192646741867065\n",
      "Warning: nan gradient found. The current loss is:  0.6021797060966492\n",
      "Warning: nan gradient found. The current loss is:  1.147263526916504\n",
      "Warning: nan gradient found. The current loss is:  0.7745192646980286\n",
      "Warning: nan gradient found. The current loss is:  0.8040895462036133\n",
      "Warning: nan gradient found. The current loss is:  0.35678958892822266\n",
      "Warning: nan gradient found. The current loss is:  0.5274537205696106\n",
      "Warning: nan gradient found. The current loss is:  0.730749785900116\n",
      "Warning: nan gradient found. The current loss is:  0.992527961730957\n",
      "Warning: nan gradient found. The current loss is:  0.08775047957897186\n",
      "Warning: nan gradient found. The current loss is:  0.7504367828369141\n",
      "Warning: nan gradient found. The current loss is:  0.9599972367286682\n",
      "Warning: nan gradient found. The current loss is:  0.27181658148765564\n",
      "Warning: nan gradient found. The current loss is:  0.714494526386261\n",
      "Warning: nan gradient found. The current loss is:  0.8996410369873047\n",
      "Warning: nan gradient found. The current loss is:  0.43921133875846863\n",
      "Warning: nan gradient found. The current loss is:  0.4821010231971741\n",
      "Warning: nan gradient found. The current loss is:  0.47401976585388184\n",
      "Warning: nan gradient found. The current loss is:  0.8657437562942505\n",
      "Warning: nan gradient found. The current loss is:  0.8686429262161255\n",
      "Warning: nan gradient found. The current loss is:  0.3354850709438324\n",
      "Warning: nan gradient found. The current loss is:  2.2295749187469482\n",
      "Warning: nan gradient found. The current loss is:  0.43198972940444946\n",
      "Warning: nan gradient found. The current loss is:  0.9832435846328735\n",
      "Warning: nan gradient found. The current loss is:  0.35288870334625244\n",
      "Warning: nan gradient found. The current loss is:  0.9580520987510681\n",
      "Warning: nan gradient found. The current loss is:  0.9730570912361145\n",
      "Warning: nan gradient found. The current loss is:  0.20704025030136108\n",
      "Warning: nan gradient found. The current loss is:  0.6776523590087891\n",
      "Warning: nan gradient found. The current loss is:  0.7258138060569763\n",
      "Warning: nan gradient found. The current loss is:  0.6301734447479248\n",
      "Warning: nan gradient found. The current loss is:  1.0103857517242432\n",
      "Warning: nan gradient found. The current loss is:  0.5020391345024109\n",
      "Warning: nan gradient found. The current loss is:  0.7914148569107056\n",
      "Warning: nan gradient found. The current loss is:  0.7210428714752197\n",
      "Warning: nan gradient found. The current loss is:  0.6361936926841736\n",
      "Warning: nan gradient found. The current loss is:  0.31352829933166504\n",
      "Warning: nan gradient found. The current loss is:  0.13472816348075867\n",
      "Warning: nan gradient found. The current loss is:  1.628669261932373\n",
      "Warning: nan gradient found. The current loss is:  0.6059728860855103\n",
      "Warning: nan gradient found. The current loss is:  0.9874667525291443\n",
      "Warning: nan gradient found. The current loss is:  0.3917831778526306\n",
      "Warning: nan gradient found. The current loss is:  0.839847207069397\n",
      "Warning: nan gradient found. The current loss is:  0.8164610862731934\n",
      "Warning: nan gradient found. The current loss is:  0.7634140849113464\n",
      "Warning: nan gradient found. The current loss is:  0.759929358959198\n",
      "Warning: nan gradient found. The current loss is:  0.5484129190444946\n",
      "Warning: nan gradient found. The current loss is:  0.8797613382339478\n",
      "Warning: nan gradient found. The current loss is:  0.35698848962783813\n",
      "Warning: nan gradient found. The current loss is:  1.4048353433609009\n",
      "Warning: nan gradient found. The current loss is:  0.44820624589920044\n",
      "Warning: nan gradient found. The current loss is:  0.20469769835472107\n",
      "Warning: nan gradient found. The current loss is:  0.30432751774787903\n",
      "Warning: nan gradient found. The current loss is:  0.38716739416122437\n",
      "Warning: nan gradient found. The current loss is:  0.38147544860839844\n",
      "Warning: nan gradient found. The current loss is:  0.6288673281669617\n",
      "Warning: nan gradient found. The current loss is:  0.7438366413116455\n",
      "Warning: nan gradient found. The current loss is:  0.8381885290145874\n",
      "Warning: nan gradient found. The current loss is:  0.601677656173706\n",
      "Warning: nan gradient found. The current loss is:  0.24426183104515076\n",
      "Warning: nan gradient found. The current loss is:  0.5019645690917969\n",
      "Warning: nan gradient found. The current loss is:  3.0236597061157227\n",
      "Warning: nan gradient found. The current loss is:  0.7736960649490356\n",
      "Warning: nan gradient found. The current loss is:  0.7598804235458374\n",
      "Warning: nan gradient found. The current loss is:  0.43109697103500366\n",
      "Warning: nan gradient found. The current loss is:  0.9178164601325989\n",
      "Warning: nan gradient found. The current loss is:  0.9881061315536499\n",
      "Current batch training loss: 0.988106  [2048000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.9557958245277405\n",
      "Warning: nan gradient found. The current loss is:  0.5642149448394775\n",
      "Warning: nan gradient found. The current loss is:  1.0912835597991943\n",
      "Warning: nan gradient found. The current loss is:  0.8974007368087769\n",
      "Warning: nan gradient found. The current loss is:  2.109516143798828\n",
      "Warning: nan gradient found. The current loss is:  0.4468976557254791\n",
      "Warning: nan gradient found. The current loss is:  0.5439110994338989\n",
      "Warning: nan gradient found. The current loss is:  0.6915391087532043\n",
      "Warning: nan gradient found. The current loss is:  0.33234578371047974\n",
      "Warning: nan gradient found. The current loss is:  0.3491312563419342\n",
      "Warning: nan gradient found. The current loss is:  -0.00367911159992218\n",
      "Warning: nan gradient found. The current loss is:  1.0987203121185303\n",
      "Warning: nan gradient found. The current loss is:  0.944674015045166\n",
      "Warning: nan gradient found. The current loss is:  0.2961045801639557\n",
      "Warning: nan gradient found. The current loss is:  0.3690735697746277\n",
      "Warning: nan gradient found. The current loss is:  1.164598822593689\n",
      "Warning: nan gradient found. The current loss is:  1.0310544967651367\n",
      "Warning: nan gradient found. The current loss is:  0.49156254529953003\n",
      "Warning: nan gradient found. The current loss is:  2.87335205078125\n",
      "Warning: nan gradient found. The current loss is:  0.47850000858306885\n",
      "Warning: nan gradient found. The current loss is:  0.30153194069862366\n",
      "Warning: nan gradient found. The current loss is:  0.6595913171768188\n",
      "Warning: nan gradient found. The current loss is:  1.7143080234527588\n",
      "Warning: nan gradient found. The current loss is:  0.9262983202934265\n",
      "Warning: nan gradient found. The current loss is:  0.5264638662338257\n",
      "Warning: nan gradient found. The current loss is:  0.9547423124313354\n",
      "Warning: nan gradient found. The current loss is:  0.8411281704902649\n",
      "Warning: nan gradient found. The current loss is:  0.7321330904960632\n",
      "Warning: nan gradient found. The current loss is:  2.481423854827881\n",
      "Warning: nan gradient found. The current loss is:  0.9124823808670044\n",
      "Warning: nan gradient found. The current loss is:  0.5623114109039307\n",
      "Warning: nan gradient found. The current loss is:  0.6393122673034668\n",
      "Warning: nan gradient found. The current loss is:  0.4368402063846588\n",
      "Warning: nan gradient found. The current loss is:  0.40846267342567444\n",
      "Warning: nan gradient found. The current loss is:  0.4224134087562561\n",
      "Warning: nan gradient found. The current loss is:  0.6556540727615356\n",
      "Warning: nan gradient found. The current loss is:  0.07930182665586472\n",
      "Warning: nan gradient found. The current loss is:  0.8578068614006042\n",
      "Warning: nan gradient found. The current loss is:  0.8368234634399414\n",
      "Warning: nan gradient found. The current loss is:  2.1019341945648193\n",
      "Warning: nan gradient found. The current loss is:  1.189963459968567\n",
      "Warning: nan gradient found. The current loss is:  0.5292917490005493\n",
      "Warning: nan gradient found. The current loss is:  0.3608319163322449\n",
      "Warning: nan gradient found. The current loss is:  0.8006222248077393\n",
      "Warning: nan gradient found. The current loss is:  1.0539863109588623\n",
      "Warning: nan gradient found. The current loss is:  1.253870964050293\n",
      "Warning: nan gradient found. The current loss is:  0.6182745695114136\n",
      "Warning: nan gradient found. The current loss is:  1.028686761856079\n",
      "Warning: nan gradient found. The current loss is:  0.3439820110797882\n",
      "Warning: nan gradient found. The current loss is:  0.4249725639820099\n",
      "Warning: nan gradient found. The current loss is:  0.428252637386322\n",
      "Warning: nan gradient found. The current loss is:  0.9738539457321167\n",
      "Warning: nan gradient found. The current loss is:  0.9659687280654907\n",
      "Warning: nan gradient found. The current loss is:  0.4709174633026123\n",
      "Warning: nan gradient found. The current loss is:  0.5781792998313904\n",
      "Warning: nan gradient found. The current loss is:  0.736865222454071\n",
      "Warning: nan gradient found. The current loss is:  0.5098891258239746\n",
      "Warning: nan gradient found. The current loss is:  0.434461772441864\n",
      "Warning: nan gradient found. The current loss is:  0.8388761878013611\n",
      "Warning: nan gradient found. The current loss is:  0.5103148818016052\n",
      "Warning: nan gradient found. The current loss is:  0.5751460790634155\n",
      "Warning: nan gradient found. The current loss is:  1.1421375274658203\n",
      "Warning: nan gradient found. The current loss is:  0.7333922386169434\n",
      "Warning: nan gradient found. The current loss is:  0.30355972051620483\n",
      "Warning: nan gradient found. The current loss is:  1.0861023664474487\n",
      "Warning: nan gradient found. The current loss is:  0.48099273443222046\n",
      "Warning: nan gradient found. The current loss is:  0.7732987403869629\n",
      "Warning: nan gradient found. The current loss is:  0.5630654096603394\n",
      "Warning: nan gradient found. The current loss is:  0.4056265950202942\n",
      "Warning: nan gradient found. The current loss is:  0.2812429964542389\n",
      "Warning: nan gradient found. The current loss is:  0.9633886814117432\n",
      "Warning: nan gradient found. The current loss is:  0.7813090085983276\n",
      "Warning: nan gradient found. The current loss is:  1.0605788230895996\n",
      "Warning: nan gradient found. The current loss is:  0.8755882382392883\n",
      "Warning: nan gradient found. The current loss is:  1.148663878440857\n",
      "Warning: nan gradient found. The current loss is:  0.5518519878387451\n",
      "Warning: nan gradient found. The current loss is:  0.5159084796905518\n",
      "Warning: nan gradient found. The current loss is:  0.48243749141693115\n",
      "Warning: nan gradient found. The current loss is:  0.7159888744354248\n",
      "Warning: nan gradient found. The current loss is:  0.43700575828552246\n",
      "Warning: nan gradient found. The current loss is:  0.07706150412559509\n",
      "Warning: nan gradient found. The current loss is:  0.8411846160888672\n",
      "Warning: nan gradient found. The current loss is:  0.23722213506698608\n",
      "Warning: nan gradient found. The current loss is:  0.40484899282455444\n",
      "Warning: nan gradient found. The current loss is:  0.8461759090423584\n",
      "Warning: nan gradient found. The current loss is:  0.1310461312532425\n",
      "Warning: nan gradient found. The current loss is:  0.49803751707077026\n",
      "Warning: nan gradient found. The current loss is:  0.3490976095199585\n",
      "Warning: nan gradient found. The current loss is:  0.7529494166374207\n",
      "Warning: nan gradient found. The current loss is:  0.5206899642944336\n",
      "Warning: nan gradient found. The current loss is:  1.3104686737060547\n",
      "Warning: nan gradient found. The current loss is:  0.22526130080223083\n",
      "Warning: nan gradient found. The current loss is:  0.15569695830345154\n",
      "Warning: nan gradient found. The current loss is:  0.8146315813064575\n",
      "Warning: nan gradient found. The current loss is:  0.3445889949798584\n",
      "Warning: nan gradient found. The current loss is:  0.9774637818336487\n",
      "Warning: nan gradient found. The current loss is:  0.30818936228752136\n",
      "Warning: nan gradient found. The current loss is:  0.5966622233390808\n",
      "Warning: nan gradient found. The current loss is:  0.5573943257331848\n",
      "Warning: nan gradient found. The current loss is:  0.8929639458656311\n",
      "Current batch training loss: 0.892964  [2073600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.5679877996444702\n",
      "Warning: nan gradient found. The current loss is:  0.7182838320732117\n",
      "Warning: nan gradient found. The current loss is:  1.1782209873199463\n",
      "Warning: nan gradient found. The current loss is:  1.489505410194397\n",
      "Warning: nan gradient found. The current loss is:  1.6065936088562012\n",
      "Warning: nan gradient found. The current loss is:  0.5137433409690857\n",
      "Warning: nan gradient found. The current loss is:  0.5947489738464355\n",
      "Warning: nan gradient found. The current loss is:  0.5388586521148682\n",
      "Warning: nan gradient found. The current loss is:  0.774858832359314\n",
      "Warning: nan gradient found. The current loss is:  1.016269326210022\n",
      "Warning: nan gradient found. The current loss is:  0.5346409678459167\n",
      "Warning: nan gradient found. The current loss is:  1.3133140802383423\n",
      "Warning: nan gradient found. The current loss is:  1.4376369714736938\n",
      "Warning: nan gradient found. The current loss is:  0.4170701503753662\n",
      "Warning: nan gradient found. The current loss is:  0.0268278568983078\n",
      "Warning: nan gradient found. The current loss is:  0.3946651220321655\n",
      "Warning: nan gradient found. The current loss is:  1.2786927223205566\n",
      "Warning: nan gradient found. The current loss is:  0.7782608270645142\n",
      "Warning: nan gradient found. The current loss is:  0.8898382186889648\n",
      "Warning: nan gradient found. The current loss is:  0.6457870602607727\n",
      "Warning: nan gradient found. The current loss is:  0.9995061159133911\n",
      "Warning: nan gradient found. The current loss is:  0.7120227813720703\n",
      "Warning: nan gradient found. The current loss is:  0.43047791719436646\n",
      "Warning: nan gradient found. The current loss is:  0.5624167323112488\n",
      "Warning: nan gradient found. The current loss is:  0.7154496908187866\n",
      "Warning: nan gradient found. The current loss is:  -0.03719809651374817\n",
      "Warning: nan gradient found. The current loss is:  0.5396075248718262\n",
      "Warning: nan gradient found. The current loss is:  0.6823006868362427\n",
      "Warning: nan gradient found. The current loss is:  0.44163352251052856\n",
      "Warning: nan gradient found. The current loss is:  1.5034422874450684\n",
      "Warning: nan gradient found. The current loss is:  0.929213285446167\n",
      "Warning: nan gradient found. The current loss is:  1.8004714250564575\n",
      "Warning: nan gradient found. The current loss is:  0.5334268808364868\n",
      "Warning: nan gradient found. The current loss is:  0.3729848563671112\n",
      "Warning: nan gradient found. The current loss is:  0.741225004196167\n",
      "Warning: nan gradient found. The current loss is:  0.6147148609161377\n",
      "Warning: nan gradient found. The current loss is:  0.650274932384491\n",
      "Warning: nan gradient found. The current loss is:  0.41329944133758545\n",
      "Warning: nan gradient found. The current loss is:  1.2779333591461182\n",
      "Warning: nan gradient found. The current loss is:  0.5662618279457092\n",
      "Warning: nan gradient found. The current loss is:  0.33707308769226074\n",
      "Warning: nan gradient found. The current loss is:  0.7267846465110779\n",
      "Warning: nan gradient found. The current loss is:  0.43920284509658813\n",
      "Warning: nan gradient found. The current loss is:  0.15426820516586304\n",
      "Warning: nan gradient found. The current loss is:  0.2435474395751953\n",
      "Warning: nan gradient found. The current loss is:  0.18955792486667633\n",
      "Warning: nan gradient found. The current loss is:  0.23667648434638977\n",
      "Warning: nan gradient found. The current loss is:  1.0341870784759521\n",
      "Warning: nan gradient found. The current loss is:  2.3227667808532715\n",
      "Warning: nan gradient found. The current loss is:  0.3968992531299591\n",
      "Warning: nan gradient found. The current loss is:  0.5094913244247437\n",
      "Warning: nan gradient found. The current loss is:  0.37039464712142944\n",
      "Warning: nan gradient found. The current loss is:  0.612383246421814\n",
      "Warning: nan gradient found. The current loss is:  0.31364139914512634\n",
      "Warning: nan gradient found. The current loss is:  1.5712841749191284\n",
      "Warning: nan gradient found. The current loss is:  1.1631428003311157\n",
      "Warning: nan gradient found. The current loss is:  1.4723105430603027\n",
      "Warning: nan gradient found. The current loss is:  1.2042526006698608\n",
      "Warning: nan gradient found. The current loss is:  0.15893512964248657\n",
      "Warning: nan gradient found. The current loss is:  0.7117849588394165\n",
      "Warning: nan gradient found. The current loss is:  1.839293360710144\n",
      "Warning: nan gradient found. The current loss is:  0.9670147895812988\n",
      "Warning: nan gradient found. The current loss is:  0.5501200556755066\n",
      "Warning: nan gradient found. The current loss is:  2.2122368812561035\n",
      "Warning: nan gradient found. The current loss is:  0.675047755241394\n",
      "Warning: nan gradient found. The current loss is:  1.2611632347106934\n",
      "Warning: nan gradient found. The current loss is:  0.506547749042511\n",
      "Warning: nan gradient found. The current loss is:  0.8769403100013733\n",
      "Warning: nan gradient found. The current loss is:  0.5338935256004333\n",
      "Warning: nan gradient found. The current loss is:  0.7648521661758423\n",
      "Warning: nan gradient found. The current loss is:  1.0927501916885376\n",
      "Warning: nan gradient found. The current loss is:  0.5755503177642822\n",
      "Warning: nan gradient found. The current loss is:  0.5215333104133606\n",
      "Warning: nan gradient found. The current loss is:  0.3412351608276367\n",
      "Warning: nan gradient found. The current loss is:  0.7072328329086304\n",
      "Warning: nan gradient found. The current loss is:  0.29565975069999695\n",
      "Warning: nan gradient found. The current loss is:  0.8323548436164856\n",
      "Warning: nan gradient found. The current loss is:  0.7517783641815186\n",
      "Warning: nan gradient found. The current loss is:  0.8522101044654846\n",
      "Warning: nan gradient found. The current loss is:  0.09158951044082642\n",
      "Warning: nan gradient found. The current loss is:  1.322451114654541\n",
      "Warning: nan gradient found. The current loss is:  0.9955964684486389\n",
      "Warning: nan gradient found. The current loss is:  0.8381842970848083\n",
      "Warning: nan gradient found. The current loss is:  0.24823668599128723\n",
      "Warning: nan gradient found. The current loss is:  0.24964821338653564\n",
      "Warning: nan gradient found. The current loss is:  0.7890498638153076\n",
      "Warning: nan gradient found. The current loss is:  0.40405112504959106\n",
      "Warning: nan gradient found. The current loss is:  1.0992226600646973\n",
      "Warning: nan gradient found. The current loss is:  1.3924412727355957\n",
      "Warning: nan gradient found. The current loss is:  0.791220486164093\n",
      "Warning: nan gradient found. The current loss is:  0.6312985420227051\n",
      "Warning: nan gradient found. The current loss is:  0.741122305393219\n",
      "Warning: nan gradient found. The current loss is:  0.6581740975379944\n",
      "Warning: nan gradient found. The current loss is:  0.1283896565437317\n",
      "Warning: nan gradient found. The current loss is:  -0.14011749625205994\n",
      "Warning: nan gradient found. The current loss is:  0.6472470760345459\n",
      "Warning: nan gradient found. The current loss is:  0.7807968854904175\n",
      "Warning: nan gradient found. The current loss is:  0.19482755661010742\n",
      "Warning: nan gradient found. The current loss is:  0.317929208278656\n",
      "Warning: nan gradient found. The current loss is:  0.7048183679580688\n",
      "Current batch training loss: 0.704818  [2099200/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.1263367086648941\n",
      "Warning: nan gradient found. The current loss is:  0.6624548435211182\n",
      "Warning: nan gradient found. The current loss is:  1.2360600233078003\n",
      "Warning: nan gradient found. The current loss is:  0.6332383155822754\n",
      "Warning: nan gradient found. The current loss is:  0.12666797637939453\n",
      "Warning: nan gradient found. The current loss is:  0.42132923007011414\n",
      "Warning: nan gradient found. The current loss is:  0.31408756971359253\n",
      "Warning: nan gradient found. The current loss is:  0.8038215637207031\n",
      "Warning: nan gradient found. The current loss is:  0.8661680221557617\n",
      "Warning: nan gradient found. The current loss is:  0.6535842418670654\n",
      "Warning: nan gradient found. The current loss is:  0.9534783363342285\n",
      "Warning: nan gradient found. The current loss is:  1.178120493888855\n",
      "Warning: nan gradient found. The current loss is:  1.2839555740356445\n",
      "Warning: nan gradient found. The current loss is:  0.987445592880249\n",
      "Warning: nan gradient found. The current loss is:  0.5880313515663147\n",
      "Warning: nan gradient found. The current loss is:  0.4213787913322449\n",
      "Warning: nan gradient found. The current loss is:  0.19879364967346191\n",
      "Warning: nan gradient found. The current loss is:  0.8061025738716125\n",
      "Warning: nan gradient found. The current loss is:  0.7463449239730835\n",
      "Warning: nan gradient found. The current loss is:  0.5359330177307129\n",
      "Warning: nan gradient found. The current loss is:  0.7775837779045105\n",
      "Warning: nan gradient found. The current loss is:  1.0501999855041504\n",
      "Warning: nan gradient found. The current loss is:  0.06398710608482361\n",
      "Warning: nan gradient found. The current loss is:  0.5670165419578552\n",
      "Warning: nan gradient found. The current loss is:  0.8624192476272583\n",
      "Warning: nan gradient found. The current loss is:  1.109494686126709\n",
      "Warning: nan gradient found. The current loss is:  0.5645915269851685\n",
      "Warning: nan gradient found. The current loss is:  1.1335176229476929\n",
      "Warning: nan gradient found. The current loss is:  0.10195814073085785\n",
      "Warning: nan gradient found. The current loss is:  0.584254264831543\n",
      "Warning: nan gradient found. The current loss is:  0.20071029663085938\n",
      "Warning: nan gradient found. The current loss is:  0.7515488862991333\n",
      "Warning: nan gradient found. The current loss is:  0.4227076768875122\n",
      "Warning: nan gradient found. The current loss is:  2.0140790939331055\n",
      "Warning: nan gradient found. The current loss is:  0.7485911846160889\n",
      "Warning: nan gradient found. The current loss is:  2.6721792221069336\n",
      "Warning: nan gradient found. The current loss is:  0.5196250677108765\n",
      "Warning: nan gradient found. The current loss is:  1.1418430805206299\n",
      "Warning: nan gradient found. The current loss is:  0.39711064100265503\n",
      "Warning: nan gradient found. The current loss is:  1.5769436359405518\n",
      "Warning: nan gradient found. The current loss is:  0.3603692054748535\n",
      "Warning: nan gradient found. The current loss is:  0.607708215713501\n",
      "Warning: nan gradient found. The current loss is:  0.828421950340271\n",
      "Warning: nan gradient found. The current loss is:  0.03468875586986542\n",
      "Warning: nan gradient found. The current loss is:  0.916445255279541\n",
      "Warning: nan gradient found. The current loss is:  0.36644425988197327\n",
      "Warning: nan gradient found. The current loss is:  0.8907923698425293\n",
      "Warning: nan gradient found. The current loss is:  0.6201739311218262\n",
      "Warning: nan gradient found. The current loss is:  0.5169708132743835\n",
      "Warning: nan gradient found. The current loss is:  0.36683395504951477\n",
      "Warning: nan gradient found. The current loss is:  1.4505207538604736\n",
      "Warning: nan gradient found. The current loss is:  0.5841023325920105\n",
      "Warning: nan gradient found. The current loss is:  0.34245961904525757\n",
      "Warning: nan gradient found. The current loss is:  0.7002314329147339\n",
      "Warning: nan gradient found. The current loss is:  1.3961859941482544\n",
      "Warning: nan gradient found. The current loss is:  1.0202457904815674\n",
      "Warning: nan gradient found. The current loss is:  1.5186569690704346\n",
      "Warning: nan gradient found. The current loss is:  0.6940310001373291\n",
      "Warning: nan gradient found. The current loss is:  2.4846298694610596\n",
      "Warning: nan gradient found. The current loss is:  0.40748071670532227\n",
      "Warning: nan gradient found. The current loss is:  0.46785351634025574\n",
      "Warning: nan gradient found. The current loss is:  0.0609552338719368\n",
      "Warning: nan gradient found. The current loss is:  0.6530027389526367\n",
      "Warning: nan gradient found. The current loss is:  0.2153996229171753\n",
      "Warning: nan gradient found. The current loss is:  0.36822181940078735\n",
      "Warning: nan gradient found. The current loss is:  0.21982119977474213\n",
      "Warning: nan gradient found. The current loss is:  0.703413724899292\n",
      "Warning: nan gradient found. The current loss is:  0.6873025894165039\n",
      "Warning: nan gradient found. The current loss is:  0.535959780216217\n",
      "Warning: nan gradient found. The current loss is:  0.16309943795204163\n",
      "Warning: nan gradient found. The current loss is:  0.535443127155304\n",
      "Warning: nan gradient found. The current loss is:  0.3320935368537903\n",
      "Warning: nan gradient found. The current loss is:  0.5585384964942932\n",
      "Warning: nan gradient found. The current loss is:  0.5330888032913208\n",
      "Warning: nan gradient found. The current loss is:  0.8565677404403687\n",
      "Warning: nan gradient found. The current loss is:  0.7927453517913818\n",
      "Warning: nan gradient found. The current loss is:  0.39175570011138916\n",
      "Warning: nan gradient found. The current loss is:  1.696214199066162\n",
      "Warning: nan gradient found. The current loss is:  0.03773495927453041\n",
      "Warning: nan gradient found. The current loss is:  0.3962280750274658\n",
      "Warning: nan gradient found. The current loss is:  0.5059505701065063\n",
      "Warning: nan gradient found. The current loss is:  1.1225477457046509\n",
      "Warning: nan gradient found. The current loss is:  0.8333531618118286\n",
      "Warning: nan gradient found. The current loss is:  1.541725993156433\n",
      "Warning: nan gradient found. The current loss is:  0.3136538863182068\n",
      "Warning: nan gradient found. The current loss is:  0.3618917465209961\n",
      "Warning: nan gradient found. The current loss is:  0.7107490301132202\n",
      "Warning: nan gradient found. The current loss is:  0.6471337080001831\n",
      "Warning: nan gradient found. The current loss is:  1.8217055797576904\n",
      "Warning: nan gradient found. The current loss is:  0.20424503087997437\n",
      "Warning: nan gradient found. The current loss is:  1.039306402206421\n",
      "Warning: nan gradient found. The current loss is:  0.4650171399116516\n",
      "Warning: nan gradient found. The current loss is:  0.46540170907974243\n",
      "Warning: nan gradient found. The current loss is:  0.44972583651542664\n",
      "Warning: nan gradient found. The current loss is:  1.1427385807037354\n",
      "Warning: nan gradient found. The current loss is:  0.6841977834701538\n",
      "Warning: nan gradient found. The current loss is:  0.46420717239379883\n",
      "Warning: nan gradient found. The current loss is:  0.30896490812301636\n",
      "Warning: nan gradient found. The current loss is:  1.097355604171753\n",
      "Warning: nan gradient found. The current loss is:  1.276179552078247\n",
      "Current batch training loss: 1.276180  [2124800/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8100998401641846\n",
      "Warning: nan gradient found. The current loss is:  0.24105849862098694\n",
      "Warning: nan gradient found. The current loss is:  0.741504967212677\n",
      "Warning: nan gradient found. The current loss is:  0.5091215968132019\n",
      "Warning: nan gradient found. The current loss is:  0.8267248868942261\n",
      "Warning: nan gradient found. The current loss is:  0.13697674870491028\n",
      "Warning: nan gradient found. The current loss is:  0.46965497732162476\n",
      "Warning: nan gradient found. The current loss is:  0.648403525352478\n",
      "Warning: nan gradient found. The current loss is:  0.24437418580055237\n",
      "Warning: nan gradient found. The current loss is:  0.8362154960632324\n",
      "Warning: nan gradient found. The current loss is:  1.007087230682373\n",
      "Warning: nan gradient found. The current loss is:  0.5303550958633423\n",
      "Warning: nan gradient found. The current loss is:  1.0448681116104126\n",
      "Warning: nan gradient found. The current loss is:  0.31724876165390015\n",
      "Warning: nan gradient found. The current loss is:  0.22435492277145386\n",
      "Warning: nan gradient found. The current loss is:  0.39847666025161743\n",
      "Warning: nan gradient found. The current loss is:  0.5618134140968323\n",
      "Warning: nan gradient found. The current loss is:  2.116483449935913\n",
      "Warning: nan gradient found. The current loss is:  0.3464856743812561\n",
      "Warning: nan gradient found. The current loss is:  0.3497079610824585\n",
      "Warning: nan gradient found. The current loss is:  0.5404043793678284\n",
      "Warning: nan gradient found. The current loss is:  0.6569101214408875\n",
      "Warning: nan gradient found. The current loss is:  0.2084776759147644\n",
      "Warning: nan gradient found. The current loss is:  0.6713864803314209\n",
      "Warning: nan gradient found. The current loss is:  0.597777247428894\n",
      "Warning: nan gradient found. The current loss is:  1.0074927806854248\n",
      "Warning: nan gradient found. The current loss is:  0.7648465037345886\n",
      "Warning: nan gradient found. The current loss is:  0.5639629364013672\n",
      "Warning: nan gradient found. The current loss is:  0.361839234828949\n",
      "Warning: nan gradient found. The current loss is:  1.002282738685608\n",
      "Warning: nan gradient found. The current loss is:  0.5356191396713257\n",
      "Warning: nan gradient found. The current loss is:  0.2979186773300171\n",
      "Warning: nan gradient found. The current loss is:  0.7455205321311951\n",
      "Warning: nan gradient found. The current loss is:  0.46032941341400146\n",
      "Warning: nan gradient found. The current loss is:  0.5691865086555481\n",
      "Warning: nan gradient found. The current loss is:  0.17753243446350098\n",
      "Warning: nan gradient found. The current loss is:  0.14551198482513428\n",
      "Warning: nan gradient found. The current loss is:  0.4735535979270935\n",
      "Warning: nan gradient found. The current loss is:  1.0087511539459229\n",
      "Warning: nan gradient found. The current loss is:  0.5244870781898499\n",
      "Warning: nan gradient found. The current loss is:  0.2446679025888443\n",
      "Warning: nan gradient found. The current loss is:  1.02395498752594\n",
      "Warning: nan gradient found. The current loss is:  0.7119030952453613\n",
      "Warning: nan gradient found. The current loss is:  0.8791378140449524\n",
      "Warning: nan gradient found. The current loss is:  0.4995231032371521\n",
      "Warning: nan gradient found. The current loss is:  0.5226535797119141\n",
      "Warning: nan gradient found. The current loss is:  1.0849330425262451\n",
      "Warning: nan gradient found. The current loss is:  0.7525988221168518\n",
      "Warning: nan gradient found. The current loss is:  0.6181355714797974\n",
      "Warning: nan gradient found. The current loss is:  0.38041794300079346\n",
      "Warning: nan gradient found. The current loss is:  0.7388736009597778\n",
      "Warning: nan gradient found. The current loss is:  0.4236990213394165\n",
      "Warning: nan gradient found. The current loss is:  0.5406394004821777\n",
      "Warning: nan gradient found. The current loss is:  0.36307987570762634\n",
      "Warning: nan gradient found. The current loss is:  0.6078373193740845\n",
      "Warning: nan gradient found. The current loss is:  0.520250678062439\n",
      "Warning: nan gradient found. The current loss is:  1.2551004886627197\n",
      "Warning: nan gradient found. The current loss is:  0.334189236164093\n",
      "Warning: nan gradient found. The current loss is:  0.33680400252342224\n",
      "Warning: nan gradient found. The current loss is:  1.2149560451507568\n",
      "Warning: nan gradient found. The current loss is:  0.608960747718811\n",
      "Warning: nan gradient found. The current loss is:  0.8468807339668274\n",
      "Warning: nan gradient found. The current loss is:  0.8444897532463074\n",
      "Warning: nan gradient found. The current loss is:  0.9354673027992249\n",
      "Warning: nan gradient found. The current loss is:  1.16653311252594\n",
      "Warning: nan gradient found. The current loss is:  0.18299967050552368\n",
      "Warning: nan gradient found. The current loss is:  0.629881739616394\n",
      "Warning: nan gradient found. The current loss is:  0.00891667976975441\n",
      "Warning: nan gradient found. The current loss is:  0.7797729969024658\n",
      "Warning: nan gradient found. The current loss is:  0.3999273180961609\n",
      "Warning: nan gradient found. The current loss is:  1.2097803354263306\n",
      "Warning: nan gradient found. The current loss is:  0.27903109788894653\n",
      "Warning: nan gradient found. The current loss is:  0.35255420207977295\n",
      "Warning: nan gradient found. The current loss is:  0.2032400518655777\n",
      "Warning: nan gradient found. The current loss is:  0.37378451228141785\n",
      "Warning: nan gradient found. The current loss is:  0.49748608469963074\n",
      "Warning: nan gradient found. The current loss is:  0.5917168259620667\n",
      "Warning: nan gradient found. The current loss is:  0.37423765659332275\n",
      "Warning: nan gradient found. The current loss is:  0.2792563736438751\n",
      "Warning: nan gradient found. The current loss is:  0.23945912718772888\n",
      "Warning: nan gradient found. The current loss is:  0.5172722339630127\n",
      "Warning: nan gradient found. The current loss is:  0.3199106454849243\n",
      "Warning: nan gradient found. The current loss is:  1.7998006343841553\n",
      "Warning: nan gradient found. The current loss is:  0.6777100563049316\n",
      "Warning: nan gradient found. The current loss is:  0.5420964360237122\n",
      "Warning: nan gradient found. The current loss is:  0.34189698100090027\n",
      "Warning: nan gradient found. The current loss is:  0.8414170145988464\n",
      "Warning: nan gradient found. The current loss is:  0.764519989490509\n",
      "Warning: nan gradient found. The current loss is:  0.538296103477478\n",
      "Warning: nan gradient found. The current loss is:  0.7788928151130676\n",
      "Warning: nan gradient found. The current loss is:  0.5921449065208435\n",
      "Warning: nan gradient found. The current loss is:  1.014120101928711\n",
      "Warning: nan gradient found. The current loss is:  0.3282720148563385\n",
      "Warning: nan gradient found. The current loss is:  0.07591266930103302\n",
      "Warning: nan gradient found. The current loss is:  0.7455331683158875\n",
      "Warning: nan gradient found. The current loss is:  0.3517190217971802\n",
      "Warning: nan gradient found. The current loss is:  0.7775391936302185\n",
      "Warning: nan gradient found. The current loss is:  0.35245245695114136\n",
      "Warning: nan gradient found. The current loss is:  0.34492766857147217\n",
      "Warning: nan gradient found. The current loss is:  0.8402432203292847\n",
      "Current batch training loss: 0.840243  [2150400/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.8828373551368713\n",
      "Warning: nan gradient found. The current loss is:  1.234687328338623\n",
      "Warning: nan gradient found. The current loss is:  0.6637451648712158\n",
      "Warning: nan gradient found. The current loss is:  0.6469388008117676\n",
      "Warning: nan gradient found. The current loss is:  1.33596932888031\n",
      "Warning: nan gradient found. The current loss is:  0.7382083535194397\n",
      "Warning: nan gradient found. The current loss is:  0.3825491964817047\n",
      "Warning: nan gradient found. The current loss is:  0.7392622828483582\n",
      "Warning: nan gradient found. The current loss is:  1.2509406805038452\n",
      "Warning: nan gradient found. The current loss is:  0.35207268595695496\n",
      "Warning: nan gradient found. The current loss is:  0.7272152900695801\n",
      "Warning: nan gradient found. The current loss is:  1.5789191722869873\n",
      "Warning: nan gradient found. The current loss is:  0.26357826590538025\n",
      "Warning: nan gradient found. The current loss is:  0.758612871170044\n",
      "Warning: nan gradient found. The current loss is:  0.43305647373199463\n",
      "Warning: nan gradient found. The current loss is:  0.7023441195487976\n",
      "Warning: nan gradient found. The current loss is:  1.0722705125808716\n",
      "Warning: nan gradient found. The current loss is:  0.7317350506782532\n",
      "Warning: nan gradient found. The current loss is:  0.089564248919487\n",
      "Warning: nan gradient found. The current loss is:  1.2266392707824707\n",
      "Warning: nan gradient found. The current loss is:  1.430505633354187\n",
      "Warning: nan gradient found. The current loss is:  0.24193352460861206\n",
      "Warning: nan gradient found. The current loss is:  1.0117501020431519\n",
      "Warning: nan gradient found. The current loss is:  0.4614434540271759\n",
      "Warning: nan gradient found. The current loss is:  0.1612354964017868\n",
      "Warning: nan gradient found. The current loss is:  1.1799278259277344\n",
      "Warning: nan gradient found. The current loss is:  0.6259053349494934\n",
      "Warning: nan gradient found. The current loss is:  0.6520599126815796\n",
      "Warning: nan gradient found. The current loss is:  0.5962609052658081\n",
      "Warning: nan gradient found. The current loss is:  0.841586709022522\n",
      "Warning: nan gradient found. The current loss is:  0.19582617282867432\n",
      "Warning: nan gradient found. The current loss is:  1.8769744634628296\n",
      "Warning: nan gradient found. The current loss is:  0.9717670679092407\n",
      "Warning: nan gradient found. The current loss is:  2.0668179988861084\n",
      "Warning: nan gradient found. The current loss is:  1.3373314142227173\n",
      "Warning: nan gradient found. The current loss is:  0.12953664362430573\n",
      "Warning: nan gradient found. The current loss is:  0.7029016613960266\n",
      "Warning: nan gradient found. The current loss is:  0.8927903175354004\n",
      "Warning: nan gradient found. The current loss is:  0.7865226864814758\n",
      "Warning: nan gradient found. The current loss is:  0.6727965474128723\n",
      "Warning: nan gradient found. The current loss is:  0.5467182397842407\n",
      "Warning: nan gradient found. The current loss is:  0.8810111284255981\n",
      "Warning: nan gradient found. The current loss is:  1.0033615827560425\n",
      "Warning: nan gradient found. The current loss is:  0.44453534483909607\n",
      "Warning: nan gradient found. The current loss is:  1.0091060400009155\n",
      "Warning: nan gradient found. The current loss is:  0.29633089900016785\n",
      "Warning: nan gradient found. The current loss is:  0.4836153984069824\n",
      "Warning: nan gradient found. The current loss is:  0.605677604675293\n",
      "Warning: nan gradient found. The current loss is:  1.2111308574676514\n",
      "Warning: nan gradient found. The current loss is:  1.206986427307129\n",
      "Warning: nan gradient found. The current loss is:  1.0074028968811035\n",
      "Warning: nan gradient found. The current loss is:  0.21660089492797852\n",
      "Warning: nan gradient found. The current loss is:  0.532107412815094\n",
      "Warning: nan gradient found. The current loss is:  1.4413083791732788\n",
      "Warning: nan gradient found. The current loss is:  1.1516400575637817\n",
      "Warning: nan gradient found. The current loss is:  0.6410439610481262\n",
      "Warning: nan gradient found. The current loss is:  0.58655846118927\n",
      "Warning: nan gradient found. The current loss is:  0.3081132471561432\n",
      "Warning: nan gradient found. The current loss is:  2.0508713722229004\n",
      "Warning: nan gradient found. The current loss is:  0.4884479343891144\n",
      "Warning: nan gradient found. The current loss is:  1.2426894903182983\n",
      "Warning: nan gradient found. The current loss is:  0.7143579721450806\n",
      "Warning: nan gradient found. The current loss is:  0.4134170413017273\n",
      "Warning: nan gradient found. The current loss is:  0.10253071039915085\n",
      "Warning: nan gradient found. The current loss is:  1.1445026397705078\n",
      "Warning: nan gradient found. The current loss is:  0.5026094913482666\n",
      "Warning: nan gradient found. The current loss is:  0.7437590956687927\n",
      "Warning: nan gradient found. The current loss is:  0.8797521591186523\n",
      "Warning: nan gradient found. The current loss is:  0.35118725895881653\n",
      "Warning: nan gradient found. The current loss is:  0.4274097681045532\n",
      "Warning: nan gradient found. The current loss is:  0.6915534734725952\n",
      "Warning: nan gradient found. The current loss is:  0.4918842315673828\n",
      "Warning: nan gradient found. The current loss is:  1.7683615684509277\n",
      "Warning: nan gradient found. The current loss is:  1.1397061347961426\n",
      "Warning: nan gradient found. The current loss is:  0.47224754095077515\n",
      "Warning: nan gradient found. The current loss is:  0.649318277835846\n",
      "Warning: nan gradient found. The current loss is:  1.5610049962997437\n",
      "Warning: nan gradient found. The current loss is:  0.9365580081939697\n",
      "Warning: nan gradient found. The current loss is:  1.0215270519256592\n",
      "Warning: nan gradient found. The current loss is:  1.2474901676177979\n",
      "Warning: nan gradient found. The current loss is:  0.8328390121459961\n",
      "Warning: nan gradient found. The current loss is:  0.4267982840538025\n",
      "Warning: nan gradient found. The current loss is:  0.25936296582221985\n",
      "Warning: nan gradient found. The current loss is:  1.1231799125671387\n",
      "Warning: nan gradient found. The current loss is:  1.1529359817504883\n",
      "Warning: nan gradient found. The current loss is:  0.46530675888061523\n",
      "Warning: nan gradient found. The current loss is:  0.6043541431427002\n",
      "Warning: nan gradient found. The current loss is:  0.2784245014190674\n",
      "Warning: nan gradient found. The current loss is:  0.7588914632797241\n",
      "Warning: nan gradient found. The current loss is:  0.6207998991012573\n",
      "Warning: nan gradient found. The current loss is:  0.41557756066322327\n",
      "Warning: nan gradient found. The current loss is:  0.5268898010253906\n",
      "Warning: nan gradient found. The current loss is:  0.7290866374969482\n",
      "Warning: nan gradient found. The current loss is:  0.1252082884311676\n",
      "Warning: nan gradient found. The current loss is:  0.5969765186309814\n",
      "Warning: nan gradient found. The current loss is:  0.5122667551040649\n",
      "Warning: nan gradient found. The current loss is:  0.4723374843597412\n",
      "Warning: nan gradient found. The current loss is:  0.780449390411377\n",
      "Warning: nan gradient found. The current loss is:  0.4603920578956604\n",
      "Warning: nan gradient found. The current loss is:  0.3982124328613281\n",
      "Current batch training loss: 0.398212  [2176000/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.4487869143486023\n",
      "Warning: nan gradient found. The current loss is:  0.5817509889602661\n",
      "Warning: nan gradient found. The current loss is:  0.7958423495292664\n",
      "Warning: nan gradient found. The current loss is:  1.2075765132904053\n",
      "Warning: nan gradient found. The current loss is:  0.14719051122665405\n",
      "Warning: nan gradient found. The current loss is:  0.5037416815757751\n",
      "Warning: nan gradient found. The current loss is:  0.22768370807170868\n",
      "Warning: nan gradient found. The current loss is:  0.10486903786659241\n",
      "Warning: nan gradient found. The current loss is:  1.3190155029296875\n",
      "Warning: nan gradient found. The current loss is:  0.22038567066192627\n",
      "Warning: nan gradient found. The current loss is:  0.26574641466140747\n",
      "Warning: nan gradient found. The current loss is:  0.6005396246910095\n",
      "Warning: nan gradient found. The current loss is:  0.6148425936698914\n",
      "Warning: nan gradient found. The current loss is:  0.8953436017036438\n",
      "Warning: nan gradient found. The current loss is:  0.6267523765563965\n",
      "Warning: nan gradient found. The current loss is:  0.2624061703681946\n",
      "Warning: nan gradient found. The current loss is:  0.5258928537368774\n",
      "Warning: nan gradient found. The current loss is:  1.0054478645324707\n",
      "Warning: nan gradient found. The current loss is:  0.2750307023525238\n",
      "Warning: nan gradient found. The current loss is:  0.0693574994802475\n",
      "Warning: nan gradient found. The current loss is:  0.20240482687950134\n",
      "Warning: nan gradient found. The current loss is:  0.39070481061935425\n",
      "Warning: nan gradient found. The current loss is:  0.9673709273338318\n",
      "Warning: nan gradient found. The current loss is:  0.8920882344245911\n",
      "Warning: nan gradient found. The current loss is:  0.048832375556230545\n",
      "Warning: nan gradient found. The current loss is:  -0.1050688773393631\n",
      "Warning: nan gradient found. The current loss is:  1.1589932441711426\n",
      "Warning: nan gradient found. The current loss is:  0.6947734951972961\n",
      "Warning: nan gradient found. The current loss is:  1.3888286352157593\n",
      "Warning: nan gradient found. The current loss is:  0.29982316493988037\n",
      "Warning: nan gradient found. The current loss is:  0.23152104020118713\n",
      "Warning: nan gradient found. The current loss is:  1.3132728338241577\n",
      "Warning: nan gradient found. The current loss is:  0.733101487159729\n",
      "Warning: nan gradient found. The current loss is:  0.6931015253067017\n",
      "Warning: nan gradient found. The current loss is:  0.333863228559494\n",
      "Warning: nan gradient found. The current loss is:  1.1630833148956299\n",
      "Warning: nan gradient found. The current loss is:  0.5646321177482605\n",
      "Warning: nan gradient found. The current loss is:  0.33743780851364136\n",
      "Warning: nan gradient found. The current loss is:  0.7286726236343384\n",
      "Warning: nan gradient found. The current loss is:  0.9032454490661621\n",
      "Warning: nan gradient found. The current loss is:  0.2754470705986023\n",
      "Warning: nan gradient found. The current loss is:  0.3741293251514435\n",
      "Warning: nan gradient found. The current loss is:  0.6505869030952454\n",
      "Warning: nan gradient found. The current loss is:  0.4444083869457245\n",
      "Warning: nan gradient found. The current loss is:  0.33787280321121216\n",
      "Warning: nan gradient found. The current loss is:  0.280219167470932\n",
      "Warning: nan gradient found. The current loss is:  0.7128438949584961\n",
      "Warning: nan gradient found. The current loss is:  0.4829789400100708\n",
      "Warning: nan gradient found. The current loss is:  1.054834246635437\n",
      "Warning: nan gradient found. The current loss is:  1.0683634281158447\n",
      "Warning: nan gradient found. The current loss is:  0.5243452191352844\n",
      "Warning: nan gradient found. The current loss is:  1.1709210872650146\n",
      "Warning: nan gradient found. The current loss is:  0.7274771928787231\n",
      "Warning: nan gradient found. The current loss is:  0.7561531066894531\n",
      "Warning: nan gradient found. The current loss is:  0.76921546459198\n",
      "Warning: nan gradient found. The current loss is:  0.5583705902099609\n",
      "Warning: nan gradient found. The current loss is:  2.0763981342315674\n",
      "Warning: nan gradient found. The current loss is:  0.30393877625465393\n",
      "Warning: nan gradient found. The current loss is:  1.004096269607544\n",
      "Warning: nan gradient found. The current loss is:  0.1627177745103836\n",
      "Warning: nan gradient found. The current loss is:  0.8650521039962769\n",
      "Warning: nan gradient found. The current loss is:  0.7275399565696716\n",
      "Warning: nan gradient found. The current loss is:  0.7086403965950012\n",
      "Warning: nan gradient found. The current loss is:  0.6172267198562622\n",
      "Warning: nan gradient found. The current loss is:  0.4495207369327545\n",
      "Warning: nan gradient found. The current loss is:  0.5467368960380554\n",
      "Warning: nan gradient found. The current loss is:  0.677676260471344\n",
      "Warning: nan gradient found. The current loss is:  0.7088976502418518\n",
      "Warning: nan gradient found. The current loss is:  0.4327128231525421\n",
      "Warning: nan gradient found. The current loss is:  0.7284837961196899\n",
      "Warning: nan gradient found. The current loss is:  0.8747758865356445\n",
      "Warning: nan gradient found. The current loss is:  1.124206304550171\n",
      "Warning: nan gradient found. The current loss is:  0.37837231159210205\n",
      "Warning: nan gradient found. The current loss is:  0.6217517852783203\n",
      "Warning: nan gradient found. The current loss is:  0.755699634552002\n",
      "Warning: nan gradient found. The current loss is:  0.9717289209365845\n",
      "Warning: nan gradient found. The current loss is:  0.31915217638015747\n",
      "Warning: nan gradient found. The current loss is:  1.684598684310913\n",
      "Warning: nan gradient found. The current loss is:  0.6373817920684814\n",
      "Warning: nan gradient found. The current loss is:  0.8470996618270874\n",
      "Warning: nan gradient found. The current loss is:  0.5837043523788452\n",
      "Warning: nan gradient found. The current loss is:  1.0954508781433105\n",
      "Warning: nan gradient found. The current loss is:  0.44308802485466003\n",
      "Warning: nan gradient found. The current loss is:  1.5459651947021484\n",
      "Warning: nan gradient found. The current loss is:  0.6088330149650574\n",
      "Warning: nan gradient found. The current loss is:  0.21266335248947144\n",
      "Warning: nan gradient found. The current loss is:  1.009562611579895\n",
      "Warning: nan gradient found. The current loss is:  0.8717645406723022\n",
      "Warning: nan gradient found. The current loss is:  0.4976470470428467\n",
      "Warning: nan gradient found. The current loss is:  0.43079936504364014\n",
      "Warning: nan gradient found. The current loss is:  0.6826810836791992\n",
      "Warning: nan gradient found. The current loss is:  0.6247915029525757\n",
      "Warning: nan gradient found. The current loss is:  0.46622177958488464\n",
      "Warning: nan gradient found. The current loss is:  0.7314162254333496\n",
      "Warning: nan gradient found. The current loss is:  0.4339509904384613\n",
      "Warning: nan gradient found. The current loss is:  0.9163315296173096\n",
      "Warning: nan gradient found. The current loss is:  0.7541759014129639\n",
      "Warning: nan gradient found. The current loss is:  0.2892245352268219\n",
      "Warning: nan gradient found. The current loss is:  0.7007599472999573\n",
      "Warning: nan gradient found. The current loss is:  0.5135641098022461\n",
      "Current batch training loss: 0.513564  [2201600/2213439]\n",
      "Warning: nan gradient found. The current loss is:  0.6121131181716919\n",
      "Warning: nan gradient found. The current loss is:  0.1735353320837021\n",
      "Warning: nan gradient found. The current loss is:  0.6706656217575073\n",
      "Warning: nan gradient found. The current loss is:  0.9497811794281006\n",
      "Warning: nan gradient found. The current loss is:  0.5721311569213867\n",
      "Warning: nan gradient found. The current loss is:  0.27201047539711\n",
      "Warning: nan gradient found. The current loss is:  1.1706011295318604\n",
      "Warning: nan gradient found. The current loss is:  0.617034375667572\n",
      "Warning: nan gradient found. The current loss is:  0.7883753776550293\n",
      "Warning: nan gradient found. The current loss is:  0.7027267217636108\n",
      "Warning: nan gradient found. The current loss is:  0.4807021915912628\n",
      "Warning: nan gradient found. The current loss is:  1.0362716913223267\n",
      "Warning: nan gradient found. The current loss is:  1.5867069959640503\n",
      "Warning: nan gradient found. The current loss is:  0.837211012840271\n",
      "Warning: nan gradient found. The current loss is:  0.625005841255188\n",
      "Warning: nan gradient found. The current loss is:  0.6680059432983398\n",
      "Warning: nan gradient found. The current loss is:  0.6683603525161743\n",
      "Warning: nan gradient found. The current loss is:  1.0922036170959473\n",
      "Warning: nan gradient found. The current loss is:  0.1658739447593689\n",
      "Warning: nan gradient found. The current loss is:  0.658663809299469\n",
      "Warning: nan gradient found. The current loss is:  0.7157178521156311\n",
      "Warning: nan gradient found. The current loss is:  0.3469935357570648\n",
      "Warning: nan gradient found. The current loss is:  0.6904085874557495\n",
      "Warning: nan gradient found. The current loss is:  2.474026679992676\n",
      "Warning: nan gradient found. The current loss is:  0.5093799829483032\n",
      "Warning: nan gradient found. The current loss is:  0.9399378299713135\n",
      "Warning: nan gradient found. The current loss is:  0.591549277305603\n",
      "Warning: nan gradient found. The current loss is:  1.0521224737167358\n",
      "Warning: nan gradient found. The current loss is:  0.6280316114425659\n",
      "Warning: nan gradient found. The current loss is:  0.9577566385269165\n",
      "Warning: nan gradient found. The current loss is:  1.3747458457946777\n",
      "Warning: nan gradient found. The current loss is:  1.5467733144760132\n",
      "Warning: nan gradient found. The current loss is:  0.8238774538040161\n",
      "Warning: nan gradient found. The current loss is:  0.5995578765869141\n",
      "Warning: nan gradient found. The current loss is:  0.5868685841560364\n",
      "Warning: nan gradient found. The current loss is:  0.3001422882080078\n",
      "Warning: nan gradient found. The current loss is:  0.2125243991613388\n",
      "Warning: nan gradient found. The current loss is:  0.7588546276092529\n",
      "Warning: nan gradient found. The current loss is:  0.2278711497783661\n",
      "Warning: nan gradient found. The current loss is:  0.45805448293685913\n",
      "Warning: nan gradient found. The current loss is:  0.7331011891365051\n",
      "Warning: nan gradient found. The current loss is:  0.7320312261581421\n",
      "Warning: nan gradient found. The current loss is:  0.717185378074646\n",
      "Warning: nan gradient found. The current loss is:  0.6875780820846558\n",
      "Warning: nan gradient found. The current loss is:  0.553999125957489\n",
      "Warning: nan gradient found. The current loss is:  0.31414365768432617\n",
      "Training loss: 0.671623\n",
      "Validation loss: 0.922677 \n",
      "\n",
      "Stopping early!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Implement early stopping in training loop\n",
    "# Stop if validation loss has not decreased for the last [patience] epochs\n",
    "# The model with the lowest loss is stored\n",
    "patience = 2\n",
    "\n",
    "Training_losses = np.array([])\n",
    "Validation_losses = np.array([])\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    Training_losses = np.append(Training_losses, mytools.train(train_dataloader, model, mytools.NLLloss, optimizer, device))\n",
    "    Validation_losses = np.append(Validation_losses, mytools.validate(val_dataloader, model, mytools.NLLloss, device))\n",
    "    \n",
    "    # Keep a running copy of the model with the lowest loss\n",
    "    # Do not copy the model if loss in nan\n",
    "    if (Validation_losses[-1] == np.min(Validation_losses)) and (~np.isnan(Validation_losses[-1])):\n",
    "        final_model = copy.deepcopy(model)\n",
    "    \n",
    "    if len(Validation_losses) > patience:\n",
    "        if np.sum((Validation_losses[-1*np.arange(patience)-1] - Validation_losses[-1*np.arange(patience)-2]) < 0) == 0:\n",
    "            print(\"Stopping early!\")\n",
    "            break\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6c9d9-757e-420c-ad6f-63911baab785",
   "metadata": {},
   "source": [
    "# Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c85e00-c2a4-47d0-9639-5c7ed7adb8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T14:27:57.396034Z",
     "iopub.status.busy": "2023-06-18T14:27:57.395557Z",
     "iopub.status.idle": "2023-06-18T14:27:57.617635Z",
     "shell.execute_reply": "2023-06-18T14:27:57.616394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b5445b78e20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Training and Validation Loss\n",
    "# Indicate where the final model stopped training\n",
    "\n",
    "best_epoch = np.argmin(Validation_losses[~np.isnan(Training_losses)])+1\n",
    "\n",
    "plt.plot(np.arange(len(Training_losses))+1,Training_losses,label=\"Training Loss\")\n",
    "plt.plot(np.arange(len(Validation_losses))+1,Validation_losses,label=\"Validation Loss\")\n",
    "plt.axvline(best_epoch,label=\"Final model stopped here\",color='k')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log Likelihood Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6f17-057a-4fad-8b14-a86b6d30e259",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176dad4a-cc29-488c-887b-aad8ae00be06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T14:27:57.622045Z",
     "iopub.status.busy": "2023-06-18T14:27:57.621215Z",
     "iopub.status.idle": "2023-06-18T14:27:57.693410Z",
     "shell.execute_reply": "2023-06-18T14:27:57.691722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to ../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(final_model, \"../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256_small1.pt\")\n",
    "print(\"Saved PyTorch Model State to ../3D_Heteroscedastic_Convnet_models/3D_HSCDC_CNN_subM-256_small1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c2ff-12bc-4799-bcfd-47142a9baebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039de899-ca03-40d8-befc-c5c75c4b87f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137a2665c242313c11d472736bb1efbdaf7608c607fce3fc4f47a32817024ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
