{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2e6ffd-ee56-4f14-ba45-439e1e45857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mytools\n",
    "import pandas as pd\n",
    "\n",
    "import spconv.pytorch as spconv\n",
    "from spconv.pytorch import functional as Fsp\n",
    "from spconv.pytorch.utils import PointToVoxel\n",
    "from spconv.pytorch.hash import HashTable\n",
    "import contextlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb73c3-5eea-45bd-a497-c986db160cfa",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10e975b-a5ce-47ca-8301-7184ade2540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  1943113\n",
      "Validation samples:  485778\n"
     ]
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "st_info = pd.read_pickle('/home/majd/sparse_training_tensors/sparse_tensor_info.pk')\n",
    "st_info.head()\n",
    "\n",
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc='/home/majd/sparse_training_tensors/', st_info=st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042d17c-6d88-4929-bc97-c1427004f3e4",
   "metadata": {},
   "source": [
    "# Define Convnet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2164f207-01a8-4f08-b732-aa8afdc31400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = spconv.SparseSequential(\n",
    "            spconv.SparseConv3d(in_channels=1, out_channels=50, kernel_size=6, stride=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseMaxPool3d(kernel_size=2, stride=2),\n",
    "            spconv.SparseConv3d(in_channels=50, out_channels=30, kernel_size=4, stride=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseConv3d(in_channels=30, out_channels=20, kernel_size=3, stride=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "            spconv.SparseMaxPool3d(kernel_size=2, stride=2),\n",
    "            spconv.ToDense(),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(12**3 *20, 100)\n",
    "        self.fc2 = nn.Linear(100, 30)\n",
    "        self.fc3 = nn.Linear(30, 3)\n",
    "        \n",
    "        self.shape = shape\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        x_sp = spconv.SparseConvTensor.from_dense(x.reshape(-1, 120, 120, 120, 1))\n",
    "        \n",
    "        \n",
    "        x = self.net(x_sp)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        output = F.normalize(self.fc3(x),dim=1)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "model = Net(shape =torch.Tensor((120,120,120))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840324e3-654c-4dd0-8de1-36fbb89a82e1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcc104e-7746-4794-a117-fa2b4b88a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-07)\n",
    "\n",
    "# Define Loss function\n",
    "CS = nn.CosineSimilarity()\n",
    "def loss_fn(output, target):\n",
    "    loss = torch.mean(-1.0*CS(output,target))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02846e2e-d3e9-4eb2-865b-d57284c43c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Current batch training loss: -0.033057  [    0/1943113]\n",
      "Current batch training loss: -0.252800  [ 6400/1943113]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     Training_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(Training_losses, \u001b[43mmytools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     Validation_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(Validation_losses, mytools\u001b[38;5;241m.\u001b[39mvalidate(val_dataloader, model, loss_fn, device))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Keep a running copy of the model with the lowest loss\u001b[39;00m\n",
      "File \u001b[0;32m~/3D_Heteroscedastic_Convnet/mytools.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 102\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    105\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()        \n",
      "File \u001b[0;32m~/.conda/envs/mypt/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mypt/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Implement early stopping in training loop\n",
    "# Stop if validation loss has not decreased for the last [patience] epochs\n",
    "# The model with the lowest loss is stored\n",
    "patience = 2\n",
    "\n",
    "Training_losses = np.array([])\n",
    "Validation_losses = np.array([])\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    Training_losses = np.append(Training_losses, mytools.train(train_dataloader, model, loss_fn, optimizer, device))\n",
    "    Validation_losses = np.append(Validation_losses, mytools.validate(val_dataloader, model, loss_fn, device))\n",
    "    \n",
    "    # Keep a running copy of the model with the lowest loss\n",
    "    if Validation_losses[-1] == np.min(Validation_losses):\n",
    "        final_model = copy.deepcopy(model)\n",
    "    \n",
    "    if len(Validation_losses) > patience:\n",
    "        if np.sum((Validation_losses[-1*np.arange(patience)-1] - Validation_losses[-1*np.arange(patience)-2]) < 0) == 0:\n",
    "            print(\"Stopping early!\")\n",
    "            break\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6c9d9-757e-420c-ad6f-63911baab785",
   "metadata": {},
   "source": [
    "# Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c85e00-c2a4-47d0-9639-5c7ed7adb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss\n",
    "# Indicate where the final model stopped training\n",
    "\n",
    "best_epoch = np.argmin(Validation_losses)\n",
    "\n",
    "plt.plot(np.arange(len(Training_losses)),Training_losses,label=\"Training Loss\")\n",
    "plt.plot(np.arange(len(Validation_losses)),Validation_losses,label=\"Validation Loss\")\n",
    "plt.axvline(best_epoch,label=\"Final model stopped here\",color='k')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cosine Similarity Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6f17-057a-4fad-8b14-a86b6d30e259",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dad4a-cc29-488c-887b-aad8ae00be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model, \"../3D_Heteroscedastic_Convnet_models/3D_CNN-sparse.pt\")\n",
    "print(\"Saved PyTorch Model State to ../3D_Heteroscedastic_Convnet_models/3D_CNN-sparse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c2ff-12bc-4799-bcfd-47142a9baebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137a2665c242313c11d472736bb1efbdaf7608c607fce3fc4f47a32817024ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
