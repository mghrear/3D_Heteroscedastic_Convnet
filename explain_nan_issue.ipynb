{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2e6ffd-ee56-4f14-ba45-439e1e45857d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import spconv.pytorch as spconv\n",
    "import matplotlib.pyplot as plt\n",
    "import mytools\n",
    "import mymodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca7436-5aa9-4d78-b591-892fff6ddbe1",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e54fbb-f8ae-4660-8e12-db36dd8417b9",
   "metadata": {},
   "source": [
    "The first note is that the 1st term of our loss function explodes, while an almost equivalent high K approximation does not. Idea: use approximation (loss2) when the original (loss1) explodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a95bed8-85d9-49e7-9ff4-9c461441cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa:  tensor(2.6500)\n",
      "Original 1st term:  tensor(3.5083)\n",
      "High K approx.:  tensor(3.5133)\n",
      "O(15) TS about 0:  tensor(3.5134)\n",
      "--------------------------------------------\n",
      "loss1-loss2 is 0:  tensor(False) tensor(-0.0050) frac error:  tensor(-0.0014)\n",
      "loss1-loss3 is 0:  tensor(False) tensor(-0.0050) frac error:  tensor(-0.0014)\n"
     ]
    }
   ],
   "source": [
    "K = torch.tensor(2.65)\n",
    "\n",
    "# Try below case to see why Taylor Series method eventually fails for arrows case\n",
    "# K = torch.tensor(538.0)\n",
    "# K = torch.tensor(2.65)\n",
    "\n",
    "# The 1st term of our loss function\n",
    "loss1 = -1.0 * torch.log(torch.div(K,4*torch.pi*torch.sinh(K)))\n",
    "\n",
    "# A high K approximation of the 1st term of our loss function\n",
    "loss2 = -1.0 * ( torch.log(torch.div(K,2*torch.pi)) - K )\n",
    "\n",
    "# 15th order Taylor series of 1st term of our loss function about K=0\n",
    "loss3 = K**2/6 - K**4/180 + K**6/2835 - K**8/37800 + K**10/467775 - (691* (K**12) )/ 3831077250 + (2 * (K**14))/127702575 + torch.log(torch.tensor(4)*torch.pi)\n",
    "\n",
    "\n",
    "\n",
    "print(\"kappa: \", K)\n",
    "print(\"Original 1st term: \", loss1)\n",
    "print(\"High K approx.: \", loss2)\n",
    "print(\"O(15) TS about 0: \", loss3)\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"loss1-loss2 is 0: \", (loss1-loss2)==0, loss1-loss2, \"frac error: \", (loss1-loss2)/loss1 )\n",
    "\n",
    "print(\"loss1-loss3 is 0: \", (loss1-loss3)==0, loss1-loss3, \"frac error: \", (loss1-loss3)/loss1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bdad89-73e9-47d1-af2d-6d8dc50a0a55",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a5a0c-9987-4dc3-9adc-b1fa77664809",
   "metadata": {},
   "source": [
    "Pytorch seems to have issues computing the gradient of 1st term of our loss function, even before it explodes. Idea: use approximation (loss2) when Kappa > 30, otherwise use original (loss1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0a28575-c160-4503-8f98-0f00a2d80bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic grad:  tensor(0.8889)\n",
      "Torch grad:  tensor(0.8889)\n",
      "Torch grad approx:  tensor(0.8889)\n",
      "Torch grad TS:  tensor(496788.5625)\n",
      "----------------------------------------------\n",
      "error grad2:  tensor(1.1921e-07) , fractional error:  tensor(0.)\n",
      "error grad3:  tensor(-496787.6875) , fractional error:  tensor(-558886.1250)\n"
     ]
    }
   ],
   "source": [
    "val = 9.0\n",
    "\n",
    "# Analytic grad of 1st term of our loss function\n",
    "x = torch.tensor(val, requires_grad = False)\n",
    "grad = (1/torch.tanh(x))-(1/x)\n",
    "print(\"Analytic grad: \", grad)\n",
    "\n",
    "# Pytorch grad of 1st term of our loss function\n",
    "x1 = torch.tensor(val, requires_grad = True)\n",
    "y1 = -1.0 * torch.log(torch.div(x1,4*torch.pi*torch.sinh(x1)))\n",
    "y1.backward()\n",
    "print(\"Torch grad: \",x1.grad)\n",
    "\n",
    "# Pytorch grad of high K approximation of the 1st term of our loss function\n",
    "x2 = torch.tensor(val, requires_grad = True)\n",
    "y2 = -1.0 * ( torch.log(torch.div(x2,2*torch.pi)) - x2 )\n",
    "y2.backward()\n",
    "print(\"Torch grad approx: \",x2.grad)\n",
    "\n",
    "# Pytorch grad of 15th order Taylor series of 1st term of our loss function about K=0\n",
    "x3 = torch.tensor(val, requires_grad = True)\n",
    "y3 = x3**2/6 - x3**4/180 + x3**6/2835 - x3**8/37800 + x3**10/467775 - (691* (x3**12) )/ 3831077250 + (2 * (x3**14))/127702575 + torch.log(torch.tensor(4)*torch.pi)\n",
    "y3.backward()\n",
    "print(\"Torch grad TS: \",x3.grad)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"error grad2: \", grad-x2.grad, \", fractional error: \", (x1.grad-x2.grad)/x1.grad)\n",
    "print(\"error grad3: \", grad-x3.grad, \", fractional error: \",  (x1.grad-x3.grad)/x1.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb73c3-5eea-45bd-a497-c986db160cfa",
   "metadata": {},
   "source": [
    "# Issues with torch.where and torch.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296729a-c53e-44fa-9431-ec22660742b8",
   "metadata": {},
   "source": [
    "If one of the arguments in torch.minimum or torch.where is inf, then the gradient will always be nan, even when the condition chooses the differentiable argument.\n",
    "\n",
    "For example: below, b exists and is differentiable but pytorch says the gradient is nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2b84cf-dcc5-4983-b805-234dd8abbbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(inf, grad_fn=<ExpBackward0>)\n",
      "tensor(101., grad_fn=<WhereBackward0>)\n",
      "tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(100., requires_grad=True)\n",
    "b = torch.where(a < 0, torch.exp(a), 1 + a)\n",
    "b.backward()\n",
    "print(torch.exp(a))\n",
    "print(b)\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b7929f-9759-47e1-af98-a61822472b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(inf, grad_fn=<ExpBackward0>)\n",
      "tensor(101., grad_fn=<MinimumBackward0>)\n",
      "tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(100., requires_grad=True)\n",
    "b = torch.minimum(torch.exp(a), 1 + a)\n",
    "b.backward()\n",
    "print(torch.exp(a))\n",
    "print(b)\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294833-544d-4b81-b917-7459332d414c",
   "metadata": {},
   "source": [
    "Below, we see what the gradient should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc974557-1362-47df-9a40-61082e21bacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(100., requires_grad=True)\n",
    "b = 1 + a\n",
    "b.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a753a4b-754d-4b0a-b01b-e3e2b360fcb5",
   "metadata": {},
   "source": [
    "# Fixing the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e4f61-2d67-4deb-924c-de3e0f64f560",
   "metadata": {},
   "source": [
    "## Attempt 1: Using torch.nan_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e71dc9-405d-4b1d-80d5-e72900b8e0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(100., requires_grad=True)\n",
    "b = torch.where(a < 0, torch.nan_to_num(torch.exp(a)), 1 + a)\n",
    "b.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31225492-c295-42b3-99d6-701aeda204fb",
   "metadata": {},
   "source": [
    "This attempt clearly did not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02433b-2866-46fb-b4b0-6382e8e8feb5",
   "metadata": {},
   "source": [
    "## Attempt 2: using masked tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cf4ff-8e42-4e71-92b2-cf577c1b8315",
   "metadata": {},
   "source": [
    "This bug is discussed here: https://github.com/pytorch/pytorch/issues/10729. Pytorch created masked tensors to adress this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff30be47-09f8-4b59-8e83-e5faf55030e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mx.grad:  MaskedTensor(\n",
      "  [  0.0000,   0.0067,       --,       --,       --,       --,       --,       --,       --,       --,       --]\n",
      ")\n",
      "my.grad:  MaskedTensor(\n",
      "  [      --,       --,   1.0000,   1.0000,   1.0000,   1.0000,   1.0000,   1.0000,   1.0000,   1.0000,   1.0000]\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majd/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:156: UserWarning: The PyTorch API of MaskedTensors is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.masked module for further information about the project.\n",
      "  warnings.warn((\"The PyTorch API of MaskedTensors is in prototype stage \"\n",
      "/home/majd/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:161: UserWarning: It is not recommended to create a MaskedTensor with a tensor that requires_grad. To avoid this, you can use data.clone().detach()\n",
      "  warnings.warn(\"It is not recommended to create a MaskedTensor with a tensor that requires_grad. \"\n"
     ]
    }
   ],
   "source": [
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "\n",
    "x = torch.tensor([-10., -5, 0, 5, 10, 50, 60, 70, 80, 90, 100], requires_grad=True)\n",
    "mask = x < 0\n",
    "mx = masked_tensor(x, mask, requires_grad=True)\n",
    "my = masked_tensor(torch.ones_like(x), ~mask, requires_grad=True)\n",
    "y = torch.where(mask, torch.exp(mx), my)\n",
    "s = y.sum()\n",
    "s.backward()\n",
    "\n",
    "print(\"mx.grad: \", mx.grad)\n",
    "print(\"my.grad: \", my.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56614f7a-0230-4dd2-8773-cfe9ecd6b5a1",
   "metadata": {},
   "source": [
    "However, I'm not totaly sure how to implement this / whether it supports the operations that I am using. See the error below when I try to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c68267dd-cb6e-49de-8387-d1a75e549a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  4000\n",
      "Validation samples:  1000\n",
      "Shape of X [N, C, H, W, D]: torch.Size([256, 120, 120, 120, 1])\n",
      "Shape of y: torch.Size([256, 3]) torch.float32\n",
      "Offsets:  torch.Size([256, 3])\n",
      "Voxel grid shape:  torch.Size([120, 120, 120])\n",
      "Using cuda device\n",
      "spConvnet_HSCDC_subM(\n",
      "  (net): SparseSequential(\n",
      "    (0): SubMConv3d(1, 32, kernel_size=[7, 7, 7], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (1): ReLU()\n",
      "    (2): SubMConv3d(32, 40, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (3): ReLU()\n",
      "    (4): SparseConv3d(40, 50, kernel_size=[6, 6, 6], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (5): ReLU()\n",
      "    (6): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (7): SparseConv3d(50, 30, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (8): ReLU()\n",
      "    (9): SparseConv3d(30, 10, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (10): ReLU()\n",
      "    (11): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (12): ToDense()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2160, out_features=500, bias=True)\n",
      "  (fc2_1): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_1): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_1): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (fc2_2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "file_loc = '/home/majd/sparse_training_arrows/'\n",
    "st_info = pd.read_pickle(file_loc+'sparse_tensor_info.pk')\n",
    "\n",
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc = file_loc, st_info = st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create training and validation DataLoaders\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print tensor shapes\n",
    "for X_plot, y_plot, offset_plot in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W, D]: {X_plot.shape}\")\n",
    "    print(f\"Shape of y: {y_plot.shape} {y_plot.dtype}\")\n",
    "    print(\"Offsets: \", offset_plot.shape)\n",
    "    break\n",
    "    \n",
    "#Record shape of voxel grid\n",
    "grid_shape = X_plot.shape[1:4]\n",
    "print(\"Voxel grid shape: \" , grid_shape)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = mymodels.spConvnet_HSCDC_subM(shape = grid_shape ).to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28c4315e-3d49-4606-b53d-39f631a16429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss for HSCDC convnet\n",
    "def NLLloss_masked(output, target):\n",
    "    \n",
    "    # target us the x parameters in the Kent distribution\n",
    "    G = output[0] # \\gamma_1 parameters in Kent distribution\n",
    "    K = output[1].flatten() # \\kappa parameter in Kent distribution\n",
    "    \n",
    "    loss1 = -1.0 * torch.log(torch.div(K,4*torch.pi*torch.sinh(K)))\n",
    "    loss2 = -1.0 * ( torch.log(torch.div(K,2*torch.pi)) - K )\n",
    "    \n",
    "    mask = K<30.0\n",
    "    \n",
    "    mx = masked_tensor(loss1.clone().detach(), mask)\n",
    "    my = masked_tensor(loss2.clone().detach(), ~mask)\n",
    "    \n",
    "    loss_K = torch.where(mask, mx, my)\n",
    "    \n",
    "        \n",
    "    # Compute negative log likelihood using Kent distribution\n",
    "    loss = torch.mean( loss_K - ( K * torch.sum(G*target,dim=1) ))\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12ab73de-1004-4736-b729-367bbd92e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majd/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:156: UserWarning: The PyTorch API of MaskedTensors is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.masked module for further information about the project.\n",
      "  warnings.warn((\"The PyTorch API of MaskedTensors is in prototype stage \"\n",
      "/home/majd/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:161: UserWarning: It is not recommended to create a MaskedTensor with a tensor that requires_grad. To avoid this, you can use data.clone().detach()\n",
      "  warnings.warn(\"It is not recommended to create a MaskedTensor with a tensor that requires_grad. \"\n",
      "/home/majd/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:299: UserWarning: softplus_backward is not implemented in __torch_dispatch__ for MaskedTensor.\n",
      "If you would like this operator to be supported, please file an issue for a feature request at https://github.com/pytorch/maskedtensor/issues with a minimal reproducible code snippet.\n",
      "In the case that the semantics for the operator are not trivial, it would be appreciated to also include a proposal for the semantics.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'torch._ops.aten.softplus_backward.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     Training_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(Training_losses, \u001b[43mmytools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNLLloss_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     Validation_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(Validation_losses, mytools\u001b[38;5;241m.\u001b[39mvalidate(val_dataloader, model, NLLloss_masked, device))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Keep a running copy of the model with the lowest loss\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Do not copy the model if loss in nan\u001b[39;00m\n",
      "File \u001b[0;32m~/3D_Heteroscedastic_Convnet/mytools.py:208\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    207\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 208\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Check for Nans in gradient (this sometimes happens for the heteroscedastic model)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Only check parameters requiring grad\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mypt3/lib/python3.10/site-packages/torch/_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/mypt3/lib/python3.10/site-packages/torch/overrides.py:1551\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1546\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1547\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1551\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/mypt3/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:274\u001b[0m, in \u001b[0;36mMaskedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.conda/envs/mypt3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mypt3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'torch._ops.aten.softplus_backward.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>]"
     ]
    }
   ],
   "source": [
    "# Implement early stopping in training loop\n",
    "# Stop if validation loss has not decreased for the last [patience] epochs\n",
    "# The model with the lowest loss is stored\n",
    "patience = 2\n",
    "\n",
    "Training_losses = np.array([])\n",
    "Validation_losses = np.array([])\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    Training_losses = np.append(Training_losses, mytools.train(train_dataloader, model, NLLloss_masked, optimizer, device))\n",
    "    Validation_losses = np.append(Validation_losses, mytools.validate(val_dataloader, model, NLLloss_masked, device))\n",
    "    \n",
    "    # Keep a running copy of the model with the lowest loss\n",
    "    # Do not copy the model if loss in nan\n",
    "    if (Validation_losses[-1] == np.min(Validation_losses)) and (~np.isnan(Validation_losses[-1])):\n",
    "        final_model = copy.deepcopy(model)\n",
    "    \n",
    "    if len(Validation_losses) > patience:\n",
    "        if np.sum((Validation_losses[-1*np.arange(patience)-1] - Validation_losses[-1*np.arange(patience)-2]) < 0) == 0:\n",
    "            print(\"Stopping early!\")\n",
    "            break\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b1a62-120d-453a-8647-65052f448bab",
   "metadata": {},
   "source": [
    "## Attempt 3: using a Taylor series to replace the function that causes the inf values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebeeb2-b250-4484-803c-6f78ff024ba6",
   "metadata": {},
   "source": [
    "The 1st term of our loss function causes the inf values which lead to nans. The high K approximation of the 1st term of our loss function is extremely accurate even to low Ks. Therefore, we replace our first time completely. For low values of K (0-2.6), we use a 15th order taylor series about k=0/ Fot higher values (K>2.6), we use the high K approximation. With this treatment there are no more inf values given to torch.where and hence no more nan gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88fc4705-a8fd-4212-8a24-aafd463b9e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  4000\n",
      "Validation samples:  1000\n",
      "Shape of X [N, C, H, W, D]: torch.Size([256, 120, 120, 120, 1])\n",
      "Shape of y: torch.Size([256, 3]) torch.float32\n",
      "Offsets:  torch.Size([256, 3])\n",
      "Voxel grid shape:  torch.Size([120, 120, 120])\n",
      "Using cuda device\n",
      "spConvnet_HSCDC_subM(\n",
      "  (net): SparseSequential(\n",
      "    (0): SubMConv3d(1, 32, kernel_size=[7, 7, 7], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (1): ReLU()\n",
      "    (2): SubMConv3d(32, 40, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (3): ReLU()\n",
      "    (4): SparseConv3d(40, 50, kernel_size=[6, 6, 6], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.Native)\n",
      "    (5): ReLU()\n",
      "    (6): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (7): SparseConv3d(50, 30, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (8): ReLU()\n",
      "    (9): SparseConv3d(30, 10, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (10): ReLU()\n",
      "    (11): SparseMaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=[0, 0, 0], dilation=[1, 1, 1], algo=ConvAlgo.MaskImplicitGemm)\n",
      "    (12): ToDense()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2160, out_features=500, bias=True)\n",
      "  (fc2_1): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_1): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_1): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (fc2_2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (fc3_2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc4_2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Read pandas dataframe with all information about sparse training tensors\n",
    "file_loc = '/home/majd/sparse_training_arrows/'\n",
    "st_info = pd.read_pickle(file_loc+'sparse_tensor_info.pk')\n",
    "\n",
    "# Make custom dataset\n",
    "MyDataset = mytools.CustomDataset(dir_loc = file_loc, st_info = st_info)\n",
    "\n",
    "# Split datat into training, validation, and testing sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(MyDataset,[0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Training samples: \", len(train_dataset))\n",
    "print(\"Validation samples: \", len(val_dataset))\n",
    "\n",
    "# Create training and validation DataLoaders\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print tensor shapes\n",
    "for X_plot, y_plot, offset_plot in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W, D]: {X_plot.shape}\")\n",
    "    print(f\"Shape of y: {y_plot.shape} {y_plot.dtype}\")\n",
    "    print(\"Offsets: \", offset_plot.shape)\n",
    "    break\n",
    "    \n",
    "#Record shape of voxel grid\n",
    "grid_shape = X_plot.shape[1:4]\n",
    "print(\"Voxel grid shape: \" , grid_shape)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = mymodels.spConvnet_HSCDC_subM(shape = grid_shape ).to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Specify optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.0001, betas=(0.94, 0.999), eps=1e-08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ca5d6f7-dc6f-4ce3-a62e-802ee84bb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss for HSCDC convnet\n",
    "def NLLloss_TS(output, target):\n",
    "    \n",
    "    # target us the x parameters in the Kent distribution\n",
    "    G = output[0] # \\gamma_1 parameters in Kent distribution\n",
    "    K = output[1].flatten() # \\kappa parameter in Kent distribution\n",
    "    \n",
    "    # 15th order taylor series about 0\n",
    "    loss1 = K**2/6 - K**4/180 + K**6/2835 - K**8/37800 + K**10/467775 - (691* (K**12) )/ 3831077250 + (2 * (K**14))/127702575 + torch.log(torch.tensor(4)*torch.pi)\n",
    "    # high K approx\n",
    "    loss2 = -1.0 * ( torch.log(torch.div(K,2*torch.pi)) - K )\n",
    "    \n",
    "    loss_K = torch.where(K<2.56, loss1, loss2)\n",
    "    \n",
    "        \n",
    "    # Compute negative log likelihood using Kent distribution\n",
    "    loss = torch.mean( loss_K  - ( K * torch.sum(G*target,dim=1) ))\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87591b5c-c02d-4ee5-a8ed-13f863b6df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Current batch training loss: 2.586734  [    0/ 4000]\n",
      "Training loss: 2.533583\n",
      "Validation loss: 2.128556 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Current batch training loss: 2.131131  [    0/ 4000]\n",
      "Training loss: 1.970529\n",
      "Validation loss: 1.886137 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Current batch training loss: 1.886433  [    0/ 4000]\n",
      "Training loss: 1.811873\n",
      "Validation loss: 1.603441 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Current batch training loss: 1.602957  [    0/ 4000]\n",
      "Training loss: 0.331600\n",
      "Validation loss: -1.465047 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Current batch training loss: -1.467933  [    0/ 4000]\n",
      "Training loss: -2.036310\n",
      "Validation loss: -2.295343 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Current batch training loss: -2.296365  [    0/ 4000]\n",
      "Training loss: -2.447041\n",
      "Validation loss: -2.573034 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Current batch training loss: -2.584703  [    0/ 4000]\n",
      "Training loss: -2.705175\n",
      "Validation loss: -2.877454 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Current batch training loss: -2.887847  [    0/ 4000]\n",
      "Training loss: -3.001905\n",
      "Validation loss: -3.207570 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Current batch training loss: -3.231961  [    0/ 4000]\n",
      "Training loss: -3.253552\n",
      "Validation loss: -3.370853 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Current batch training loss: -3.326795  [    0/ 4000]\n",
      "Training loss: -3.344251\n",
      "Validation loss: -3.357823 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Current batch training loss: -3.322397  [    0/ 4000]\n",
      "Training loss: -3.437818\n",
      "Validation loss: -3.582695 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Current batch training loss: -3.522435  [    0/ 4000]\n",
      "Training loss: -3.640247\n",
      "Validation loss: -3.601645 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Current batch training loss: -3.672229  [    0/ 4000]\n",
      "Training loss: -3.677710\n",
      "Validation loss: -3.660520 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Current batch training loss: -3.770112  [    0/ 4000]\n",
      "Training loss: -3.786581\n",
      "Validation loss: -3.776084 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Current batch training loss: -3.778540  [    0/ 4000]\n",
      "Training loss: -3.859106\n",
      "Validation loss: -3.909065 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Current batch training loss: -4.008731  [    0/ 4000]\n",
      "Training loss: -3.768727\n",
      "Validation loss: -3.604027 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Current batch training loss: -3.635709  [    0/ 4000]\n",
      "Training loss: -3.918511\n",
      "Validation loss: -3.958214 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Current batch training loss: -4.001272  [    0/ 4000]\n",
      "Warning: nan gradient found. The current loss is:  -4.029574394226074\n",
      "Warning: nan gradient found. The current loss is:  -4.081812381744385\n",
      "Warning: nan gradient found. The current loss is:  -4.046936511993408\n",
      "Warning: nan gradient found. The current loss is:  -4.121067047119141\n",
      "Warning: nan gradient found. The current loss is:  -4.144624710083008\n",
      "Warning: nan gradient found. The current loss is:  -4.1492919921875\n",
      "Warning: nan gradient found. The current loss is:  -4.0692548751831055\n",
      "Warning: nan gradient found. The current loss is:  -4.055560111999512\n",
      "Warning: nan gradient found. The current loss is:  -3.9874515533447266\n",
      "Warning: nan gradient found. The current loss is:  -4.044483184814453\n",
      "Warning: nan gradient found. The current loss is:  -3.957928419113159\n",
      "Training loss: -4.054417\n",
      "Validation loss: -3.984800 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Warning: nan gradient found. The current loss is:  -3.926037073135376\n",
      "Current batch training loss: -3.926037  [    0/ 4000]\n",
      "Warning: nan gradient found. The current loss is:  -4.0001091957092285\n",
      "Warning: nan gradient found. The current loss is:  -4.052319049835205\n",
      "Warning: nan gradient found. The current loss is:  -4.048274517059326\n",
      "Warning: nan gradient found. The current loss is:  -4.0612263679504395\n",
      "Warning: nan gradient found. The current loss is:  -4.032163143157959\n",
      "Warning: nan gradient found. The current loss is:  -3.982036828994751\n",
      "Warning: nan gradient found. The current loss is:  -3.968377113342285\n",
      "Warning: nan gradient found. The current loss is:  -3.9267148971557617\n",
      "Warning: nan gradient found. The current loss is:  -3.8945107460021973\n",
      "Warning: nan gradient found. The current loss is:  -3.914149761199951\n",
      "Warning: nan gradient found. The current loss is:  -4.029667377471924\n",
      "Warning: nan gradient found. The current loss is:  -3.952544689178467\n",
      "Warning: nan gradient found. The current loss is:  -3.9766786098480225\n",
      "Warning: nan gradient found. The current loss is:  -4.00880241394043\n",
      "Warning: nan gradient found. The current loss is:  -3.9891037940979004\n",
      "Training loss: -3.985170\n",
      "Validation loss: -3.983254 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Warning: nan gradient found. The current loss is:  -3.9350807666778564\n",
      "Current batch training loss: -3.935081  [    0/ 4000]\n",
      "Warning: nan gradient found. The current loss is:  -3.983391523361206\n",
      "Warning: nan gradient found. The current loss is:  -3.9991211891174316\n",
      "Warning: nan gradient found. The current loss is:  -3.9359047412872314\n",
      "Warning: nan gradient found. The current loss is:  -3.9911751747131348\n",
      "Warning: nan gradient found. The current loss is:  -4.059366226196289\n",
      "Warning: nan gradient found. The current loss is:  -4.003267288208008\n",
      "Warning: nan gradient found. The current loss is:  -3.9795427322387695\n",
      "Warning: nan gradient found. The current loss is:  -4.0938286781311035\n",
      "Warning: nan gradient found. The current loss is:  -3.909294366836548\n",
      "Warning: nan gradient found. The current loss is:  -4.002553462982178\n",
      "Warning: nan gradient found. The current loss is:  -3.974365234375\n",
      "Warning: nan gradient found. The current loss is:  -3.875128746032715\n",
      "Warning: nan gradient found. The current loss is:  -4.0463104248046875\n",
      "Warning: nan gradient found. The current loss is:  -3.9619569778442383\n",
      "Warning: nan gradient found. The current loss is:  -4.0264153480529785\n",
      "Training loss: -3.986044\n",
      "Validation loss: -3.984019 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Warning: nan gradient found. The current loss is:  -3.9603078365325928\n",
      "Current batch training loss: -3.960308  [    0/ 4000]\n",
      "Warning: nan gradient found. The current loss is:  -3.9361252784729004\n",
      "Warning: nan gradient found. The current loss is:  -3.9712817668914795\n",
      "Warning: nan gradient found. The current loss is:  -3.964503765106201\n",
      "Warning: nan gradient found. The current loss is:  -4.049472808837891\n",
      "Warning: nan gradient found. The current loss is:  -4.039616584777832\n",
      "Warning: nan gradient found. The current loss is:  -3.9827961921691895\n",
      "Warning: nan gradient found. The current loss is:  -4.00740385055542\n",
      "Warning: nan gradient found. The current loss is:  -3.980402946472168\n",
      "Warning: nan gradient found. The current loss is:  -4.0104875564575195\n",
      "Warning: nan gradient found. The current loss is:  -3.9882125854492188\n",
      "Warning: nan gradient found. The current loss is:  -3.915605306625366\n",
      "Warning: nan gradient found. The current loss is:  -3.9766435623168945\n",
      "Warning: nan gradient found. The current loss is:  -3.9960477352142334\n",
      "Warning: nan gradient found. The current loss is:  -4.020016670227051\n",
      "Training loss: -3.984851\n",
      "Validation loss: -3.954136 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Warning: nan gradient found. The current loss is:  -3.9451560974121094\n",
      "Current batch training loss: -3.945156  [    0/ 4000]\n",
      "Warning: nan gradient found. The current loss is:  -3.9087159633636475\n",
      "Warning: nan gradient found. The current loss is:  -4.05008602142334\n",
      "Warning: nan gradient found. The current loss is:  -3.8781280517578125\n",
      "Warning: nan gradient found. The current loss is:  -4.020653247833252\n",
      "Warning: nan gradient found. The current loss is:  -3.997894525527954\n",
      "Warning: nan gradient found. The current loss is:  -4.013490200042725\n",
      "Warning: nan gradient found. The current loss is:  -3.88070011138916\n",
      "Warning: nan gradient found. The current loss is:  -4.010725975036621\n",
      "Warning: nan gradient found. The current loss is:  -3.996821403503418\n",
      "Warning: nan gradient found. The current loss is:  -3.967468500137329\n",
      "Warning: nan gradient found. The current loss is:  -3.8270084857940674\n",
      "Warning: nan gradient found. The current loss is:  -4.06229305267334\n",
      "Warning: nan gradient found. The current loss is:  -3.995936393737793\n",
      "Warning: nan gradient found. The current loss is:  -3.998260736465454\n",
      "Warning: nan gradient found. The current loss is:  -3.959540605545044\n",
      "Training loss: -3.969555\n",
      "Validation loss: -3.954027 \n",
      "\n",
      "Stopping early!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Implement early stopping in training loop\n",
    "# Stop if validation loss has not decreased for the last [patience] epochs\n",
    "# The model with the lowest loss is stored\n",
    "patience = 2\n",
    "\n",
    "Training_losses = np.array([])\n",
    "Validation_losses = np.array([])\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    Training_losses = np.append(Training_losses, mytools.train(train_dataloader, model, NLLloss_TS, optimizer, device))\n",
    "    Validation_losses = np.append(Validation_losses, mytools.validate(val_dataloader, model, NLLloss_TS, device))\n",
    "    \n",
    "    # Keep a running copy of the model with the lowest loss\n",
    "    # Do not copy the model if loss in nan\n",
    "    if (Validation_losses[-1] == np.min(Validation_losses)) and (~np.isnan(Validation_losses[-1])):\n",
    "        final_model = copy.deepcopy(model)\n",
    "    \n",
    "    if len(Validation_losses) > patience:\n",
    "        if np.sum((Validation_losses[-1*np.arange(patience)-1] - Validation_losses[-1*np.arange(patience)-2]) < 0) == 0:\n",
    "            print(\"Stopping early!\")\n",
    "            break\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc28f9-674c-4f23-b693-a924ae1ca74f",
   "metadata": {},
   "source": [
    "Interestingly, only using the high K approximation seems to work best for the simple arrows case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdc940-77d1-4cdb-81bb-aa95813690bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypt",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137a2665c242313c11d472736bb1efbdaf7608c607fce3fc4f47a32817024ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
